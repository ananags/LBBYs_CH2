{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesado "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/processed/train_simp_preprocess_v2.csv\")\n",
    "df_test = pd.read_csv(\"../../data/processed/test_simp_preprocess_v2.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesado inicial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8950 entries, 0 to 8949\n",
      "Data columns (total 31 columns):\n",
      " #   Column                              Non-Null Count  Dtype \n",
      "---  ------                              --------------  ----- \n",
      " 0   id                                  8950 non-null   object\n",
      " 1   label                               8950 non-null   int64 \n",
      " 2   statement                           8950 non-null   object\n",
      " 3   subject                             8950 non-null   object\n",
      " 4   speaker                             8950 non-null   object\n",
      " 5   speaker_job                         8950 non-null   object\n",
      " 6   state_info                          8950 non-null   object\n",
      " 7   party_affiliation                   8950 non-null   object\n",
      " 8   party_affiliation_uni               8950 non-null   object\n",
      " 9   party_affiliation_category_map      8950 non-null   object\n",
      " 10  statement_tokens                    8950 non-null   object\n",
      " 11  num_tokens                          8950 non-null   int64 \n",
      " 12  num_sentences                       8950 non-null   int64 \n",
      " 13  pos_info                            8950 non-null   object\n",
      " 14  pos_freq                            8950 non-null   object\n",
      " 15  lemma_freq                          8950 non-null   object\n",
      " 16  tag_freq                            8950 non-null   object\n",
      " 17  entities                            8950 non-null   object\n",
      " 18  stopwords                           8950 non-null   object\n",
      " 19  statement_tokens_without_stopwords  8950 non-null   object\n",
      " 20  num_tokens_without_stopwords        8950 non-null   int64 \n",
      " 21  pos_info_without_stopwords          8950 non-null   object\n",
      " 22  pos_freq_without_stopwords          8950 non-null   object\n",
      " 23  lemma_freq_without_stopwords        8950 non-null   object\n",
      " 24  tag_freq_without_stopwords          8950 non-null   object\n",
      " 25  processed_subject                   8950 non-null   object\n",
      " 26  speaker_entities                    8950 non-null   object\n",
      " 27  speaker_type                        8950 non-null   object\n",
      " 28  speaker_job_tokens                  8950 non-null   object\n",
      " 29  state_info_tokens                   8950 non-null   object\n",
      " 30  party_affiliation_tokens            8950 non-null   object\n",
      "dtypes: int64(4), object(27)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intento 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['statement']\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6208530805687204\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.38      0.40      1014\n",
      "           1       0.70      0.75      0.72      1940\n",
      "\n",
      "    accuracy                           0.62      2954\n",
      "   macro avg       0.57      0.56      0.56      2954\n",
      "weighted avg       0.61      0.62      0.61      2954\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 381  633]\n",
      " [ 487 1453]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intento 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    'statement_tokens_without_stopwords',\n",
    "    'processed_subject',\n",
    "    'party_affiliation_category_map',\n",
    "]\n",
    "\n",
    "X = df[cols]\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     31\u001b[39m pipeline = Pipeline([\n\u001b[32m     32\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mpreprocessor\u001b[39m\u001b[33m'\u001b[39m, preprocessor),\n\u001b[32m     33\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mclf\u001b[39m\u001b[33m'\u001b[39m, LinearSVC())\n\u001b[32m     34\u001b[39m ])\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# --- 5. Ajustar modelo ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# --- 6. Predecir y evaluar ---\u001b[39;00m\n\u001b[32m     40\u001b[39m y_pred = pipeline.predict(X_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\pipeline.py:654\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    647\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    648\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe `transform_input` parameter can only be set if metadata \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    649\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrouting is enabled. You can enable metadata routing using \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    650\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`sklearn.set_config(enable_metadata_routing=True)`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    651\u001b[39m     )\n\u001b[32m    653\u001b[39m routed_params = \u001b[38;5;28mself\u001b[39m._check_method_params(method=\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, props=params)\n\u001b[32m--> \u001b[39m\u001b[32m654\u001b[39m Xt = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[33m\"\u001b[39m\u001b[33mPipeline\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m._log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.steps) - \u001b[32m1\u001b[39m)):\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\pipeline.py:588\u001b[39m, in \u001b[36mPipeline._fit\u001b[39m\u001b[34m(self, X, y, routed_params, raw_params)\u001b[39m\n\u001b[32m    581\u001b[39m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[32m    582\u001b[39m step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    583\u001b[39m     step_idx=step_idx,\n\u001b[32m    584\u001b[39m     step_params=routed_params[name],\n\u001b[32m    585\u001b[39m     all_params=raw_params,\n\u001b[32m    586\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m588\u001b[39m X, fitted_transformer = \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPipeline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[32m    598\u001b[39m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[32m    600\u001b[39m \u001b[38;5;28mself\u001b[39m.steps[step_idx] = (name, fitted_transformer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\joblib\\memory.py:326\u001b[39m, in \u001b[36mNotMemorizedFunc.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\pipeline.py:1551\u001b[39m, in \u001b[36m_fit_transform_one\u001b[39m\u001b[34m(transformer, X, y, weight, message_clsname, message, params)\u001b[39m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[32m   1550\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1551\u001b[39m         res = \u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit_transform\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1552\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1553\u001b[39m         res = transformer.fit(X, y, **params.get(\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, {})).transform(\n\u001b[32m   1554\u001b[39m             X, **params.get(\u001b[33m\"\u001b[39m\u001b[33mtransform\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m   1555\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:993\u001b[39m, in \u001b[36mColumnTransformer.fit_transform\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    990\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_transformers()\n\u001b[32m    991\u001b[39m n_samples = _num_samples(X)\n\u001b[32m--> \u001b[39m\u001b[32m993\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_column_callables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    994\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_remainder(X)\n\u001b[32m    996\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _routing_enabled():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:552\u001b[39m, in \u001b[36mColumnTransformer._validate_column_callables\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    550\u001b[39m         columns = columns(X)\n\u001b[32m    551\u001b[39m     all_columns.append(columns)\n\u001b[32m--> \u001b[39m\u001b[32m552\u001b[39m     transformer_to_input_indices[name] = \u001b[43m_get_column_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[38;5;28mself\u001b[39m._columns = all_columns\n\u001b[32m    555\u001b[39m \u001b[38;5;28mself\u001b[39m._transformer_to_input_indices = transformer_to_input_indices\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\utils\\_indexing.py:333\u001b[39m, in \u001b[36m_get_column_indices\u001b[39m\u001b[34m(X, key)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_interchange_protocol(X):\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_column_indices_interchange(X.__dataframe__(), key, key_dtype)\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m n_columns = \u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m key:\n\u001b[32m    335\u001b[39m     \u001b[38;5;66;03m# we get an empty list\u001b[39;00m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n",
      "\u001b[31mIndexError\u001b[39m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "X = df[[\n",
    "    'statement_tokens_without_stopwords',\n",
    "    'processed_subject',\n",
    "    'party_affiliation_category_map'\n",
    "]]\n",
    "y = df['label']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('txt1', TfidfVectorizer(), 'statement_tokens_without_stopwords'),\n",
    "    ('txt2', TfidfVectorizer(), 'processed_subject'),\n",
    "    ('cat',  OneHotEncoder(handle_unknown='ignore'),\n",
    "             ['party_affiliation_category_map'])\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('clf', LinearSVC())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred):.4f}')\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intento 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "▶︎ Mejores parámetros: {'clf__C': 0.1, 'clf__class_weight': 'balanced', 'pre__txt1__max_df': 0.8, 'pre__txt1__ngram_range': (1, 2), 'pre__txt2__max_df': 0.8, 'pre__txt2__ngram_range': (1, 2)}\n",
      "▶︎ Mejor F1-macro CV: 0.5745775498838119\n",
      "\n",
      "Accuracy (test): 0.5992\n",
      "\n",
      "Classification Report (test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.53      0.48      1041\n",
      "           1       0.71      0.64      0.67      1913\n",
      "\n",
      "    accuracy                           0.60      2954\n",
      "   macro avg       0.58      0.58      0.58      2954\n",
      "weighted avg       0.62      0.60      0.61      2954\n",
      "\n",
      "\n",
      "Confusion Matrix (test):\n",
      "[[ 547  494]\n",
      " [ 690 1223]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "X = df[[\n",
    "    'statement_tokens_without_stopwords',\n",
    "    'processed_subject',\n",
    "    'party_affiliation_category_map',\n",
    "    'num_tokens_without_stopwords',\n",
    "    'num_sentences'\n",
    "]]\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('txt1', TfidfVectorizer(), 'statement_tokens_without_stopwords'),\n",
    "    ('txt2', TfidfVectorizer(), 'processed_subject'),\n",
    "    ('cat',  OneHotEncoder(handle_unknown='ignore'),\n",
    "             ['party_affiliation_category_map']),\n",
    "    ('num', StandardScaler(),\n",
    "            ['num_tokens_without_stopwords', 'num_sentences'])\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('pre', preprocessor),\n",
    "    ('clf', LinearSVC(max_iter=5000))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'pre__txt1__ngram_range': [(1,1), (1,2)],\n",
    "    'pre__txt1__max_df':      [0.8, 1.0],\n",
    "    'pre__txt2__ngram_range': [(1,1), (1,2)],\n",
    "    'pre__txt2__max_df':      [0.8, 1.0],\n",
    "    'clf__C':                [0.1, 1, 10],\n",
    "    'clf__class_weight':    [None, 'balanced']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\" Mejores parámetros:\", grid.best_params_)\n",
    "print(\" Mejor F1-macro CV:\", grid.best_score_)\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(f\"\\nAccuracy (test): {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\nClassification Report (test):\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix (test):\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "cols_submission = [\n",
    "    'statement_tokens_without_stopwords',\n",
    "    'processed_subject',\n",
    "    'party_affiliation_category_map',\n",
    "    'num_tokens_without_stopwords',\n",
    "    'num_sentences'\n",
    "]\n",
    "\n",
    "X_test_raw = df_test[cols_submission]\n",
    "\n",
    "y_pred = best_model.predict(X_test_raw)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id':    df_test['id'],\n",
    "    'label': y_pred\n",
    "})\n",
    "\n",
    "filename = f\"svc_submission_{datetime.datetime.now().strftime('%Y%m%d')}.csv\"\n",
    "submission.to_csv(filename, index=False, columns=['id', 'label'])\n",
    "\n",
    "print(f\" Submission generada correctamente: '{filename}'\")\n",
    "print(submission.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intento 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "path = r\"C:\\modelos\\all-MiniLM-L6-v2\"\n",
    "bert = SentenceTransformer(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anaeg\\AppData\\Local\\Temp\\ipykernel_13236\\2367161636.py:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\anaeg\\AppData\\Local\\Temp\\ipykernel_13236\\2367161636.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_test[col].fillna('', inplace=True)\n",
      "C:\\Users\\anaeg\\AppData\\Local\\Temp\\ipykernel_13236\\2367161636.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\anaeg\\AppData\\Local\\Temp\\ipykernel_13236\\2367161636.py:27: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_test[col].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 100 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n100 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\pipeline.py\", line 654, in fit\n    Xt = self._fit(X, y, routed_params, raw_params=params)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\pipeline.py\", line 588, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\joblib\\memory.py\", line 326, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\pipeline.py\", line 1551, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 319, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 1001, in fit_transform\n    result = self._call_func_on_transformers(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 910, in _call_func_on_transformers\n    return Parallel(n_jobs=self.n_jobs)(jobs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 77, in __call__\n    return super().__call__(iterable_with_config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\joblib\\parallel.py\", line 1985, in __call__\n    return output if self.return_generator else list(output)\n                                                ^^^^^^^^^^^^\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\joblib\\parallel.py\", line 1913, in _get_sequential_output\n    res = func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 139, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\pipeline.py\", line 1551, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 319, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\base.py\", line 921, in fit_transform\n    return self.fit(X, y, **fit_params).transform(X)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 319, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\anaeg\\AppData\\Local\\Temp\\ipykernel_13236\\2367161636.py\", line 44, in transform\n  File \"spacy\\tokens\\doc.pyx\", line 926, in sents\nValueError: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 105\u001b[39m\n\u001b[32m     98\u001b[39m search = RandomizedSearchCV(\n\u001b[32m     99\u001b[39m     pipeline, param_distributions=param_dist,\n\u001b[32m    100\u001b[39m     n_iter=\u001b[32m20\u001b[39m, cv=\u001b[32m5\u001b[39m, scoring=\u001b[33m'\u001b[39m\u001b[33mf1_macro\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    101\u001b[39m     n_jobs=-\u001b[32m1\u001b[39m, verbose=\u001b[32m2\u001b[39m, random_state=\u001b[32m42\u001b[39m\n\u001b[32m    102\u001b[39m )\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# --- Entrenamiento ---\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m \u001b[43msearch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMejores parámetros:\u001b[39m\u001b[33m\"\u001b[39m, search.best_params_)\n\u001b[32m    108\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMejor F1 macro CV:\u001b[39m\u001b[33m\"\u001b[39m, search.best_score_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1018\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1019\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1020\u001b[39m     )\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1027\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1028\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1951\u001b[39m, in \u001b[36mRandomizedSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1950\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1951\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1952\u001b[39m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1953\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[32m   1954\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1955\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1001\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    994\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) != n_candidates * n_splits:\n\u001b[32m    995\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    996\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcv.split and cv.get_n_splits returned \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    997\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    998\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(n_splits, \u001b[38;5;28mlen\u001b[39m(out) // n_candidates)\n\u001b[32m    999\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[32m   1004\u001b[39m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[32m   1005\u001b[39m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[32m   1006\u001b[39m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.scoring):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:517\u001b[39m, in \u001b[36m_warn_or_raise_about_fit_failures\u001b[39m\u001b[34m(results, error_score)\u001b[39m\n\u001b[32m    510\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits == num_fits:\n\u001b[32m    511\u001b[39m     all_fits_failed_message = (\n\u001b[32m    512\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    513\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    514\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou can try to debug the error by setting error_score=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    516\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[32m    519\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    520\u001b[39m     some_fits_failed_message = (\n\u001b[32m    521\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    522\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe score on these train-test partitions for these parameters\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    526\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    527\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: \nAll the 100 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n100 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\pipeline.py\", line 654, in fit\n    Xt = self._fit(X, y, routed_params, raw_params=params)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\pipeline.py\", line 588, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\joblib\\memory.py\", line 326, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\pipeline.py\", line 1551, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 319, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 1001, in fit_transform\n    result = self._call_func_on_transformers(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 910, in _call_func_on_transformers\n    return Parallel(n_jobs=self.n_jobs)(jobs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 77, in __call__\n    return super().__call__(iterable_with_config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\joblib\\parallel.py\", line 1985, in __call__\n    return output if self.return_generator else list(output)\n                                                ^^^^^^^^^^^^\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\joblib\\parallel.py\", line 1913, in _get_sequential_output\n    res = func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 139, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\pipeline.py\", line 1551, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 319, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\base.py\", line 921, in fit_transform\n    return self.fit(X, y, **fit_params).transform(X)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\anaeg\\Desktop\\LBBYs_CH2\\spacy_env\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 319, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\anaeg\\AppData\\Local\\Temp\\ipykernel_13236\\2367161636.py\", line 44, in transform\n  File \"spacy\\tokens\\doc.pyx\", line 926, in sents\nValueError: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import spacy\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "from scipy.stats import uniform\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "for col in ['statement_tokens_without_stopwords','processed_subject','party_affiliation_category_map']:\n",
    "    df[col].fillna('', inplace=True)\n",
    "    df_test[col].fillna('', inplace=True)\n",
    "\n",
    "for col in ['num_tokens_without_stopwords','num_sentences']:\n",
    "    df[col].fillna(0, inplace=True)\n",
    "    df_test[col].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser','ner','tok2vec','tagger'])\n",
    "\n",
    "class StyleFeaturesExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): \n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        rows = []\n",
    "        for doc in nlp.pipe(X, batch_size=32):\n",
    "            rows.append([\n",
    "                sum(1 for t in doc if t.text.lower() in {'i','me','my','mine','we','us','our','ours'}),\n",
    "                sum(1 for t in doc if t.text.lower() in {'they','them','their','theirs','he','she','him','her'}),\n",
    "                sum(1 for t in doc if t.dep_ == 'neg'),\n",
    "                sum(1 for t in doc if t.text.lower() in {'maybe','perhaps','possibly','could','might'}),\n",
    "                sum(1 for t in doc if t.text.lower() in {'but','however','although','though','yet'}),\n",
    "                len(doc) / max(1, len(list(doc.sents)))\n",
    "            ])\n",
    "        return np.array(rows)\n",
    "\n",
    "\n",
    "model_path = r\"C:\\modelos\\all-MiniLM-L6-v2\" \n",
    "bert = SentenceTransformer(model_path)\n",
    "\n",
    "bert_transformer = FunctionTransformer(\n",
    "    func=lambda X: bert.encode(X.tolist(), batch_size=32, show_progress_bar=False),\n",
    "    validate=False\n",
    ")\n",
    "\n",
    "cols = [\n",
    "    'statement_tokens_without_stopwords',\n",
    "    'processed_subject',\n",
    "    'party_affiliation_category_map',\n",
    "    'num_tokens_without_stopwords',\n",
    "    'num_sentences'\n",
    "]\n",
    "\n",
    "X = df[cols]\n",
    "y = df['label']\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('bert', bert_transformer, 'statement_tokens_without_stopwords'),          \n",
    "    ('tfidf_subj', TfidfVectorizer(), 'processed_subject'),                   \n",
    "    ('cat_party', OneHotEncoder(handle_unknown='ignore'), ['party_affiliation_category_map']),  \n",
    "    ('num_feats', StandardScaler(), ['num_tokens_without_stopwords', 'num_sentences']),         \n",
    "    ('style', StyleFeaturesExtractor(), 'statement_tokens_without_stopwords')                    \n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preproc', preprocessor),\n",
    "    ('clf', LinearSVC(max_iter=5000))\n",
    "])\n",
    "\n",
    "param_dist = {\n",
    "    'preproc__tfidf_subj__max_df':      uniform(0.7,0.3),\n",
    "    'preproc__tfidf_subj__ngram_range': [(1,1), (1,2)],\n",
    "    'clf__C':                           uniform(0.01, 10),\n",
    "    'clf__class_weight':                [None, 'balanced']\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    pipeline, param_distributions=param_dist,\n",
    "    n_iter=20, cv=5, scoring='f1_macro',\n",
    "    n_jobs=-1, verbose=2, random_state=42\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Mejores parámetros:\", search.best_params_)\n",
    "print(\"Mejor F1 macro CV:\", search.best_score_)\n",
    "\n",
    "y_pred = search.predict(X_valid)\n",
    "print(\"Accuracy:\", accuracy_score(y_valid, y_pred))\n",
    "print(classification_report(y_valid, y_pred))\n",
    "print(\"Matriz de confusión:\\n\", confusion_matrix(y_valid, y_pred))\n",
    "try:\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_valid, search.decision_function(X_valid)))\n",
    "except:\n",
    "    print(\"ROC AUC no disponible para este clasificador.\")\n",
    "\n",
    "test_pred = search.predict(df_test[cols])\n",
    "submission = pd.DataFrame({'id': df_test['id'], 'label': test_pred})\n",
    "filename = f\"svc_final_submission_{datetime.datetime.now():%Y%m%d}.csv\"\n",
    "submission.to_csv(filename, index=False)\n",
    "print(f\"Submission guardada en: {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusión**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque el experimento híbrido con SBERT y rasgos de estilo (intento 4) alcanzó el F1-macro más alto (~0,58) y mejoró notablemente el equilibrio entre precisión y cobertura de la clase minoritaria, la variabilidad en la validación y los problemas de integración de librerías impidieron establecerlo como claro vencedor; por lo tanto, el que mejor metrcias finales ha dado es el primero. El clásico pipeline de TF-IDF sobre el texto crudo y LinearSVC obtuvo una accuracy de test del 62,09 % y un F1-macro ≈ 0,56 (precisión/recall 0 → 0,44/0,38; 1 → 0,70/0,75). Aunque sencillo, demostró ser muy competitivo frente a configuraciones intermedias que sólo añadían metadatos o contadores numéricos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
