{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5718d783",
   "metadata": {},
   "source": [
    "# Modelo GRU para detección de noticias falsas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9922532d",
   "metadata": {},
   "source": [
    "### Análisis básico de los datasets de entrenamiento\n",
    "\n",
    "En este paso, cargamos los dos datasets de entrenamiento disponibles para nuestro problema de clasificación binaria de noticias falsas.\n",
    "\n",
    "Para cada dataset mostramos:\n",
    "\n",
    "- Información general (número de filas, tipos de datos, valores nulos)\n",
    "- Distribución relativa de la variable objetivo `label` (balance entre clases)\n",
    "- Estadísticas básicas de la longitud del texto de la columna `statement` (mínimo, máximo, media, percentiles)\n",
    "\n",
    "Esto nos ayudará a decidir cuál dataset es más adecuado para entrenar nuestro modelo GRU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cdb3e714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "194422bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== train_simp_preprocess_v2.csv ==\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8950 entries, 0 to 8949\n",
      "Data columns (total 31 columns):\n",
      " #   Column                              Non-Null Count  Dtype \n",
      "---  ------                              --------------  ----- \n",
      " 0   id                                  8950 non-null   object\n",
      " 1   label                               8950 non-null   int64 \n",
      " 2   statement                           8950 non-null   object\n",
      " 3   subject                             8950 non-null   object\n",
      " 4   speaker                             8950 non-null   object\n",
      " 5   speaker_job                         8950 non-null   object\n",
      " 6   state_info                          8950 non-null   object\n",
      " 7   party_affiliation                   8950 non-null   object\n",
      " 8   party_affiliation_uni               8950 non-null   object\n",
      " 9   party_affiliation_category_map      8950 non-null   object\n",
      " 10  statement_tokens                    8950 non-null   object\n",
      " 11  num_tokens                          8950 non-null   int64 \n",
      " 12  num_sentences                       8950 non-null   int64 \n",
      " 13  pos_info                            8950 non-null   object\n",
      " 14  pos_freq                            8950 non-null   object\n",
      " 15  lemma_freq                          8950 non-null   object\n",
      " 16  tag_freq                            8950 non-null   object\n",
      " 17  entities                            8950 non-null   object\n",
      " 18  stopwords                           8950 non-null   object\n",
      " 19  statement_tokens_without_stopwords  8950 non-null   object\n",
      " 20  num_tokens_without_stopwords        8950 non-null   int64 \n",
      " 21  pos_info_without_stopwords          8950 non-null   object\n",
      " 22  pos_freq_without_stopwords          8950 non-null   object\n",
      " 23  lemma_freq_without_stopwords        8950 non-null   object\n",
      " 24  tag_freq_without_stopwords          8950 non-null   object\n",
      " 25  processed_subject                   8950 non-null   object\n",
      " 26  speaker_entities                    8950 non-null   object\n",
      " 27  speaker_type                        8950 non-null   object\n",
      " 28  speaker_job_tokens                  8950 non-null   object\n",
      " 29  state_info_tokens                   8950 non-null   object\n",
      " 30  party_affiliation_tokens            8950 non-null   object\n",
      "dtypes: int64(4), object(27)\n",
      "memory usage: 2.1+ MB\n",
      "None\n",
      "\n",
      "Distribución de clases (label):\n",
      "label\n",
      "1    0.647486\n",
      "0    0.352514\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Estadísticas longitud de textos (statement):\n",
      "count    8950.000000\n",
      "mean      107.318659\n",
      "std        59.990511\n",
      "min        11.000000\n",
      "25%        73.000000\n",
      "50%        99.000000\n",
      "75%       133.000000\n",
      "max      3204.000000\n",
      "Name: statement, dtype: float64\n",
      "\n",
      "== train_simp_limpieza_v2.csv ==\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8950 entries, 0 to 8949\n",
      "Data columns (total 10 columns):\n",
      " #   Column                          Non-Null Count  Dtype \n",
      "---  ------                          --------------  ----- \n",
      " 0   id                              8950 non-null   object\n",
      " 1   label                           8950 non-null   int64 \n",
      " 2   statement                       8950 non-null   object\n",
      " 3   subject                         8950 non-null   object\n",
      " 4   speaker                         8950 non-null   object\n",
      " 5   speaker_job                     8950 non-null   object\n",
      " 6   state_info                      8950 non-null   object\n",
      " 7   party_affiliation               8950 non-null   object\n",
      " 8   party_affiliation_uni           8950 non-null   object\n",
      " 9   party_affiliation_category_map  8950 non-null   object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 699.3+ KB\n",
      "None\n",
      "\n",
      "Distribución de clases (label):\n",
      "label\n",
      "1    0.647486\n",
      "0    0.352514\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Estadísticas longitud de textos (statement):\n",
      "count    8950.000000\n",
      "mean      107.318659\n",
      "std        59.990511\n",
      "min        11.000000\n",
      "25%        73.000000\n",
      "50%        99.000000\n",
      "75%       133.000000\n",
      "max      3204.000000\n",
      "Name: statement, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Cargar datasets de entrenamiento\n",
    "train_preprocess = pd.read_csv('train_simp_preprocess_v2.csv')\n",
    "train_limpieza = pd.read_csv('train_simp_limpieza_v2.csv')\n",
    "\n",
    "# Mostrar información básica y distribución clases train_preprocess\n",
    "print(\"== train_simp_preprocess_v2.csv ==\")\n",
    "print(train_preprocess.info())\n",
    "print(\"\\nDistribución de clases (label):\")\n",
    "print(train_preprocess['label'].value_counts(normalize=True))\n",
    "print(\"\\nEstadísticas longitud de textos (statement):\")\n",
    "print(train_preprocess['statement'].str.len().describe())\n",
    "\n",
    "print(\"\\n== train_simp_limpieza_v2.csv ==\")\n",
    "print(train_limpieza.info())\n",
    "print(\"\\nDistribución de clases (label):\")\n",
    "print(train_limpieza['label'].value_counts(normalize=True))\n",
    "print(\"\\nEstadísticas longitud de textos (statement):\")\n",
    "print(train_limpieza['statement'].str.len().describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b386df80",
   "metadata": {},
   "source": [
    "### Preprocesamiento del texto\n",
    "\n",
    "Antes de entrenar el modelo GRU, necesitamos preparar el texto de entrada para que sea compatible con el modelo.\n",
    "\n",
    "Pasos a realizar:\n",
    "\n",
    "- Normalización del texto (minúsculas y limpieza básica si es necesario)  \n",
    "- Tokenización usando `Tokenizer` de Keras para convertir palabras a índices enteros  \n",
    "- Padding para que todas las secuencias tengan la misma longitud, necesaria para procesar en batches en el modelo  \n",
    "\n",
    "Esto nos permitirá transformar las oraciones en secuencias numéricas que el modelo podrá entender.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2fe47ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    China is in the South China Sea and (building)...\n",
      "1    With the resources it takes to execute just ov...\n",
      "2    The (Wisconsin) governor has proposed tax give...\n",
      "3    Says her representation of an ex-boyfriend who...\n",
      "4    At protests in Wisconsin against proposed coll...\n",
      "Name: statement, dtype: object\n",
      "Ejemplo texto: china is in the south china sea and (building)a military fortress the likes of which perhaps the world has not seen.\n",
      "Secuencia tokenizada: [318, 11, 3, 2, 657, 318, 2536, 7, 777, 6, 295, 6487, 2, 3289, 4, 179, 2537, 2, 165, 13, 30, 522]\n",
      "Secuencia padded: [ 318   11    3    2  657  318 2536    7  777    6  295 6487    2 3289\n",
      "    4  179 2537    2  165   13   30  522    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n",
      "Etiqueta: 1\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Cargar dataset elegido\n",
    "train = pd.read_csv('train_simp_limpieza_v2.csv')\n",
    "\n",
    "# Inspección rápida para comprobar la columna\n",
    "print(train['statement'].head())\n",
    "\n",
    "# Normalizar texto: pasamos a minúsculas (opcional se puede limpiar más)\n",
    "train['statement'] = train['statement'].str.lower()\n",
    "\n",
    "# Parámetros para tokenización\n",
    "max_words = 10000   # máximo vocabulario\n",
    "max_len = 150       # longitud máxima de secuencia (ajustable)\n",
    "\n",
    "# Crear el tokenizer y ajustarlo al texto\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train['statement'])\n",
    "\n",
    "# Convertir textos a secuencias de enteros\n",
    "train_sequences = tokenizer.texts_to_sequences(train['statement'])\n",
    "\n",
    "# Padding para igualar la longitud de las secuencias\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# Etiquetas\n",
    "train_labels = train['label'].values\n",
    "\n",
    "# Mostrar ejemplo de secuencia tokenizada y padded\n",
    "print(\"Ejemplo texto:\", train['statement'].iloc[0])\n",
    "print(\"Secuencia tokenizada:\", train_sequences[0])\n",
    "print(\"Secuencia padded:\", train_padded[0])\n",
    "print(\"Etiqueta:\", train_labels[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7b6d99",
   "metadata": {},
   "source": [
    "### Construcción y entrenamiento del modelo GRU\n",
    "\n",
    "En esta etapa construimos un modelo secuencial con las siguientes capas:\n",
    "\n",
    "- Embedding: para convertir índices de palabras en vectores densos de tamaño fijo  \n",
    "- GRU: capa recurrente que captura dependencias temporales en la secuencia  \n",
    "- Dense: capa densa final con activación sigmoid para clasificación binaria  \n",
    "\n",
    "Después compilamos el modelo con pérdida `binary_crossentropy`, optimizador Adam y medimos precisión.\n",
    "\n",
    "Finalmente, entrenamos el modelo usando validación interna para monitorear performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "353c8c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">640,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,960</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │       \u001b[38;5;34m640,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m24,960\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">665,025</span> (2.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m665,025\u001b[0m (2.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">665,025</span> (2.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m665,025\u001b[0m (2.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout\n",
    "\n",
    "# Variables globales de tokenización que definiste\n",
    "max_words = 10000\n",
    "max_len = 150\n",
    "\n",
    "embedding_dim = 64\n",
    "gru_units = 64\n",
    "\n",
    "# Crear el modelo asegurando que el input_length y input_dim están bien definidos\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len),\n",
    "    GRU(gru_units, return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Forzar construcción del modelo para que se muestre correctamente el summary\n",
    "model.build(input_shape=(None, max_len))\n",
    "\n",
    "# Ahora muestra el resumen con los parámetros\n",
    "model.summary()\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a09292",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy scipy scikit-learn tensorflow matplotlib pandas\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
