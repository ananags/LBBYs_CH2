{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79a011b9",
   "metadata": {},
   "source": [
    "# 1. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1ff4929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting numpy>=1.19.5 (from scikit-learn)\n",
      "  Using cached numpy-2.2.5-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Using cached scipy-1.15.3-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached scikit_learn-1.6.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.2 MB)\n",
      "Using cached pandas-2.2.3-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "Using cached joblib-1.5.0-py3-none-any.whl (307 kB)\n",
      "Using cached numpy-2.2.5-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached scipy-1.15.3-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.3 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, threadpoolctl, numpy, joblib, scipy, pandas, scikit-learn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [scikit-learn][0m [scikit-learn]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed joblib-1.5.0 numpy-2.2.5 pandas-2.2.3 pytz-2025.2 scikit-learn-1.6.1 scipy-1.15.3 threadpoolctl-3.6.0 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad907754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12cbcfc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>statement</th>\n",
       "      <th>subject</th>\n",
       "      <th>speaker</th>\n",
       "      <th>speaker_job</th>\n",
       "      <th>state_info</th>\n",
       "      <th>party_affiliation</th>\n",
       "      <th>party_affiliation_uni</th>\n",
       "      <th>party_affiliation_category_map</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_info_without_stopwords</th>\n",
       "      <th>pos_freq_without_stopwords</th>\n",
       "      <th>lemma_freq_without_stopwords</th>\n",
       "      <th>tag_freq_without_stopwords</th>\n",
       "      <th>processed_subject</th>\n",
       "      <th>speaker_entities</th>\n",
       "      <th>speaker_type</th>\n",
       "      <th>speaker_job_tokens</th>\n",
       "      <th>state_info_tokens</th>\n",
       "      <th>party_affiliation_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81f884c64a7</td>\n",
       "      <td>1</td>\n",
       "      <td>china is in the south china sea and (building)...</td>\n",
       "      <td>china,foreign-policy,military</td>\n",
       "      <td>donald-trump</td>\n",
       "      <td>president-elect</td>\n",
       "      <td>new york</td>\n",
       "      <td>republican</td>\n",
       "      <td>republican</td>\n",
       "      <td>political-affiliation</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'lemma': 'china', 'pos': 'PROPN', 'tag': 'NN...</td>\n",
       "      <td>Counter({'PROPN': 4, 'NOUN': 4, 'ADJ': 1, 'VER...</td>\n",
       "      <td>Counter({'china': 2, 'south': 1, 'sea': 1, 'bu...</td>\n",
       "      <td>Counter({'NNP': 4, 'NN': 3, 'JJ': 1, 'NNS': 1,...</td>\n",
       "      <td>['china', 'foreign-policy', 'military']</td>\n",
       "      <td>['donald trump']</td>\n",
       "      <td>['PERSON']</td>\n",
       "      <td>['president', '-', 'elect']</td>\n",
       "      <td>['new', 'york']</td>\n",
       "      <td>['republican']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30c2723a188</td>\n",
       "      <td>0</td>\n",
       "      <td>with the resources it takes to execute just ov...</td>\n",
       "      <td>health-care</td>\n",
       "      <td>chris-dodd</td>\n",
       "      <td>u.s. senator</td>\n",
       "      <td>connecticut</td>\n",
       "      <td>democrat</td>\n",
       "      <td>democrat</td>\n",
       "      <td>political-affiliation</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'lemma': 'resource', 'pos': 'NOUN', 'tag': '...</td>\n",
       "      <td>Counter({'NOUN': 7, 'VERB': 4, 'PROPN': 2, 'AD...</td>\n",
       "      <td>Counter({'resource': 1, 'take': 1, 'execute': ...</td>\n",
       "      <td>Counter({'NN': 4, 'NNS': 3, 'VB': 2, 'NNP': 2,...</td>\n",
       "      <td>['health-care']</td>\n",
       "      <td>['chris dodd']</td>\n",
       "      <td>['PERSON']</td>\n",
       "      <td>['u.s', '.', 'senator']</td>\n",
       "      <td>['connecticut']</td>\n",
       "      <td>['democrat']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6936b216e5d</td>\n",
       "      <td>0</td>\n",
       "      <td>the (wisconsin) governor has proposed tax give...</td>\n",
       "      <td>corporations,pundits,taxes,abc-news-week</td>\n",
       "      <td>donna-brazile</td>\n",
       "      <td>political commentator</td>\n",
       "      <td>washington, d.c.</td>\n",
       "      <td>democrat</td>\n",
       "      <td>democrat</td>\n",
       "      <td>political-affiliation</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'lemma': 'wisconsin', 'pos': 'PROPN', 'tag':...</td>\n",
       "      <td>Counter({'NOUN': 4, 'PROPN': 1, 'VERB': 1})</td>\n",
       "      <td>Counter({'wisconsin': 1, 'governor': 1, 'propo...</td>\n",
       "      <td>Counter({'NN': 2, 'NNS': 2, 'NNP': 1, 'VBN': 1})</td>\n",
       "      <td>['corporations', 'pundits', 'taxes', 'abc-news...</td>\n",
       "      <td>['donna brazile']</td>\n",
       "      <td>['PERSON']</td>\n",
       "      <td>['political', 'commentator']</td>\n",
       "      <td>['washington', ',', 'd.c', '.']</td>\n",
       "      <td>['democrat']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b5cd9195738</td>\n",
       "      <td>1</td>\n",
       "      <td>says her representation of an ex-boyfriend who...</td>\n",
       "      <td>candidates-biography,children,ethics,families,...</td>\n",
       "      <td>rebecca-bradley</td>\n",
       "      <td>non-define</td>\n",
       "      <td>non-define</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>other-political-groups</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'lemma': 'say', 'pos': 'VERB', 'tag': 'VBZ',...</td>\n",
       "      <td>Counter({'NOUN': 9, 'VERB': 1, 'ADJ': 1})</td>\n",
       "      <td>Counter({'say': 1, 'representation': 1, 'ex': ...</td>\n",
       "      <td>Counter({'NN': 8, 'VBZ': 1, 'NNS': 1, 'JJ': 1})</td>\n",
       "      <td>['candidates-biography', 'children', 'ethics',...</td>\n",
       "      <td>['rebecca bradley']</td>\n",
       "      <td>['PERSON']</td>\n",
       "      <td>['non', '-', 'define']</td>\n",
       "      <td>['non', '-', 'define']</td>\n",
       "      <td>['none']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84f8dac7737</td>\n",
       "      <td>0</td>\n",
       "      <td>at protests in wisconsin against proposed coll...</td>\n",
       "      <td>health-care,labor,state-budget</td>\n",
       "      <td>republican-party-wisconsin</td>\n",
       "      <td>non-define</td>\n",
       "      <td>wisconsin</td>\n",
       "      <td>republican</td>\n",
       "      <td>republican</td>\n",
       "      <td>political-affiliation</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'lemma': 'protest', 'pos': 'NOUN', 'tag': 'N...</td>\n",
       "      <td>Counter({'NOUN': 7, 'VERB': 4, 'ADJ': 3})</td>\n",
       "      <td>Counter({'protest': 1, 'wisconsin': 1, 'propos...</td>\n",
       "      <td>Counter({'NNS': 4, 'NN': 3, 'JJ': 3, 'VBN': 2,...</td>\n",
       "      <td>['health-care', 'labor', 'state-budget']</td>\n",
       "      <td>['republican party', 'wisconsin']</td>\n",
       "      <td>['ORG', 'GPE']</td>\n",
       "      <td>['non', '-', 'define']</td>\n",
       "      <td>['wisconsin']</td>\n",
       "      <td>['republican']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  label                                          statement  \\\n",
       "0  81f884c64a7      1  china is in the south china sea and (building)...   \n",
       "1  30c2723a188      0  with the resources it takes to execute just ov...   \n",
       "2  6936b216e5d      0  the (wisconsin) governor has proposed tax give...   \n",
       "3  b5cd9195738      1  says her representation of an ex-boyfriend who...   \n",
       "4  84f8dac7737      0  at protests in wisconsin against proposed coll...   \n",
       "\n",
       "                                             subject  \\\n",
       "0                      china,foreign-policy,military   \n",
       "1                                        health-care   \n",
       "2           corporations,pundits,taxes,abc-news-week   \n",
       "3  candidates-biography,children,ethics,families,...   \n",
       "4                     health-care,labor,state-budget   \n",
       "\n",
       "                      speaker            speaker_job        state_info  \\\n",
       "0                donald-trump        president-elect          new york   \n",
       "1                  chris-dodd           u.s. senator       connecticut   \n",
       "2               donna-brazile  political commentator  washington, d.c.   \n",
       "3             rebecca-bradley             non-define        non-define   \n",
       "4  republican-party-wisconsin             non-define         wisconsin   \n",
       "\n",
       "  party_affiliation party_affiliation_uni party_affiliation_category_map  ...  \\\n",
       "0        republican            republican          political-affiliation  ...   \n",
       "1          democrat              democrat          political-affiliation  ...   \n",
       "2          democrat              democrat          political-affiliation  ...   \n",
       "3              none                  none         other-political-groups  ...   \n",
       "4        republican            republican          political-affiliation  ...   \n",
       "\n",
       "                          pos_info_without_stopwords  \\\n",
       "0  [{'lemma': 'china', 'pos': 'PROPN', 'tag': 'NN...   \n",
       "1  [{'lemma': 'resource', 'pos': 'NOUN', 'tag': '...   \n",
       "2  [{'lemma': 'wisconsin', 'pos': 'PROPN', 'tag':...   \n",
       "3  [{'lemma': 'say', 'pos': 'VERB', 'tag': 'VBZ',...   \n",
       "4  [{'lemma': 'protest', 'pos': 'NOUN', 'tag': 'N...   \n",
       "\n",
       "                          pos_freq_without_stopwords  \\\n",
       "0  Counter({'PROPN': 4, 'NOUN': 4, 'ADJ': 1, 'VER...   \n",
       "1  Counter({'NOUN': 7, 'VERB': 4, 'PROPN': 2, 'AD...   \n",
       "2        Counter({'NOUN': 4, 'PROPN': 1, 'VERB': 1})   \n",
       "3          Counter({'NOUN': 9, 'VERB': 1, 'ADJ': 1})   \n",
       "4          Counter({'NOUN': 7, 'VERB': 4, 'ADJ': 3})   \n",
       "\n",
       "                        lemma_freq_without_stopwords  \\\n",
       "0  Counter({'china': 2, 'south': 1, 'sea': 1, 'bu...   \n",
       "1  Counter({'resource': 1, 'take': 1, 'execute': ...   \n",
       "2  Counter({'wisconsin': 1, 'governor': 1, 'propo...   \n",
       "3  Counter({'say': 1, 'representation': 1, 'ex': ...   \n",
       "4  Counter({'protest': 1, 'wisconsin': 1, 'propos...   \n",
       "\n",
       "                          tag_freq_without_stopwords  \\\n",
       "0  Counter({'NNP': 4, 'NN': 3, 'JJ': 1, 'NNS': 1,...   \n",
       "1  Counter({'NN': 4, 'NNS': 3, 'VB': 2, 'NNP': 2,...   \n",
       "2   Counter({'NN': 2, 'NNS': 2, 'NNP': 1, 'VBN': 1})   \n",
       "3    Counter({'NN': 8, 'VBZ': 1, 'NNS': 1, 'JJ': 1})   \n",
       "4  Counter({'NNS': 4, 'NN': 3, 'JJ': 3, 'VBN': 2,...   \n",
       "\n",
       "                                   processed_subject  \\\n",
       "0            ['china', 'foreign-policy', 'military']   \n",
       "1                                    ['health-care']   \n",
       "2  ['corporations', 'pundits', 'taxes', 'abc-news...   \n",
       "3  ['candidates-biography', 'children', 'ethics',...   \n",
       "4           ['health-care', 'labor', 'state-budget']   \n",
       "\n",
       "                    speaker_entities    speaker_type  \\\n",
       "0                   ['donald trump']      ['PERSON']   \n",
       "1                     ['chris dodd']      ['PERSON']   \n",
       "2                  ['donna brazile']      ['PERSON']   \n",
       "3                ['rebecca bradley']      ['PERSON']   \n",
       "4  ['republican party', 'wisconsin']  ['ORG', 'GPE']   \n",
       "\n",
       "             speaker_job_tokens                state_info_tokens  \\\n",
       "0   ['president', '-', 'elect']                  ['new', 'york']   \n",
       "1       ['u.s', '.', 'senator']                  ['connecticut']   \n",
       "2  ['political', 'commentator']  ['washington', ',', 'd.c', '.']   \n",
       "3        ['non', '-', 'define']           ['non', '-', 'define']   \n",
       "4        ['non', '-', 'define']                    ['wisconsin']   \n",
       "\n",
       "  party_affiliation_tokens  \n",
       "0           ['republican']  \n",
       "1             ['democrat']  \n",
       "2             ['democrat']  \n",
       "3                 ['none']  \n",
       "4           ['republican']  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../../data/processed/train_preprocess_v1.csv')\n",
    "#data = pd.read_csv('../../data/processed/train_limpieza_v1.csv')\n",
    "data.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e282f357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar las características y etiquetas\n",
    "X = data['statement']  # Texto de la afirmación\n",
    "y = data['label']  # Etiqueta de veracidad (0 para falso, 1 para verdadero)\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Vectorización del texto utilizando TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "aaa95cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Entrenamiento del modelo Random Forest\n",
    "class_weights = {0: 5, 1: 2}  # No modificar los pesos, se empeora\n",
    "rf_model = RandomForestClassifier(\n",
    "    criterion='gini',\n",
    "    n_estimators=500,\n",
    "    max_depth=80,\n",
    "    max_features='sqrt',        # 'max_features': ['sqrt', 'log2', None],\n",
    "    class_weight=class_weights, #'balanced\n",
    "    random_state=42\n",
    ")\n",
    "rf_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Realizar predicciones\n",
    "y_pred = rf_model.predict(X_test_tfidf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "172bcef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.66      0.51       923\n",
      "           1       0.74      0.52      0.61      1762\n",
      "\n",
      "    accuracy                           0.57      2685\n",
      "   macro avg       0.58      0.59      0.56      2685\n",
      "weighted avg       0.63      0.57      0.58      2685\n",
      "\n",
      "Macro Average F1-Score: 0.5604\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo con Macro Average F1-Score\n",
    "# Reporte completo de métricas\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(f\"Macro Average F1-Score: {macro_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8b36533f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.joblib']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "# Guardar el vectorizador TF-IDF\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a658ee3",
   "metadata": {},
   "source": [
    "## Random forest solo con columnas determinadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1354d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.67      0.51       923\n",
      "           1       0.74      0.50      0.60      1762\n",
      "\n",
      "    accuracy                           0.56      2685\n",
      "   macro avg       0.58      0.59      0.56      2685\n",
      "weighted avg       0.63      0.56      0.57      2685\n",
      "\n",
      "Macro Average F1-Score: 0.5557\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar el dataset\n",
    "data = pd.read_csv('../../data/processed/train_preprocess_v1.csv')\n",
    "\n",
    "# Seleccionar las características relevantes y la etiqueta\n",
    "X = data[['statement', 'subject', 'party_affiliation', 'speaker', 'speaker_job', 'state_info', 'party_affiliation_uni']]  # Columnas relevantes\n",
    "y = data['label']  # Etiqueta de veracidad\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Vectorización del texto utilizando TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['statement'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['statement'])\n",
    "\n",
    "# Entrenamiento del modelo Random Forest\n",
    "class_weights = {0: 5, 1: 2}  # No modificar los pesos, ya que empeora el modelo\n",
    "rf_model = RandomForestClassifier(\n",
    "    criterion='gini',\n",
    "    n_estimators=500,\n",
    "    max_depth=80,\n",
    "    max_features='sqrt',        # 'max_features': ['sqrt', 'log2', None],\n",
    "    class_weight=class_weights, #'balanced'\n",
    "    random_state=42\n",
    ")\n",
    "rf_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Realizar predicciones\n",
    "y_pred = rf_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluar el modelo con Macro Average F1-Score\n",
    "# Reporte completo de métricas\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(f\"Macro Average F1-Score: {macro_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff0583d",
   "metadata": {},
   "source": [
    "### Aplico Smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7dacea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Using cached imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n",
      "Collecting imbalanced-learn (from imblearn)\n",
      "  Using cached imbalanced_learn-0.13.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from imbalanced-learn->imblearn) (2.2.5)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from imbalanced-learn->imblearn) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from imbalanced-learn->imblearn) (1.6.1)\n",
      "Collecting sklearn-compat<1,>=0.1 (from imbalanced-learn->imblearn)\n",
      "  Using cached sklearn_compat-0.1.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from imbalanced-learn->imblearn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from imbalanced-learn->imblearn) (3.6.0)\n",
      "Using cached imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Using cached imbalanced_learn-0.13.0-py3-none-any.whl (238 kB)\n",
      "Using cached sklearn_compat-0.1.3-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: sklearn-compat, imbalanced-learn, imblearn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [imblearn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed imbalanced-learn-0.13.0 imblearn-0.0 sklearn-compat-0.1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install imblearn SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "757394f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.80      0.52       923\n",
      "           1       0.76      0.33      0.46      1762\n",
      "\n",
      "    accuracy                           0.49      2685\n",
      "   macro avg       0.57      0.57      0.49      2685\n",
      "weighted avg       0.63      0.49      0.48      2685\n",
      "\n",
      "Macro Average F1-Score: 0.4915\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE  # Importamos SMOTE\n",
    "\n",
    "# Cargar el dataset\n",
    "data = pd.read_csv('../../data/processed/train_preprocess_v1.csv')\n",
    "\n",
    "# Seleccionar las características relevantes y la etiqueta\n",
    "X = data[['statement', 'subject', 'party_affiliation', 'speaker', 'speaker_job', 'state_info', 'party_affiliation_uni']]  # Columnas relevantes\n",
    "y = data['label']  # Etiqueta de veracidad\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Vectorización del texto utilizando TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['statement'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['statement'])\n",
    "\n",
    "# --- Aplicar SMOTE solo en el conjunto de entrenamiento ---\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_tfidf, y_train)\n",
    "\n",
    "# Entrenamiento del modelo Random Forest\n",
    "class_weights = {0: 5, 1: 2}  # No modificar los pesos, ya que empeora el modelo\n",
    "rf_model = RandomForestClassifier(\n",
    "    criterion='gini',\n",
    "    n_estimators=500,\n",
    "    max_depth=80,\n",
    "    max_features='sqrt',        # 'max_features': ['sqrt', 'log2', None],\n",
    "    class_weight=class_weights, #'balanced'\n",
    "    random_state=42\n",
    ")\n",
    "rf_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Realizar predicciones\n",
    "y_pred = rf_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluar el modelo con Macro Average F1-Score\n",
    "# Reporte completo de métricas\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(f\"Macro Average F1-Score: {macro_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283cf3cb",
   "metadata": {},
   "source": [
    "### Aplico la reducción de la mayoritaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "730d7cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.93      0.50       923\n",
      "           1       0.67      0.07      0.13      1762\n",
      "\n",
      "    accuracy                           0.37      2685\n",
      "   macro avg       0.51      0.50      0.32      2685\n",
      "weighted avg       0.56      0.37      0.26      2685\n",
      "\n",
      "Macro Average F1-Score: 0.3178\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar el dataset\n",
    "data = pd.read_csv('../../data/processed/train_preprocess_v1.csv')\n",
    "\n",
    "# Seleccionar las características relevantes y la etiqueta\n",
    "X = data[['statement', 'subject', 'party_affiliation', 'speaker', 'speaker_job', 'state_info', 'party_affiliation_uni']]  # Columnas relevantes\n",
    "y = data['label']  # Etiqueta de veracidad\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Vectorización del texto utilizando TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['statement'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['statement'])\n",
    "\n",
    "# --- Realizar submuestreo (undersampling) en el conjunto de entrenamiento ---\n",
    "# Extraemos los DataFrames de cada clase\n",
    "df_train = pd.DataFrame(X_train_tfidf.toarray())  # Convertimos a DataFrame para manipularlo\n",
    "df_train['label'] = y_train\n",
    "\n",
    "# Extraemos las clases 0 (minoritaria) y 1 (mayoritaria)\n",
    "df_train_class_0 = df_train[df_train['label'] == 0]\n",
    "df_train_class_1 = df_train[df_train['label'] == 1]\n",
    "\n",
    "# Realizamos un muestreo aleatorio de la clase mayoritaria (1) para igualar el número de la minoritaria (0)\n",
    "n_minority = len(df_train_class_0)\n",
    "df_train_class_1_under = df_train_class_1.sample(n=n_minority, random_state=42)\n",
    "\n",
    "# Combinamos ambas clases y mezclamos los registros\n",
    "df_train_balanced = pd.concat([df_train_class_0, df_train_class_1_under]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Volver a separar las características y etiquetas después del submuestreo\n",
    "X_train_balanced = df_train_balanced.drop(columns=['label'])\n",
    "y_train_balanced = df_train_balanced['label']\n",
    "\n",
    "# Entrenamiento del modelo Random Forest\n",
    "class_weights = {0: 5, 1: 2}  # No modificar los pesos, ya que empeora el modelo\n",
    "rf_model = RandomForestClassifier(\n",
    "    criterion='gini',\n",
    "    n_estimators=500,\n",
    "    max_depth=80,\n",
    "    max_features='sqrt',        # 'max_features': ['sqrt', 'log2', None],\n",
    "    class_weight=class_weights, #'balanced'\n",
    "    random_state=42\n",
    ")\n",
    "rf_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Realizar predicciones\n",
    "y_pred = rf_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluar el modelo con Macro Average F1-Score\n",
    "# Reporte completo de métricas\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(f\"Macro Average F1-Score: {macro_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e863926b",
   "metadata": {},
   "source": [
    "## Conclusiones,\n",
    "Si dejo los pesos 5:2, de 0.56 bajo a 0.52, sin embargo, si modifico los pesos a 2:1, donde aparantemente se produce una mejora, cuando lo subo a kaggle, me baja de un 0.57 a un 0.50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4969a80",
   "metadata": {},
   "source": [
    "## GridSearchCV para random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c03eb086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Average F1-Score: 0.4070\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.01      0.02       923\n",
      "           1       0.66      0.99      0.79      1762\n",
      "\n",
      "    accuracy                           0.66      2685\n",
      "   macro avg       0.56      0.50      0.41      2685\n",
      "weighted avg       0.59      0.66      0.53      2685\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import joblib\n",
    "\n",
    "# Cargar el dataset de entrenamiento\n",
    "data = pd.read_csv('../../data/processed/train_preprocess_v1.csv')\n",
    "\n",
    "# Seleccionar las características y etiquetas\n",
    "X = data['statement']  # Texto de la afirmación\n",
    "y = data['label']  # Etiqueta de veracidad (0 para falso, 1 para verdadero)\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Vectorización del texto utilizando TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Guardar el vectorizador para su uso posterior (si es necesario)\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.joblib')\n",
    "\n",
    "# Definir el modelo Random Forest\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Definir los parámetros a probar para el modelo\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "}\n",
    "\n",
    "# Realizar GridSearchCV para encontrar los mejores parámetros\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Obtener el mejor modelo después de la búsqueda de hiperparámetros\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Realizar predicciones con el mejor modelo\n",
    "y_test_pred = best_rf_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluar el modelo con el Macro Average F1-Score\n",
    "macro_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "print(f\"Macro Average F1-Score: {macro_f1:.4f}\")\n",
    "\n",
    "# Reporte completo de métricas\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecccb9f6",
   "metadata": {},
   "source": [
    "# 2. DistilBERT (fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa4b5835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting datasets\n",
      "  Using cached datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting torch\n",
      "  Using cached torch-2.7.0-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting huggingface_hub[hf_xet]\n",
      "  Downloading huggingface_hub-0.31.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "  Downloading numpy-2.2.5-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from transformers) (25.0)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached PyYAML-6.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface_hub[hf_xet])\n",
      "  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface_hub[hf_xet])\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.0 (from huggingface_hub[hf_xet])\n",
      "  Downloading hf_xet-1.1.1-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-20.0.0-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Using cached pandas-2.2.3-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface_hub[hf_xet])\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.11.18-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting setuptools (from torch)\n",
      "  Downloading setuptools-80.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch)\n",
      "  Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n",
      "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.0 (from torch)\n",
      "  Downloading triton-3.3.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.3-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting accelerate>=0.26.0 (from transformers[torch])\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: psutil in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from accelerate>=0.26.0->transformers[torch]) (7.0.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.6.0-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.4.3-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "Collecting idna>=2.0 (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Downloading charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-3.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.31.1-py3-none-any.whl (484 kB)\n",
      "Downloading hf_xet-1.1.1-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.5/25.5 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading torch-2.7.0-cp313-cp313-manylinux_2_28_x86_64.whl (865.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.0/865.0 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.3.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached scikit_learn-1.6.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.2 MB)\n",
      "Downloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "Downloading numpy-2.2.5-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.11.18-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.4.3-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (222 kB)\n",
      "Downloading yarl-1.20.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.6.0-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (305 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading joblib-1.5.0-py3-none-any.whl (307 kB)\n",
      "Downloading propcache-0.3.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (228 kB)\n",
      "Downloading pyarrow-20.0.0-cp313-cp313-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached PyYAML-6.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (759 kB)\n",
      "Downloading regex-2024.11.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (148 kB)\n",
      "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading scipy-1.15.3-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading setuptools-80.4.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached pandas-2.2.3-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading xxhash-3.5.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: pytz, nvidia-cusparselt-cu12, mpmath, xxhash, urllib3, tzdata, typing-extensions, tqdm, threadpoolctl, sympy, setuptools, safetensors, regex, pyyaml, pyarrow, propcache, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multidict, MarkupSafe, joblib, idna, hf-xet, fsspec, frozenlist, filelock, dill, charset-normalizer, certifi, attrs, aiohappyeyeballs, yarl, triton, scipy, requests, pandas, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, multiprocess, jinja2, aiosignal, scikit-learn, nvidia-cusolver-cu12, huggingface_hub, aiohttp, torch, tokenizers, transformers, datasets, accelerate\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60/60\u001b[0m [accelerate]celerate]tasets]ers]ub]cu12]2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 accelerate-1.6.0 aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 attrs-25.3.0 certifi-2025.4.26 charset-normalizer-3.4.2 datasets-3.6.0 dill-0.3.8 filelock-3.18.0 frozenlist-1.6.0 fsspec-2025.3.0 hf-xet-1.1.1 huggingface_hub-0.31.1 idna-3.10 jinja2-3.1.6 joblib-1.5.0 mpmath-1.3.0 multidict-6.4.3 multiprocess-0.70.16 networkx-3.4.2 numpy-2.2.5 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 pandas-2.2.3 propcache-0.3.1 pyarrow-20.0.0 pytz-2025.2 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.5.3 scikit-learn-1.6.1 scipy-1.15.3 setuptools-80.4.0 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.21.1 torch-2.7.0 tqdm-4.67.1 transformers-4.51.3 triton-3.3.0 typing-extensions-4.13.2 tzdata-2025.2 urllib3-2.4.0 xxhash-3.5.0 yarl-1.20.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers datasets torch scikit-learn huggingface_hub[hf_xet] transformers[torch]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57326e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (2.2.5)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.3-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: scikit-learn in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (1.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.58.0-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (104 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.8-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from matplotlib) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.2.1-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading matplotlib-3.10.3-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading contourpy-1.3.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (322 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.58.0-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached kiwisolver-1.4.8-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "Downloading pillow-11.2.1-cp313-cp313-manylinux_2_28_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib, seaborn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [seaborn]m7/8\u001b[0m [seaborn]ib]\n",
      "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.0 kiwisolver-1.4.8 matplotlib-3.10.3 pillow-11.2.1 pyparsing-3.2.3 seaborn-0.13.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65bb95d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b82f97ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset de entrenamiento\n",
    "data = pd.read_csv('../../data/processed/train_preprocess_v1.csv')\n",
    "\n",
    "# Seleccionar las características y etiquetas\n",
    "X = data['statement']  # Texto de la afirmación\n",
    "y = data['label']  # Etiqueta de veracidad (0 para falso, 1 para verdadero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76b12996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# --- Tokenización con DistilBERT ---\n",
    "# Cargar el tokenizador de DistilBERT\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0592c80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizar las afirmaciones de entrenamiento y prueba\n",
    "train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Convertir a formato Dataset compatible con Hugging Face\n",
    "train_dataset = Dataset.from_dict({'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask'], 'label': y_train.tolist()})\n",
    "test_dataset = Dataset.from_dict({'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask'], 'label': y_test.tolist()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc58b7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate>=0.26.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85ff5302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "\n",
    "# Cargar el modelo DistilBERT para clasificación\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# Configuración de los parámetros de entrenamiento\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',          # Carpeta para guardar los resultados\n",
    "#     num_train_epochs=3,              # Número de épocas\n",
    "#     per_device_train_batch_size=8,   # Tamaño del batch de entrenamiento\n",
    "#     per_device_eval_batch_size=16,   # Tamaño del batch de evaluación\n",
    "#     warmup_steps=500,                # Número de pasos para el calentamiento\n",
    "#     weight_decay=0.01,               # Decaimiento de peso\n",
    "#     logging_dir='./logs',            # Carpeta para los logs\n",
    "#     logging_steps=10,                # Intervalo de logging\n",
    "# )\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Carpeta para guardar los resultados\n",
    "    num_train_epochs=1,              # Usamos 1 época para prueba rápida\n",
    "    per_device_train_batch_size=4,   # Tamaño de batch reducido\n",
    "    per_device_eval_batch_size=8,    # Tamaño de batch reducido\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbacdead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1567' max='1567' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1567/1567 35:38, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.656300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.633000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.630200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1567, training_loss=0.6402191449368008, metrics={'train_runtime': 2140.8003, 'train_samples_per_second': 2.926, 'train_steps_per_second': 0.732, 'total_flos': 829908252579840.0, 'train_loss': 0.6402191449368008, 'epoch': 1.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    # p.predictions es un numpy.ndarray de forma (batch_size, num_labels)\n",
    "    preds = np.argmax(p.predictions, axis=1)        # ahora preds es un array de 0s y 1s\n",
    "    labels = p.label_ids                            # etiquetas verdaderas\n",
    "    f1 = f1_score(labels, preds, average='macro')   # F1 macro\n",
    "    return {'f1': f1}\n",
    "\n",
    "\n",
    "\n",
    "# Iniciar el entrenamiento con Hugging Face Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "# Entrenar el modelo\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4f4a94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.33      0.41       923\n",
      "           1       0.71      0.84      0.77      1762\n",
      "\n",
      "    accuracy                           0.67      2685\n",
      "   macro avg       0.61      0.59      0.59      2685\n",
      "weighted avg       0.64      0.67      0.64      2685\n",
      "\n",
      "Macro F1: 0.5863277630992005\n"
     ]
    }
   ],
   "source": [
    "# Evaluar (aquí compute_metrics ya usará numpy.argmax)\n",
    "trainer.evaluate()\n",
    "\n",
    "# Predecir sobre test_dataset\n",
    "pred_out = trainer.predict(test_dataset)\n",
    "\n",
    "# Sacar las predicciones\n",
    "test_preds = np.argmax(pred_out.predictions, axis=1)\n",
    "\n",
    "# Reporte completo de métricas\n",
    "print(classification_report(y_test, test_preds))\n",
    "print(\"Macro F1:\", f1_score(y_test, test_preds, average='macro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b645ac6",
   "metadata": {},
   "source": [
    "# 3. Entrega Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fae77fe",
   "metadata": {},
   "source": [
    "## Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1443660a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo de submission 'randomforest_2025-05-13_00-23-50.csv' generado correctamente.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "# Cargar el dataset de test\n",
    "df_test = pd.read_csv('../../data/processed/test_preprocess_v1.csv')  # Asegúrate de usar la ruta correcta\n",
    "\n",
    "# --- Preprocesamiento ---\n",
    "# 1. Asegurarse de que el texto esté preprocesado de la misma manera que en el entrenamiento:\n",
    "#    Esto incluye la vectorización TF-IDF.\n",
    "\n",
    "# Cargar el vectorizador TF-IDF entrenado previamente\n",
    "tfidf_vectorizer = joblib.load('tfidf_vectorizer.joblib')\n",
    "\n",
    "# Aplicar la transformación de texto en los datos de test\n",
    "X_test_tfidf = tfidf_vectorizer.transform(df_test['statement'])\n",
    "\n",
    "# --- Predicción ---\n",
    "# Cargar el modelo Random Forest entrenado\n",
    "rf_model = joblib.load('random_forest_model.joblib')\n",
    "\n",
    "# Realizar las predicciones en los datos de test\n",
    "y_test_pred = rf_model.predict(X_test_tfidf)\n",
    "\n",
    "# --- Exportar Submission ---\n",
    "# Crear el archivo CSV con las columnas requeridas: 'id' y 'label'\n",
    "submission = pd.DataFrame({\n",
    "    'id': df_test['id'],  # Usar el 'id' del dataset de prueba\n",
    "    'label': y_test_pred  # Las predicciones del modelo\n",
    "})\n",
    "\n",
    "# Obtener la fecha actual en formato 'YYYY-MM-DD'\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Generar el nombre del archivo con la fecha actual\n",
    "filename = f'randomforest_{current_date}.csv'\n",
    "\n",
    "# Guardar el archivo con el nombre que incluye la fecha\n",
    "submission.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Archivo de submission '{filename}' generado correctamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46635b9",
   "metadata": {},
   "source": [
    "## Disbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182df94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import Trainer, TrainingArguments, EvalPrediction\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el dataset de test desde Kaggle (ruta del archivo que hayas cargado)\n",
    "df_test = pd.read_csv('../../data/processed/test_preprocess_v1.csv')  # Asegúrate de usar la ruta correcta\n",
    "\n",
    "# --- Preprocesamiento ---\n",
    "# Cargar el tokenizador DistilBERT directamente desde Hugging Face\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenizar las afirmaciones de test\n",
    "test_encodings = tokenizer(list(df_test['statement']), truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Convertir a formato Dataset compatible con Hugging Face\n",
    "test_dataset = Dataset.from_dict({\n",
    "    'input_ids': test_encodings['input_ids'],\n",
    "    'attention_mask': test_encodings['attention_mask'],\n",
    "    'label': df_test['label'].tolist()  # Asume que df_test tiene la columna 'label'\n",
    "})\n",
    "\n",
    "# Cargar el modelo DistilBERT desde Hugging Face (sin necesidad de archivos previamente guardados)\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# Configuración del entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',  # Carpeta para guardar los resultados\n",
    "    num_train_epochs=1,      # Usamos 1 época para prueba rápida\n",
    "    per_device_train_batch_size=4,  # Tamaño de batch reducido\n",
    "    per_device_eval_batch_size=8,   # Tamaño de batch reducido\n",
    ")\n",
    "\n",
    "# Función de evaluación (cálculo de F1-score)\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    predictions, labels = p\n",
    "    preds = np.argmax(predictions, axis=1)  # Convertir predicciones a etiquetas\n",
    "    f1 = f1_score(labels, preds, average='macro')  # F1-score (macro)\n",
    "    return {'f1': f1}\n",
    "\n",
    "# Iniciar el entrenamiento con Hugging Face Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=test_dataset,  # Usar dataset de test como ejemplo para la predicción\n",
    "    eval_dataset=test_dataset,   # Igual para evaluación\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Entrenar el modelo (aunque solo sea por 1 época)\n",
    "trainer.train()\n",
    "\n",
    "# Evaluar el modelo (esto calculará las métricas de evaluación automáticamente)\n",
    "trainer.evaluate()\n",
    "\n",
    "# Realizar predicciones sobre test_dataset\n",
    "pred_out = trainer.predict(test_dataset)\n",
    "\n",
    "# Convertir las predicciones a etiquetas (tensores de PyTorch a numpy arrays)\n",
    "test_preds = np.argmax(pred_out.predictions, axis=1)\n",
    "\n",
    "# Reporte de métricas\n",
    "print(classification_report(df_test['label'], test_preds))\n",
    "print(\"Macro F1:\", f1_score(df_test['label'], test_preds, average='macro'))\n",
    "\n",
    "# --- Exportar Submission ---\n",
    "# Crear el archivo CSV con las columnas requeridas: 'id' y 'label'\n",
    "submission = pd.DataFrame({\n",
    "    'id': df_test['id'],  # Usar el 'id' del dataset de prueba\n",
    "    'label': test_preds  # Las predicciones del modelo\n",
    "})\n",
    "\n",
    "# Obtener la fecha actual en formato 'YYYY-MM-DD'\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Generar el nombre del archivo con la fecha actual\n",
    "filename = f'distilbert_{current_date}.csv'\n",
    "\n",
    "# Guardar el archivo con el nombre que incluye la fecha\n",
    "submission.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Archivo de submission '{filename}' generado correctamente.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
