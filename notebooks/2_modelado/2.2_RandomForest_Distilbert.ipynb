{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a01de36f",
   "metadata": {},
   "source": [
    "# 0. Instalación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26dcb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn pandas xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8814e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install imblearn SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6b2d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers datasets torch scikit-learn huggingface_hub[hf_xet] transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269940d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas numpy matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da294391",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a011b9",
   "metadata": {},
   "source": [
    "# 1. Modelos de Árboles de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e636f55",
   "metadata": {},
   "source": [
    "## 1.1 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca069f8c",
   "metadata": {},
   "source": [
    "**Descripción**\n",
    "\n",
    "Random Forest es un modelo de aprendizaje supervisado basado en árboles de decisión que emplea el principio de ensamble para mejorar la precisión y la robustez de las predicciones. Este enfoque se utiliza principalmente en datos tabulares y es una de las técnicas más populares y eficientes en tareas de clasificación y regresión.\n",
    "\n",
    "Además, este modelo pertenece a la categoría de modelos de tipo bagging ya que emplea una combinación de múltiples árboles de decisión entrenados de manera independiente y al final se elige la solución mayoritaria o el promedio de las predicciones. Por lo que Random Forest al permitir emplear múltiples árboles de decisión entrenados de distinta manera, permitiendo reducir el sobreajuste y mejorar la generalización.\n",
    "\n",
    "**Implementación**\n",
    "\n",
    "En este proyecto vamos a emplear el primer preprocesado realizado. A continuación para balancear los datos, después de haber realizado un análisis con SMOTE y una técnica de submuestreo para balancear el conjunto de los datos. Dinalmente, se ha llegado a la conlcusión de no emplear ninguna estas técnicas, ya que empeoraban las métricas, por lo que nos hemos decantado en unicamente entrenar los modelos con los datos preprocesados. Se ha dejado un apartado con los resultados obtenidos con SMOTE, el submuestreo y además el empleo del Grid Search Vector, que nos empeoraba también las métricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad907754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12cbcfc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>statement</th>\n",
       "      <th>subject</th>\n",
       "      <th>speaker</th>\n",
       "      <th>speaker_job</th>\n",
       "      <th>state_info</th>\n",
       "      <th>party_affiliation</th>\n",
       "      <th>party_affiliation_uni</th>\n",
       "      <th>party_affiliation_category_map</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_info_without_stopwords</th>\n",
       "      <th>pos_freq_without_stopwords</th>\n",
       "      <th>lemma_freq_without_stopwords</th>\n",
       "      <th>tag_freq_without_stopwords</th>\n",
       "      <th>processed_subject</th>\n",
       "      <th>speaker_entities</th>\n",
       "      <th>speaker_type</th>\n",
       "      <th>speaker_job_tokens</th>\n",
       "      <th>state_info_tokens</th>\n",
       "      <th>party_affiliation_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81f884c64a7</td>\n",
       "      <td>1</td>\n",
       "      <td>china is in the south china sea and (building)...</td>\n",
       "      <td>china,foreign-policy,military</td>\n",
       "      <td>donald-trump</td>\n",
       "      <td>president-elect</td>\n",
       "      <td>new york</td>\n",
       "      <td>republican</td>\n",
       "      <td>republican</td>\n",
       "      <td>political-affiliation</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'lemma': 'china', 'pos': 'PROPN', 'tag': 'NN...</td>\n",
       "      <td>Counter({'PROPN': 4, 'NOUN': 4, 'ADJ': 1, 'VER...</td>\n",
       "      <td>Counter({'china': 2, 'south': 1, 'sea': 1, 'bu...</td>\n",
       "      <td>Counter({'NNP': 4, 'NN': 3, 'JJ': 1, 'NNS': 1,...</td>\n",
       "      <td>['china', 'foreign-policy', 'military']</td>\n",
       "      <td>['donald trump']</td>\n",
       "      <td>['PERSON']</td>\n",
       "      <td>['president', '-', 'elect']</td>\n",
       "      <td>['new', 'york']</td>\n",
       "      <td>['republican']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30c2723a188</td>\n",
       "      <td>0</td>\n",
       "      <td>with the resources it takes to execute just ov...</td>\n",
       "      <td>health-care</td>\n",
       "      <td>chris-dodd</td>\n",
       "      <td>u.s. senator</td>\n",
       "      <td>connecticut</td>\n",
       "      <td>democrat</td>\n",
       "      <td>democrat</td>\n",
       "      <td>political-affiliation</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'lemma': 'resource', 'pos': 'NOUN', 'tag': '...</td>\n",
       "      <td>Counter({'NOUN': 7, 'VERB': 4, 'PROPN': 2, 'AD...</td>\n",
       "      <td>Counter({'resource': 1, 'take': 1, 'execute': ...</td>\n",
       "      <td>Counter({'NN': 4, 'NNS': 3, 'VB': 2, 'NNP': 2,...</td>\n",
       "      <td>['health-care']</td>\n",
       "      <td>['chris dodd']</td>\n",
       "      <td>['PERSON']</td>\n",
       "      <td>['u.s', '.', 'senator']</td>\n",
       "      <td>['connecticut']</td>\n",
       "      <td>['democrat']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6936b216e5d</td>\n",
       "      <td>0</td>\n",
       "      <td>the (wisconsin) governor has proposed tax give...</td>\n",
       "      <td>corporations,pundits,taxes,abc-news-week</td>\n",
       "      <td>donna-brazile</td>\n",
       "      <td>political commentator</td>\n",
       "      <td>washington, d.c.</td>\n",
       "      <td>democrat</td>\n",
       "      <td>democrat</td>\n",
       "      <td>political-affiliation</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'lemma': 'wisconsin', 'pos': 'PROPN', 'tag':...</td>\n",
       "      <td>Counter({'NOUN': 4, 'PROPN': 1, 'VERB': 1})</td>\n",
       "      <td>Counter({'wisconsin': 1, 'governor': 1, 'propo...</td>\n",
       "      <td>Counter({'NN': 2, 'NNS': 2, 'NNP': 1, 'VBN': 1})</td>\n",
       "      <td>['corporations', 'pundits', 'taxes', 'abc-news...</td>\n",
       "      <td>['donna brazile']</td>\n",
       "      <td>['PERSON']</td>\n",
       "      <td>['political', 'commentator']</td>\n",
       "      <td>['washington', ',', 'd.c', '.']</td>\n",
       "      <td>['democrat']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b5cd9195738</td>\n",
       "      <td>1</td>\n",
       "      <td>says her representation of an ex-boyfriend who...</td>\n",
       "      <td>candidates-biography,children,ethics,families,...</td>\n",
       "      <td>rebecca-bradley</td>\n",
       "      <td>non-define</td>\n",
       "      <td>non-define</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>other-political-groups</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'lemma': 'say', 'pos': 'VERB', 'tag': 'VBZ',...</td>\n",
       "      <td>Counter({'NOUN': 9, 'VERB': 1, 'ADJ': 1})</td>\n",
       "      <td>Counter({'say': 1, 'representation': 1, 'ex': ...</td>\n",
       "      <td>Counter({'NN': 8, 'VBZ': 1, 'NNS': 1, 'JJ': 1})</td>\n",
       "      <td>['candidates-biography', 'children', 'ethics',...</td>\n",
       "      <td>['rebecca bradley']</td>\n",
       "      <td>['PERSON']</td>\n",
       "      <td>['non', '-', 'define']</td>\n",
       "      <td>['non', '-', 'define']</td>\n",
       "      <td>['none']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84f8dac7737</td>\n",
       "      <td>0</td>\n",
       "      <td>at protests in wisconsin against proposed coll...</td>\n",
       "      <td>health-care,labor,state-budget</td>\n",
       "      <td>republican-party-wisconsin</td>\n",
       "      <td>non-define</td>\n",
       "      <td>wisconsin</td>\n",
       "      <td>republican</td>\n",
       "      <td>republican</td>\n",
       "      <td>political-affiliation</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'lemma': 'protest', 'pos': 'NOUN', 'tag': 'N...</td>\n",
       "      <td>Counter({'NOUN': 7, 'VERB': 4, 'ADJ': 3})</td>\n",
       "      <td>Counter({'protest': 1, 'wisconsin': 1, 'propos...</td>\n",
       "      <td>Counter({'NNS': 4, 'NN': 3, 'JJ': 3, 'VBN': 2,...</td>\n",
       "      <td>['health-care', 'labor', 'state-budget']</td>\n",
       "      <td>['republican party', 'wisconsin']</td>\n",
       "      <td>['ORG', 'GPE']</td>\n",
       "      <td>['non', '-', 'define']</td>\n",
       "      <td>['wisconsin']</td>\n",
       "      <td>['republican']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  label                                          statement  \\\n",
       "0  81f884c64a7      1  china is in the south china sea and (building)...   \n",
       "1  30c2723a188      0  with the resources it takes to execute just ov...   \n",
       "2  6936b216e5d      0  the (wisconsin) governor has proposed tax give...   \n",
       "3  b5cd9195738      1  says her representation of an ex-boyfriend who...   \n",
       "4  84f8dac7737      0  at protests in wisconsin against proposed coll...   \n",
       "\n",
       "                                             subject  \\\n",
       "0                      china,foreign-policy,military   \n",
       "1                                        health-care   \n",
       "2           corporations,pundits,taxes,abc-news-week   \n",
       "3  candidates-biography,children,ethics,families,...   \n",
       "4                     health-care,labor,state-budget   \n",
       "\n",
       "                      speaker            speaker_job        state_info  \\\n",
       "0                donald-trump        president-elect          new york   \n",
       "1                  chris-dodd           u.s. senator       connecticut   \n",
       "2               donna-brazile  political commentator  washington, d.c.   \n",
       "3             rebecca-bradley             non-define        non-define   \n",
       "4  republican-party-wisconsin             non-define         wisconsin   \n",
       "\n",
       "  party_affiliation party_affiliation_uni party_affiliation_category_map  ...  \\\n",
       "0        republican            republican          political-affiliation  ...   \n",
       "1          democrat              democrat          political-affiliation  ...   \n",
       "2          democrat              democrat          political-affiliation  ...   \n",
       "3              none                  none         other-political-groups  ...   \n",
       "4        republican            republican          political-affiliation  ...   \n",
       "\n",
       "                          pos_info_without_stopwords  \\\n",
       "0  [{'lemma': 'china', 'pos': 'PROPN', 'tag': 'NN...   \n",
       "1  [{'lemma': 'resource', 'pos': 'NOUN', 'tag': '...   \n",
       "2  [{'lemma': 'wisconsin', 'pos': 'PROPN', 'tag':...   \n",
       "3  [{'lemma': 'say', 'pos': 'VERB', 'tag': 'VBZ',...   \n",
       "4  [{'lemma': 'protest', 'pos': 'NOUN', 'tag': 'N...   \n",
       "\n",
       "                          pos_freq_without_stopwords  \\\n",
       "0  Counter({'PROPN': 4, 'NOUN': 4, 'ADJ': 1, 'VER...   \n",
       "1  Counter({'NOUN': 7, 'VERB': 4, 'PROPN': 2, 'AD...   \n",
       "2        Counter({'NOUN': 4, 'PROPN': 1, 'VERB': 1})   \n",
       "3          Counter({'NOUN': 9, 'VERB': 1, 'ADJ': 1})   \n",
       "4          Counter({'NOUN': 7, 'VERB': 4, 'ADJ': 3})   \n",
       "\n",
       "                        lemma_freq_without_stopwords  \\\n",
       "0  Counter({'china': 2, 'south': 1, 'sea': 1, 'bu...   \n",
       "1  Counter({'resource': 1, 'take': 1, 'execute': ...   \n",
       "2  Counter({'wisconsin': 1, 'governor': 1, 'propo...   \n",
       "3  Counter({'say': 1, 'representation': 1, 'ex': ...   \n",
       "4  Counter({'protest': 1, 'wisconsin': 1, 'propos...   \n",
       "\n",
       "                          tag_freq_without_stopwords  \\\n",
       "0  Counter({'NNP': 4, 'NN': 3, 'JJ': 1, 'NNS': 1,...   \n",
       "1  Counter({'NN': 4, 'NNS': 3, 'VB': 2, 'NNP': 2,...   \n",
       "2   Counter({'NN': 2, 'NNS': 2, 'NNP': 1, 'VBN': 1})   \n",
       "3    Counter({'NN': 8, 'VBZ': 1, 'NNS': 1, 'JJ': 1})   \n",
       "4  Counter({'NNS': 4, 'NN': 3, 'JJ': 3, 'VBN': 2,...   \n",
       "\n",
       "                                   processed_subject  \\\n",
       "0            ['china', 'foreign-policy', 'military']   \n",
       "1                                    ['health-care']   \n",
       "2  ['corporations', 'pundits', 'taxes', 'abc-news...   \n",
       "3  ['candidates-biography', 'children', 'ethics',...   \n",
       "4           ['health-care', 'labor', 'state-budget']   \n",
       "\n",
       "                    speaker_entities    speaker_type  \\\n",
       "0                   ['donald trump']      ['PERSON']   \n",
       "1                     ['chris dodd']      ['PERSON']   \n",
       "2                  ['donna brazile']      ['PERSON']   \n",
       "3                ['rebecca bradley']      ['PERSON']   \n",
       "4  ['republican party', 'wisconsin']  ['ORG', 'GPE']   \n",
       "\n",
       "             speaker_job_tokens                state_info_tokens  \\\n",
       "0   ['president', '-', 'elect']                  ['new', 'york']   \n",
       "1       ['u.s', '.', 'senator']                  ['connecticut']   \n",
       "2  ['political', 'commentator']  ['washington', ',', 'd.c', '.']   \n",
       "3        ['non', '-', 'define']           ['non', '-', 'define']   \n",
       "4        ['non', '-', 'define']                    ['wisconsin']   \n",
       "\n",
       "  party_affiliation_tokens  \n",
       "0           ['republican']  \n",
       "1             ['democrat']  \n",
       "2             ['democrat']  \n",
       "3                 ['none']  \n",
       "4           ['republican']  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../../data/processed/train_preprocess_v1.csv')\n",
    "#data = pd.read_csv('../../data/processed/train_limpieza_v1.csv')\n",
    "data.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c4118e",
   "metadata": {},
   "source": [
    "Con esos datos preprocesados, seleccionamos que columnas no van a participar en el proceso de entrenamieto. Se ha decidido no usar esas columnas, ya que de todas las características analizadas, eran las que menos información podían aportar a la hora de decidir si estamos ante una *fake news* o no.\n",
    "\n",
    "En una primera idea se realizó el entrenamiento con todas las columnas, pese al mal funcionamiento de los modelos de árboles de decisión, se cambió la estrategiay se emplea esta estrategia de seleccionar solo las columnas que aportan información importante y no es redundadnte.\n",
    "\n",
    "Además, se aplicó la vectorización TF-IDF, permitiendo una representación númerica desde texto, limitando a 5000 las palabras más relevantes del inglés y eliminando palabras que no aportan información relevante. De esta forma se captura la importancia de cada término. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e282f357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar la noticia y la etiqueta de veracidad de si es fake new\n",
    "X = data['statement']  \n",
    "y = data['label']  \n",
    "\n",
    "# Dividir los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Vectorización de texto utilizando TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3a184a",
   "metadata": {},
   "source": [
    "**Configuración de hiperparámetros de Random Forest y entrenamiento**\n",
    "\n",
    "Para llevar a cabo la configuración de los hiperparámetros hemos tenido en cuenta lo la documentación del modelo. Para el criterio de división empleado, se utiliza Gini ya que proporciona una solución rápida y eficiente en la construcción de los árboles. Además, en cuanto al número de estimadores y la profundidad máxima, es decir, como de grande va a ser nuestro bosque y como de profundo va a ser cada árbol se ha optado por valores más altos. El número de estimadores es elevado, ya que como cada árbol de forma individual tiene que dar su valoración, si se poseen gran cantidad de árboles, el error individual promedio disminuye, y por lo tanto, es más robusto pero tiene mayor consumo computacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aaa95cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Entrenamiento del modelo Random Forest\n",
    "class_weights = {0: 5, 1: 2}  \n",
    "rf_model = RandomForestClassifier(\n",
    "    criterion='gini',\n",
    "    n_estimators=500,\n",
    "    max_depth=80,\n",
    "    max_features='sqrt',        \n",
    "    class_weight=class_weights, \n",
    "    random_state=42\n",
    ")\n",
    "rf_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Realizar predicciones\n",
    "y_pred = rf_model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b998e7",
   "metadata": {},
   "source": [
    "**Evaluación**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "172bcef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.66      0.51       923\n",
      "           1       0.74      0.52      0.61      1762\n",
      "\n",
      "    accuracy                           0.57      2685\n",
      "   macro avg       0.58      0.59      0.56      2685\n",
      "weighted avg       0.63      0.57      0.58      2685\n",
      "\n",
      "Macro Average F1-Score: 0.5604\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo con Macro Average F1-Score\n",
    "# Reporte completo de métricas\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(f\"Macro Average F1-Score: {macro_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf08fb6",
   "metadata": {},
   "source": [
    "Finalmente, guardamos el vectorizador tfidf, empleando la biblioteca joblib. De esta manera, al guardarlo, nos permite poder reutilizarlos posteriamente, ya sea para un reentreno o para poder realizar la submissión de kaggle con los datos para probar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b36533f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.joblib']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "# Guardar el vectorizador TF-IDF\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d770114",
   "metadata": {},
   "source": [
    "A cotninuación se proporcionan 3 planteamientos diferentes\n",
    "1. Utilizando columnas específicas\n",
    "2.  Aplicando smote\n",
    "3. Submuestreo\n",
    "con los cuales no se han obtenido resultados favorables, por no se entra en profundidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a658ee3",
   "metadata": {},
   "source": [
    "### 1.1.2 Random forest solo con columnas determinadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1354d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.67      0.51       923\n",
      "           1       0.74      0.50      0.60      1762\n",
      "\n",
      "    accuracy                           0.56      2685\n",
      "   macro avg       0.58      0.59      0.56      2685\n",
      "weighted avg       0.63      0.56      0.57      2685\n",
      "\n",
      "Macro Average F1-Score: 0.5557\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar el dataset\n",
    "data = pd.read_csv('../../data/processed/train_preprocess_v1.csv')\n",
    "\n",
    "# Seleccionar características relevantes\n",
    "X = data[['statement', 'subject', 'party_affiliation', 'speaker', 'speaker_job', 'state_info', 'party_affiliation_uni']]\n",
    "y = data['label']  \n",
    "\n",
    "# División de los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Vectorización del texto mediante TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['statement'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['statement'])\n",
    "\n",
    "# Entrenamiento del modelo \n",
    "class_weights = {0: 5, 1: 2}  # No modificar los pesos, ya que empeora el modelo\n",
    "rf_model = RandomForestClassifier(\n",
    "    criterion='gini',\n",
    "    n_estimators=500,\n",
    "    max_depth=80,\n",
    "    max_features='sqrt',        \n",
    "    class_weight=class_weights, \n",
    "    random_state=42\n",
    ")\n",
    "rf_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predicciones para evaluar el rendimiento del modelo\n",
    "y_pred = rf_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluar el modelo con Macro Average F1-Score\n",
    "# Reporte completo de métricas\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(f\"Macro Average F1-Score: {macro_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd64e3d0",
   "metadata": {},
   "source": [
    "sin usar statement con one hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64928998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.65      0.49       923\n",
      "           1       0.72      0.48      0.57      1762\n",
      "\n",
      "    accuracy                           0.54      2685\n",
      "   macro avg       0.56      0.56      0.53      2685\n",
      "weighted avg       0.61      0.54      0.54      2685\n",
      "\n",
      "Macro Average F1-Score: 0.5318\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el dataset\n",
    "data = pd.read_csv('../../data/processed/train_preprocess_v1.csv')\n",
    "\n",
    "# Seleccionar las características relevantes (sin la columna 'statement') y la etiqueta\n",
    "X = data[['subject', 'speaker', 'speaker_job', 'state_info', 'party_affiliation', 'party_affiliation_uni']]  # Columnas relevantes\n",
    "y = data['label']  # Etiqueta de veracidad\n",
    "\n",
    "# Realizar el OneHotEncoder para las columnas categóricas\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "\n",
    "# Aplicar One-Hot Encoding a las columnas categóricas\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Entrenamiento del modelo Random Forest\n",
    "class_weights = {0: 5, 1: 2}  # No modificar los pesos, ya que empeora el modelo\n",
    "rf_model = RandomForestClassifier(\n",
    "    criterion='gini',\n",
    "    n_estimators=500,\n",
    "    max_depth=80,\n",
    "    max_features='sqrt',  # 'max_features': ['sqrt', 'log2', None],\n",
    "    class_weight=class_weights,  # 'balanced'\n",
    "    random_state=42\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo con Macro Average F1-Score\n",
    "# Reporte completo de métricas\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(f\"Macro Average F1-Score: {macro_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff0583d",
   "metadata": {},
   "source": [
    "### 1.1.3 Aplico Smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757394f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.80      0.52       923\n",
      "           1       0.76      0.33      0.46      1762\n",
      "\n",
      "    accuracy                           0.49      2685\n",
      "   macro avg       0.57      0.57      0.49      2685\n",
      "weighted avg       0.63      0.49      0.48      2685\n",
      "\n",
      "Macro Average F1-Score: 0.4915\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE  \n",
    "\n",
    "# Cargar del dataset\n",
    "data = pd.read_csv('../../data/processed/train_preprocess_v1.csv')\n",
    "\n",
    "# Seleccionar las características \n",
    "X = data[['statement', 'subject', 'party_affiliation', 'speaker', 'speaker_job', 'state_info', 'party_affiliation_uni']] \n",
    "y = data['label']  \n",
    "\n",
    "# Dividimos los datos \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Vectorización del texto con TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['statement'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['statement'])\n",
    "\n",
    "# Aplicamos SMOTE ÚNICAMENTE AL CONJUNTO DE ENTRENAMIENTO!!! (al de prueba no)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_tfidf, y_train)\n",
    "\n",
    "# Entrenamiento del modelo \n",
    "class_weights = {0: 5, 1: 2}  # No modificar los pesos, ya que empeora el modelo\n",
    "rf_model = RandomForestClassifier(\n",
    "    criterion='gini',\n",
    "    n_estimators=500,\n",
    "    max_depth=80,\n",
    "    max_features='sqrt',        # 'max_features': ['sqrt', 'log2', None],\n",
    "    class_weight=class_weights, #'balanced'\n",
    "    random_state=42\n",
    ")\n",
    "rf_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = rf_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluar el modelo con Macro Average F1-Score\n",
    "# Reporte completo de métricas\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(f\"Macro Average F1-Score: {macro_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283cf3cb",
   "metadata": {},
   "source": [
    "### 1.1.4 Aplico la técnica del submuestreo para el balanceo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730d7cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.93      0.50       923\n",
      "           1       0.67      0.07      0.13      1762\n",
      "\n",
      "    accuracy                           0.37      2685\n",
      "   macro avg       0.51      0.50      0.32      2685\n",
      "weighted avg       0.56      0.37      0.26      2685\n",
      "\n",
      "Macro Average F1-Score: 0.3178\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Carga del dataset\n",
    "data = pd.read_csv('../../data/processed/train_preprocess_v1.csv')\n",
    "\n",
    "# Seleccionar las características \n",
    "X = data[['statement', 'subject', 'party_affiliation', 'speaker', 'speaker_job', 'state_info', 'party_affiliation_uni']] \n",
    "y = data['label'] \n",
    "\n",
    "# Dividimos los datos \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Vectorización del texto utilizando TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['statement'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['statement'])\n",
    "\n",
    "# Submuestreo del conjunto de entrenamiento siendo la clase 0 la minotitaria y la clase 1 la mayoritaria\n",
    "# Vamos a realizar un submuestreo aleatorio igualando el número de ejemplos de ambas clases\n",
    "df_train = pd.DataFrame(X_train_tfidf.toarray())  \n",
    "df_train['label'] = y_train\n",
    "df_train_class_0 = df_train[df_train['label'] == 0]\n",
    "df_train_class_1 = df_train[df_train['label'] == 1]\n",
    "n_minority = len(df_train_class_0)\n",
    "df_train_class_1_under = df_train_class_1.sample(n=n_minority, random_state=42)\n",
    "\n",
    "# Combinamos ambas clases \n",
    "df_train_balanced = pd.concat([df_train_class_0, df_train_class_1_under]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Separar las características y etiquetas después del submuestreo\n",
    "X_train_balanced = df_train_balanced.drop(columns=['label'])\n",
    "y_train_balanced = df_train_balanced['label']\n",
    "\n",
    "# Entrenamiento\n",
    "class_weights = {0: 5, 1: 2}  # No modificar los pesos, ya que empeora el modelo\n",
    "rf_model = RandomForestClassifier(\n",
    "    criterion='gini',\n",
    "    n_estimators=500,\n",
    "    max_depth=80,\n",
    "    max_features='sqrt',        # 'max_features': ['sqrt', 'log2', None],\n",
    "    class_weight=class_weights, #'balanced'\n",
    "    random_state=42\n",
    ")\n",
    "rf_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = rf_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluar el modelo con Macro Average F1-Score\n",
    "# Reporte completo de métricas\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(f\"Macro Average F1-Score: {macro_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6357e4eb",
   "metadata": {},
   "source": [
    "## 1.2 XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61d9f24",
   "metadata": {},
   "source": [
    "**Descripción**\n",
    "\n",
    "Este modelo también se trata de un algoritmo de aprendizaje supervisado basado en el enfoque de boosting, empleando tamién para las tareas de clsificación y regresión. A diferencia del Random Forest, XGBoost, se encarga de construir los árboles de manera secuencial, y cada árbol va a intentar corregir los errores producidos por el árbol precedente.\n",
    "\n",
    "**Implementación**\n",
    "\n",
    "La implementación es muy similar a la utilizada en el Random Forest, a diferencia de la configuracion de los hiperparámetros.\n",
    "\n",
    "Para la configuración, en el caso de XGBoost, el número de estimadores y la profundidad máxima de los árboles se configuraron de manera similar a los valores de Random Forest, siguiendo las recomendaciones de la documentación de XGBoost. Además de estos hiperparámetros, se utilizó la métrica logloss para la evaluación. También se ha configurado la tasa de aprendizaje del modelo es baja, de tal manera que va a permitir que vaya aprendiendo de forma gradual y sea más generalizable, evitando tender al sobreajuste, y en cada árbol al usar datos con tantas características, se va a emplear un 60% de estas distribuidas de forma aleatoria.\n",
    "Además, otra cosa a destacar es que vamos a codificar las variables categóricas para que se pueda tratar sin problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cfc5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.46      0.44       923\n",
      "           1       0.71      0.68      0.69      1762\n",
      "\n",
      "    accuracy                           0.60      2685\n",
      "   macro avg       0.57      0.57      0.57      2685\n",
      "weighted avg       0.61      0.60      0.61      2685\n",
      "\n",
      "Macro F1-Score: 0.5670\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Carga del dataset\n",
    "data = pd.read_csv('../../data/processed/train_preprocess_v1.csv')\n",
    "\n",
    "# Seleccionar las columnas relevantes \n",
    "X = data[['subject', 'party_affiliation', 'speaker', 'speaker_job', 'state_info', 'party_affiliation_uni']] \n",
    "y = data['label']  \n",
    "\n",
    "# División de los datos \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Codificación de las variables categóricas\n",
    "X_train_encoded = pd.get_dummies(X_train, drop_first=True)\n",
    "X_test_encoded = pd.get_dummies(X_test, drop_first=True)\n",
    "\n",
    "# Nos asrguramos que ambas columnas son del mismo tamaño\n",
    "X_train_encoded, X_test_encoded = X_train_encoded.align(X_test_encoded, join='left', axis=1)\n",
    "\n",
    "# Vectorización del texto mediante TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(data['statement'][X_train.index])  \n",
    "X_test_tfidf = tfidf_vectorizer.transform(data['statement'][X_test.index]) \n",
    "\n",
    "# Concatenamos las características de texto mediante TF-IDF y las características categóricas\n",
    "X_train_combined = np.hstack([X_train_tfidf.toarray(), X_train_encoded.values])\n",
    "X_test_combined = np.hstack([X_test_tfidf.toarray(), X_test_encoded.values])\n",
    "\n",
    "# Entrenamos el modelo\n",
    "sample_weights = y_train.map({0: 5, 1: 2}) \n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=80,\n",
    "    learning_rate=0.05,\n",
    "    eval_metric='logloss',\n",
    "    colsample_bytree=0.6,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train_combined, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# Prediccion\n",
    "y_pred = xgb_model.predict(X_test_combined)\n",
    "\n",
    "# Evaluar el modelo con Macro Average F1-Score\n",
    "print(\"Reporte de Clasificación:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Calcular el Macro F1-Score\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(f\"Macro F1-Score: {macro_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449a1305",
   "metadata": {},
   "source": [
    "Nos guardamos el vectorizador tfidf y el modelo entrenado para facilitar el reentrenamiento y la predicción del conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c36491c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgb_model.joblib']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(X_train_encoded.columns, 'train_cat_columns.joblib')\n",
    "\n",
    "# Guardamos el vectorizador TF-IDF\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.joblib')\n",
    "\n",
    "# Guardamos ek modelo entrenado\n",
    "joblib.dump(xgb_model, 'xgb_model.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c64d1a4",
   "metadata": {},
   "source": [
    "## 1.3 Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5f977e",
   "metadata": {},
   "source": [
    "**Descripción del Ensemble: Voting Classifier**\n",
    "\n",
    "Finalmente, para terminar con los árboles de decisón, se ha llevado a cabo el entrenamiento de un ensemble que combina tanto el modelo Random Forest, como el modelo XGBoost, en búsqueda de que se mejore el entrenamiento, la precisión y la robustez del model; y por tanto las métricas. \n",
    "\n",
    "Para ello, vamos a emplear **Voting Classifier**. Esta técnica permite elegir entre un hard voting (donde cada modelo vota por una clase y se toma la mayoría) o un soft voting (donde se promedian las prbabilidades de cada clase y se selecciona aquella con mayor probabilidad final).\n",
    "\n",
    "**Implementación con Voting Classifier**\n",
    "\n",
    "La implementación es similar a los dos modelos anteriores entrenados con las configuraciones ya establecidas. Cabe resaltar que al final, nos hemos decantado por un *soft voting*, la cuál es menos agresiva y clasifica mejor los datos. Además, también vamos a realifzar tanto la codificación de variables categóricas como la vectorización TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf5bcac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.37      0.43       923\n",
      "           1       0.71      0.81      0.76      1762\n",
      "\n",
      "    accuracy                           0.66      2685\n",
      "   macro avg       0.61      0.59      0.59      2685\n",
      "weighted avg       0.64      0.66      0.64      2685\n",
      "\n",
      "Macro F1-Score: 0.5926\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Cargarmos el dataset\n",
    "data = pd.read_csv('../../data/processed/train_preprocess_v1.csv')\n",
    "\n",
    "# Seleccionar las columnas relevantes para el modelo\n",
    "X = data[['subject', 'party_affiliation', 'speaker', 'speaker_job', 'state_info', 'party_affiliation_uni']] \n",
    "y = data['label']  \n",
    "\n",
    "# Dividimos los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Codificamos las variables categóricas\n",
    "X_train_encoded = pd.get_dummies(X_train, drop_first=True)\n",
    "X_test_encoded = pd.get_dummies(X_test, drop_first=True)\n",
    "\n",
    "# Nos asegurarnos de que ambos conjuntos tengan las mismas columnas\n",
    "X_train_encoded, X_test_encoded = X_train_encoded.align(X_test_encoded, join='left', axis=1)\n",
    "\n",
    "# Vectorización del texto empleando TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(data['statement'][X_train.index])  # Usar solo las filas de X_train\n",
    "X_test_tfidf = tfidf_vectorizer.transform(data['statement'][X_test.index])  # Usar solo las filas de X_test\n",
    "\n",
    "# Concatenamos las características de texto mediante TF-IDF y las características categóricas\n",
    "X_train_combined = np.hstack([X_train_tfidf.toarray(), X_train_encoded.values])\n",
    "X_test_combined = np.hstack([X_test_tfidf.toarray(), X_test_encoded.values])\n",
    "\n",
    "# Configuraciones de los modelos\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=80,\n",
    "    learning_rate=0.05,\n",
    "    eval_metric='logloss',\n",
    "    colsample_bytree=0.6,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    criterion='gini',\n",
    "    n_estimators=500,\n",
    "    max_depth=80,\n",
    "    max_features='sqrt',\n",
    "    class_weight={0: 5, 1: 2},\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Combinamos de los modelos en un ensemble\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('xgb', xgb_model), ('rf', rf_model)],\n",
    "    voting='soft',  # 'soft' para promediar probabilidades\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Entrenamos el ensemble\n",
    "voting_clf.fit(X_train_combined, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = voting_clf.predict(X_test_combined)\n",
    "\n",
    "# Evaluar el modelo\n",
    "print(\"Reporte de Clasificación:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Calcular el Macro F1-Score\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(f\"Macro F1-Score: {macro_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3032a1b1",
   "metadata": {},
   "source": [
    "finalmente volvemos a guardarnos tanto el vectorizador TF-IDF como el modelo una vez entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7476c970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['voting_classifier_model.joblib']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "X_train_encoded = pd.get_dummies(X_train, drop_first=True)\n",
    "\n",
    "joblib.dump(X_train_encoded.columns, 'train_cat_columns.joblib')\n",
    "\n",
    "# Guardamos el vectorizador y el modelo \n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.joblib')\n",
    "joblib.dump(voting_clf, 'voting_classifier_model.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e863926b",
   "metadata": {},
   "source": [
    "## Conclusiones,\n",
    "Finalmente, con el modelo Voting Classifier, que combinar Random Forest y XGBoost, se obtiene un rendimiento más solido, obteniendo una *accuracy* del 0.66 y un Mcro F1-Score de 0.5926, siendo el más robusto entre los tres modelos evaluados. Este modelo muestra un buen equilibrio entre precisión y recall, especialmente para la clase mayoritaria (fake news dectectadas), con un recall del 81% y una precisión del 71%. Para la clase minoritaria (fake news no detectadas), el recall es disminuye hasta el 37%, y la precisión disminuye hasta un 51%, reflejando que, aunque detecta menos, la predicción de esta clase es adecuada. En conjunto, el ensemble logra un desempeño más robusto y equilibrado que cualquiera de los modelos individuales.\n",
    "\n",
    "El modelo XGBoost por sí solo presenta una precisión del 60% y un Macro F1-Score de 0.5670. Este modelo obtiene un recall del 68% para la clase mayoritaria y un 46% para la minoritaria, mostrando un mejor equilibrio que Random Forest, aunque con menor precisión para la clase minoritaria (43%).\n",
    "\n",
    "Por último, Random Forest presenta una precisión general más baja (57%) y un Macro F1-Score de 0.5604. Aunque tiene un buen recall para la clase minoritaria (66%), su recall para la clase mayoritaria es más limitado (52%), indicando una menor capacidad para clasificar correctamente las noticias.\n",
    "\n",
    "En resumen, el Voting Classifier combina las fortalezas de ambos modelos y consigue mejorar el equilibrio global en la clasificación, siendo el modelo más adecuado para este problema de clasificación de las diferentes fake news."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4969a80",
   "metadata": {},
   "source": [
    "## GridSearchCV para random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03eb086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Average F1-Score: 0.4070\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.01      0.02       923\n",
      "           1       0.66      0.99      0.79      1762\n",
      "\n",
      "    accuracy                           0.66      2685\n",
      "   macro avg       0.56      0.50      0.41      2685\n",
      "weighted avg       0.59      0.66      0.53      2685\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import joblib\n",
    "\n",
    "# Cargar el dataset de entrenamiento\n",
    "data = pd.read_csv('../../data/processed/train_preprocess_v1.csv')\n",
    "\n",
    "# Seleccionar las características y etiquetas\n",
    "X = data['statement']  \n",
    "y = data['label'] \n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Vectorización del texto utilizando TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Guardar el vectorizador para su uso posterior (si es necesario)\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.joblib')\n",
    "\n",
    "# Definir el modelo Random Forest\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Definir los parámetros a probar para el modelo\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "}\n",
    "\n",
    "# Realizar GridSearchCV para encontrar los mejores parámetros\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Obtener el mejor modelo después de la búsqueda de hiperparámetros\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Realizar predicciones con el mejor modelo\n",
    "y_test_pred = best_rf_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluar el modelo con el Macro Average F1-Score\n",
    "macro_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "print(f\"Macro Average F1-Score: {macro_f1:.4f}\")\n",
    "\n",
    "# Reporte completo de métricas\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecccb9f6",
   "metadata": {},
   "source": [
    "# 2. DistilBERT (fine-tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc35594",
   "metadata": {},
   "source": [
    "**Descripción**\n",
    "\n",
    "DistilBERT es una versión ligera y optimizada de BERT, un modelo de lenguaje basado en Transformers que permite realizar tareas de clasificación de texto con alta precisión. Este modelo se encarga de  captura las relaciones contextuales entre palabras en las frases, ofreciendo un rendimiento superior en tareas de procesamiento de lenguaje natural, como la clasificación de afirmaciones en verdadero o falso de fake news[1].\n",
    "\n",
    "**Implementación**\n",
    "\n",
    "En este proyecto, hemos trabajado con el dataset preprocesado y etiquetado. Para la tokenización y el entrenamiento hemos empleado el tokenizador y el modelo DistilBERT. El entrenamiento se realiza directamente sobre los datos preprocesados originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65bb95d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\afgdl.ASUSS15ANGEL\\Documents\\GitHub\\LBBYs_CH2\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b82f97ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el dataset de entrenamiento\n",
    "data = pd.read_csv('../../data/processed/train_preprocess_v1.csv')\n",
    "\n",
    "# Seleccionar las características\n",
    "X = data['statement'] \n",
    "y = data['label'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49262a96",
   "metadata": {},
   "source": [
    "Dividimos y tokenizamos con distilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76b12996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Cargar el tokenizador de DistilBERT\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d596345",
   "metadata": {},
   "source": [
    "Realizamos la tokenización de los discursos del conjunto de entrenamiento y probamos usando un tokenizador de Hugging Face, asegurando que cada texto tenga una longitud máxima de 512 tokens y aplicando relleno para uniformizar su tamaño. Luego, convertimos los datos tokenizados y las etiquetas en un  Dataset para entrenar y evaluar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0592c80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizar las afirmaciones de entrenamiento y prueba\n",
    "train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Convertir a formato Dataset compatible con Hugging Face\n",
    "train_dataset = Dataset.from_dict({'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask'], 'label': y_train.tolist()})\n",
    "test_dataset = Dataset.from_dict({'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask'], 'label': y_test.tolist()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03167f82",
   "metadata": {},
   "source": [
    "**Configuramos los parámetros**\n",
    "\n",
    "Inicialmente, se realizó un entrenamiento de prueba con una configuración más rápida, reduciendo el número de épocas a 1 y disminuyendo el tamaño de los batches, tamaño reducido de statements que se procesa a la vez para mejorar el uso de memoria y sea más eficiente, tanto para entrenamiento como para evaluación. Esta configuración permitió validar rápidamente el pipeline y asegurarse de que el modelo iba a permitir proporcionar buenos resultados.\n",
    "\n",
    "Sin embargo, aunque este entrenamiento rápido es útil para pruebas preliminares, resulta menos eficiente en términos de desempeño final del modelo. El número reducido de épocas limita la capacidad del modelo para aprender patrones complejos, y los batches pequeños pueden afectar la estabilidad y calidad del aprendizaje. Ya que \n",
    "- Batch pequeño: actualizaciones más frecuentes, pero es menos estable.\n",
    "\n",
    "- Batch grande: actualizaciones menos frecuentes, pero más estables y con mejor estimación del gradiente, aunque requiere de más memoria y por lo tanto mayor tiempo computacional.\n",
    "\n",
    "Por ello, en la configuración actual se incrementó el número de épocas a 3 y se utilizaron tamaños de batch mayores, permitiendo un entrenamiento más profundo y robusto. Esto mejora la capacidad del modelo para generalizar mejor a nuevos datos, aunque con un coste computacional elevado provocando tiempos de entrenamiento excesivamente largos, alcanzando las 4 horas de ejecuciación. Así, se busca un equilibrio óptimo entre precisión y recursos disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ff5302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "\n",
    "# Cargar el modelo DistilBERT para clasificación\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# Configuración de los parámetros de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    num_train_epochs=3,              \n",
    "    per_device_train_batch_size=8,   # batch de entrenamiento\n",
    "    per_device_eval_batch_size=16,   # batch de evaluación\n",
    "    warmup_steps=500,                # Número de pasos \n",
    "    weight_decay=0.01,               \n",
    "    logging_dir='./logs',            \n",
    "    logging_steps=10,                \n",
    ")\n",
    "\n",
    "# Confiugración de modelo más básica\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',          \n",
    "#     num_train_epochs=1,              # Usamos 1 época para prueba rápida\n",
    "#     per_device_train_batch_size=4,   # batch de entrenamiento\n",
    "#     per_device_eval_batch_size=8,    # batch de evaluación\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e1fc9f",
   "metadata": {},
   "source": [
    "Entrenamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bbacdead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\afgdl.ASUSS15ANGEL\\Documents\\GitHub\\LBBYs_CH2\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2352' max='2352' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2352/2352 10:26:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.708500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.710100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.716800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.693400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.687700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.686200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.661700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.712300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.657800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.693000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.661200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.666800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.645300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.618800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.641100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.653400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.613800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.624400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.654900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.680900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.631700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.614100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.573700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.635000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.679300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.571100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.594000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.727400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.616600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.645500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.610200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.625800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.614800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.587900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.637200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.534900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.651900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.677700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.613700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.622600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.647600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.569900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.662800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.630500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.680400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.589500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.696300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.635600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.681600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.599200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.581400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.660700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.660400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.643700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.643900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.656600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.614300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.593300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.654500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.678000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.632800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.668600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.606600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.683800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.670300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.629500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.617900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.475800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.604200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.763500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.655300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.668600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.577300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.651300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.669800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.647000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.626400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.576900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.582200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.655800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.519000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.570300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.655900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.568800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.609000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.600500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.513500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.613400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.463000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.655600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.665500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.607900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.579400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.619700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.617600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.609900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.622900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.552200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.586700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.635300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.632200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.556100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.530500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.584100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.678100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.645400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.616800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.584700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.532900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.650100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.601600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.606800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.511500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.548700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.452500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.647900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.657900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.514200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>0.549700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.529000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>0.617500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.575800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.606000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.630400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>0.557200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.571200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>0.528500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.628000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>0.563300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.511100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>0.584800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.665700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.573400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.598900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>0.584800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>0.530900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.682000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>0.544300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>0.594600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>0.567500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.566800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.467100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>0.464400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>0.510700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>0.607500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>0.670100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.573200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>0.619900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.608800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>0.534500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>0.632300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.504100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.520900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>0.642300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>0.519200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>0.511800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.452400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>0.493300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>0.386600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>0.409600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>0.456500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.397600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>0.431800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>0.492100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.502000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>0.395700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.421100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>0.486500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>0.419500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>0.511300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>0.476200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.406200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.428700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>0.537800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>0.417600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>0.360500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.392400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>0.469400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>0.327300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>0.459800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.292700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.462900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>0.404700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1870</td>\n",
       "      <td>0.496100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>0.338400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1890</td>\n",
       "      <td>0.403800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.430700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1910</td>\n",
       "      <td>0.583600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.544900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1930</td>\n",
       "      <td>0.388500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>0.301600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.410800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>0.378900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970</td>\n",
       "      <td>0.409800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>0.468700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990</td>\n",
       "      <td>0.471800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.292100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010</td>\n",
       "      <td>0.497500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>0.331200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2030</td>\n",
       "      <td>0.434300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>0.496300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.416200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>0.442700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2070</td>\n",
       "      <td>0.404300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.389800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2090</td>\n",
       "      <td>0.314900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.446700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2110</td>\n",
       "      <td>0.438400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>0.417600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2130</td>\n",
       "      <td>0.436000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>0.427400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.345700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.375600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2170</td>\n",
       "      <td>0.404600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>0.380100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2190</td>\n",
       "      <td>0.535300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.245100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2210</td>\n",
       "      <td>0.491400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>0.563300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2230</td>\n",
       "      <td>0.476700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.396700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.445400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>0.292200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2270</td>\n",
       "      <td>0.248300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>0.433300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2290</td>\n",
       "      <td>0.398900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.425600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2310</td>\n",
       "      <td>0.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>0.413300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2330</td>\n",
       "      <td>0.345600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>0.341200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.495000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\afgdl.ASUSS15ANGEL\\Documents\\GitHub\\LBBYs_CH2\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\afgdl.ASUSS15ANGEL\\Documents\\GitHub\\LBBYs_CH2\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2352, training_loss=0.5511063802607206, metrics={'train_runtime': 37596.2394, 'train_samples_per_second': 0.5, 'train_steps_per_second': 0.063, 'total_flos': 2489724757739520.0, 'train_loss': 0.5511063802607206, 'epoch': 3.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = np.argmax(p.predictions, axis=1)        \n",
    "    labels = p.label_ids                           \n",
    "    f1 = f1_score(labels, preds, average='macro')  \n",
    "    return {'f1': f1}\n",
    "\n",
    "\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46223370",
   "metadata": {},
   "source": [
    "Evaluamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4f4a94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\afgdl.ASUSS15ANGEL\\Documents\\GitHub\\LBBYs_CH2\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\afgdl.ASUSS15ANGEL\\Documents\\GitHub\\LBBYs_CH2\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.45      0.47       923\n",
      "           1       0.72      0.75      0.74      1762\n",
      "\n",
      "    accuracy                           0.65      2685\n",
      "   macro avg       0.61      0.60      0.60      2685\n",
      "weighted avg       0.64      0.65      0.64      2685\n",
      "\n",
      "Macro F1: 0.6032715196851095\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo\n",
    "trainer.evaluate()\n",
    "\n",
    "# Predecir\n",
    "pred_out = trainer.predict(test_dataset)\n",
    "test_preds = np.argmax(pred_out.predictions, axis=1)\n",
    "\n",
    "# Reporte completo de métricas\n",
    "print(classification_report(y_test, test_preds))\n",
    "print(\"Macro F1:\", f1_score(y_test, test_preds, average='macro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4055609c",
   "metadata": {},
   "source": [
    "## Conclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4d6bbd",
   "metadata": {},
   "source": [
    "En resumen, DistilBERT demostró ser un modelo poderoso para tareas de clasificación de texto, destacándose por su eficiencia y efectividad, pero también dejando espacio para mejorar en términos de las métricas de evaluación. Se observa como se obtiene una accuracy de 0.65 y un Macro F1-Score de 0.60, lo que lo convierte en un modelo eficaz, aunque con espacio para mejoras. Este modelo ha demostrado un buen desempeño al identificar la clase mayoritaria (noticias verdaderas), alcanzando un recall del 75% y una precisión del 72%. Sin embargo, para la clase minoritaria (noticias falsas), el rendimiento fue más bajo, con un recall de 45% y una precisión de 49%, lo que refleja que el modelo tiene dificultades para detectar correctamente las noticias falsas, aunque aún proporciona predicciones aceptables.\n",
    "\n",
    "En comparación con los otros modelos analizados, DistilBERT es el que obtiene el mejor rendimeinto, sin embargo, en ese análisis del modelo se observa como todavía existen ajustes y mejoras que permitan identificar con claridad la clase minoritaria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b645ac6",
   "metadata": {},
   "source": [
    "# 3. Entrega Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fae77fe",
   "metadata": {},
   "source": [
    "## Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1443660a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo de submission 'randomforest_2025-05-13_00-23-50.csv' generado correctamente.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "# Cargar el dataset de test\n",
    "df_test = pd.read_csv('../../data/processed/test_preprocess_v1.csv')  # Asegúrate de usar la ruta correcta\n",
    "\n",
    "# --- Preprocesamiento ---\n",
    "# 1. Asegurarse de que el texto esté preprocesado de la misma manera que en el entrenamiento:\n",
    "#    Esto incluye la vectorización TF-IDF.\n",
    "\n",
    "# Cargar el vectorizador TF-IDF entrenado previamente\n",
    "tfidf_vectorizer = joblib.load('tfidf_vectorizer.joblib')\n",
    "\n",
    "# Aplicar la transformación de texto en los datos de test\n",
    "X_test_tfidf = tfidf_vectorizer.transform(df_test['statement'])\n",
    "\n",
    "# --- Predicción ---\n",
    "# Cargar el modelo Random Forest entrenado\n",
    "rf_model = joblib.load('random_forest_model.joblib')\n",
    "\n",
    "# Realizar las predicciones en los datos de test\n",
    "y_test_pred = rf_model.predict(X_test_tfidf)\n",
    "\n",
    "# --- Exportar Submission ---\n",
    "# Crear el archivo CSV con las columnas requeridas: 'id' y 'label'\n",
    "submission = pd.DataFrame({\n",
    "    'id': df_test['id'],  # Usar el 'id' del dataset de prueba\n",
    "    'label': y_test_pred  # Las predicciones del modelo\n",
    "})\n",
    "\n",
    "# Obtener la fecha actual en formato 'YYYY-MM-DD'\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Generar el nombre del archivo con la fecha actual\n",
    "filename = f'randomforest_{current_date}.csv'\n",
    "\n",
    "# Guardar el archivo con el nombre que incluye la fecha\n",
    "submission.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Archivo de submission '{filename}' generado correctamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7ec856",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be06a5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo de submission 'xgboost_submission_2025-05-17_15-48-05.csv' generado correctamente.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el dataset de test\n",
    "df_test = pd.read_csv('../../data/processed/test_preprocess_v1.csv')\n",
    "\n",
    "# Columnas categóricas usadas en entrenamiento\n",
    "cat_columns = ['subject', 'party_affiliation', 'speaker', 'speaker_job', 'state_info', 'party_affiliation_uni']\n",
    "\n",
    "# Cargar vectorizador y columnas categóricas guardadas en entrenamiento\n",
    "tfidf_vectorizer = joblib.load('tfidf_vectorizer.joblib')\n",
    "train_cat_columns = joblib.load('train_cat_columns.joblib')\n",
    "\n",
    "# Codificar variables categóricas en test y alinear con columnas de entrenamiento\n",
    "X_test_cat = pd.get_dummies(df_test[cat_columns], drop_first=True)\n",
    "X_test_cat_aligned = X_test_cat.reindex(columns=train_cat_columns, fill_value=0)\n",
    "\n",
    "# Vectorizar texto 'statement' en test\n",
    "X_test_tfidf = tfidf_vectorizer.transform(df_test['statement'])\n",
    "\n",
    "# Combinar características TF-IDF y variables categóricas\n",
    "X_test_combined = np.hstack([X_test_tfidf.toarray(), X_test_cat_aligned.values])\n",
    "\n",
    "# Cargar modelo XGBoost entrenado\n",
    "xgb_model = joblib.load('xgb_model.joblib')\n",
    "\n",
    "# Predecir etiquetas\n",
    "y_test_pred = xgb_model.predict(X_test_combined)\n",
    "\n",
    "# Crear archivo de submission para Kaggle\n",
    "submission = pd.DataFrame({\n",
    "    'id': df_test['id'],\n",
    "    'label': y_test_pred\n",
    "})\n",
    "\n",
    "# Fecha actual para nombre del archivo\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "filename = f'xgboost_submission_{current_date}.csv'\n",
    "\n",
    "submission.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Archivo de submission '{filename}' generado correctamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cfdade",
   "metadata": {},
   "source": [
    "## Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a2657cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo de submission 'voting_classifier_submission_2025-05-17_15-44-25.csv' generado correctamente.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el dataset de test\n",
    "df_test = pd.read_csv('../../data/processed/test_preprocess_v1.csv')  # Ajusta la ruta si es necesario\n",
    "\n",
    "# Cargar el vectorizador TF-IDF entrenado previamente\n",
    "tfidf_vectorizer = joblib.load('tfidf_vectorizer.joblib')\n",
    "\n",
    "# Columnas categóricas usadas en entrenamiento (las que se usaron para get_dummies)\n",
    "cat_columns = ['subject', 'party_affiliation', 'speaker', 'speaker_job', 'state_info', 'party_affiliation_uni']\n",
    "\n",
    "# Preprocesar las variables categóricas de test con get_dummies\n",
    "X_test_cat = pd.get_dummies(df_test[cat_columns], drop_first=True)\n",
    "\n",
    "# Vectorizar la columna de texto 'statement'\n",
    "X_test_tfidf = tfidf_vectorizer.transform(df_test['statement'])\n",
    "\n",
    "# Alinear las columnas categóricas para que coincidan con las usadas en entrenamiento\n",
    "X_test_cat_aligned = X_test_cat.reindex(columns=joblib.load('train_cat_columns.joblib'), fill_value=0)\n",
    "\n",
    "# Combinar las columnas vectorizadas TF-IDF con las categóricas codificadas\n",
    "X_test_combined = np.hstack([X_test_tfidf.toarray(), X_test_cat_aligned.values])\n",
    "\n",
    "# Cargar el modelo VotingClassifier entrenado\n",
    "voting_clf = joblib.load('voting_classifier_model.joblib')\n",
    "\n",
    "# Realizar las predicciones\n",
    "y_test_pred = voting_clf.predict(X_test_combined)\n",
    "\n",
    "# Crear DataFrame para submission con las columnas requeridas\n",
    "submission = pd.DataFrame({\n",
    "    'id': df_test['id'],\n",
    "    'label': y_test_pred\n",
    "})\n",
    "\n",
    "# Guardar el archivo con timestamp para evitar sobreescrituras\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "filename = f'voting_classifier_submission_{current_date}.csv'\n",
    "submission.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Archivo de submission '{filename}' generado correctamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46635b9",
   "metadata": {},
   "source": [
    "## Distilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182df94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import Trainer, TrainingArguments, EvalPrediction\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el dataset de test desde Kaggle (ruta del archivo que hayas cargado)\n",
    "df_test = pd.read_csv('../../data/processed/test_preprocess_v1.csv')  # Asegúrate de usar la ruta correcta\n",
    "\n",
    "# --- Preprocesamiento ---\n",
    "# Cargar el tokenizador DistilBERT directamente desde Hugging Face\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenizar las afirmaciones de test\n",
    "test_encodings = tokenizer(list(df_test['statement']), truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Convertir a formato Dataset compatible con Hugging Face\n",
    "test_dataset = Dataset.from_dict({\n",
    "    'input_ids': test_encodings['input_ids'],\n",
    "    'attention_mask': test_encodings['attention_mask'],\n",
    "    'label': df_test['label'].tolist()  # Asume que df_test tiene la columna 'label'\n",
    "})\n",
    "\n",
    "# Cargar el modelo DistilBERT desde Hugging Face (sin necesidad de archivos previamente guardados)\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# Configuración del entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',  # Carpeta para guardar los resultados\n",
    "    num_train_epochs=1,      # Usamos 1 época para prueba rápida\n",
    "    per_device_train_batch_size=4,  # Tamaño de batch reducido\n",
    "    per_device_eval_batch_size=8,   # Tamaño de batch reducido\n",
    ")\n",
    "\n",
    "# Función de evaluación (cálculo de F1-score)\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    predictions, labels = p\n",
    "    preds = np.argmax(predictions, axis=1)  # Convertir predicciones a etiquetas\n",
    "    f1 = f1_score(labels, preds, average='macro')  # F1-score (macro)\n",
    "    return {'f1': f1}\n",
    "\n",
    "# Iniciar el entrenamiento con Hugging Face Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=test_dataset,  # Usar dataset de test como ejemplo para la predicción\n",
    "    eval_dataset=test_dataset,   # Igual para evaluación\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Entrenar el modelo (aunque solo sea por 1 época)\n",
    "trainer.train()\n",
    "\n",
    "# Evaluar el modelo (esto calculará las métricas de evaluación automáticamente)\n",
    "trainer.evaluate()\n",
    "\n",
    "# Realizar predicciones sobre test_dataset\n",
    "pred_out = trainer.predict(test_dataset)\n",
    "\n",
    "# Convertir las predicciones a etiquetas (tensores de PyTorch a numpy arrays)\n",
    "test_preds = np.argmax(pred_out.predictions, axis=1)\n",
    "\n",
    "# Reporte de métricas\n",
    "print(classification_report(df_test['label'], test_preds))\n",
    "print(\"Macro F1:\", f1_score(df_test['label'], test_preds, average='macro'))\n",
    "\n",
    "# --- Exportar Submission ---\n",
    "# Crear el archivo CSV con las columnas requeridas: 'id' y 'label'\n",
    "submission = pd.DataFrame({\n",
    "    'id': df_test['id'],  # Usar el 'id' del dataset de prueba\n",
    "    'label': test_preds  # Las predicciones del modelo\n",
    "})\n",
    "\n",
    "# Obtener la fecha actual en formato 'YYYY-MM-DD'\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Generar el nombre del archivo con la fecha actual\n",
    "filename = f'distilbert_{current_date}.csv'\n",
    "\n",
    "# Guardar el archivo con el nombre que incluye la fecha\n",
    "submission.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Archivo de submission '{filename}' generado correctamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fd9133",
   "metadata": {},
   "source": [
    "# 4. Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195d8a38",
   "metadata": {},
   "source": [
    "1. [IA generativa, modelos ligeros y DistilBERT – Josué Cajahuamán Oscátegui](https://www.linkedin.com/pulse/ia-generativa-modelos-ligeros-distilbert-y-josu%C3%A9-cajahuam%C3%A1n-osc%C3%A1tegui-qxxge/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
