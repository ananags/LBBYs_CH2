{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bfa0406",
   "metadata": {},
   "source": [
    "# 0. Instalación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0475ba54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (1.6.1)\n",
      "Requirement already satisfied: pandas in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: xgboost in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (3.0.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from xgboost) (2.26.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn pandas xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80f1e22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (0.0)\n",
      "Requirement already satisfied: SMOTE in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (0.1)\n",
      "Requirement already satisfied: imbalanced-learn in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from imblearn) (0.13.0)\n",
      "Requirement already satisfied: numpy>=1.14.3 in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from SMOTE) (2.2.6)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from SMOTE) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from scikit-learn>=0.19.1->SMOTE) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from scikit-learn>=0.19.1->SMOTE) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from scikit-learn>=0.19.1->SMOTE) (3.6.0)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in /home/angel/LBBYs_CH2/.venv/lib/python3.13/site-packages (from imbalanced-learn->imblearn) (0.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install imblearn SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2a7537",
   "metadata": {},
   "source": [
    "# 1. Modelos de Árboles de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85907a1e",
   "metadata": {},
   "source": [
    "**Descripción**\n",
    "\n",
    "Random Forest es un modelo de aprendizaje supervisado basado en árboles de decisión que emplea el principio de ensamble para mejorar la precisión y la robustez de las predicciones. Este enfoque se utiliza principalmente en datos tabulares y es una de las técnicas más populares y eficientes en tareas de clasificación y regresión.\n",
    "\n",
    "Además, este modelo pertenece a la categoría de modelos de tipo bagging ya que emplea una combinación de múltiples árboles de decisión entrenados de manera independiente y al final se elige la solución mayoritaria o el promedio de las predicciones. Por lo que Random Forest al permitir emplear múltiples árboles de decisión entrenados de distinta manera, permitiendo reducir el sobreajuste y mejorar la generalización.\n",
    "\n",
    "**Implementación**\n",
    "\n",
    "En este proyecto vamos a emplear el primer preprocesado realizado. A continuación para balancear los datos, después de haber realizado un análisis con SMOTE y una técnica de submuestreo para balancear el conjunto de los datos. Dinalmente, se ha llegado a la conlcusión de no emplear ninguna estas técnicas, ya que empeoraban las métricas, por lo que nos hemos decantado en unicamente entrenar los modelos con los datos preprocesados. Se ha dejado un apartado con los resultados obtenidos con SMOTE, el submuestreo y además el empleo del Grid Search Vector, que nos empeoraba también las métricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a4f8a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c266e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>statement</th>\n",
       "      <th>subject</th>\n",
       "      <th>speaker</th>\n",
       "      <th>speaker_job</th>\n",
       "      <th>state_info</th>\n",
       "      <th>party_affiliation</th>\n",
       "      <th>party_affiliation_uni</th>\n",
       "      <th>party_affiliation_category_map</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_info_without_stopwords</th>\n",
       "      <th>pos_freq_without_stopwords</th>\n",
       "      <th>lemma_freq_without_stopwords</th>\n",
       "      <th>tag_freq_without_stopwords</th>\n",
       "      <th>processed_subject</th>\n",
       "      <th>speaker_entities</th>\n",
       "      <th>speaker_type</th>\n",
       "      <th>speaker_job_tokens</th>\n",
       "      <th>state_info_tokens</th>\n",
       "      <th>party_affiliation_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81f884c64a7</td>\n",
       "      <td>1</td>\n",
       "      <td>china is in the south china sea and (building)...</td>\n",
       "      <td>china,foreign-policy,military</td>\n",
       "      <td>donald-trump</td>\n",
       "      <td>president-elect</td>\n",
       "      <td>new york</td>\n",
       "      <td>republican</td>\n",
       "      <td>republican</td>\n",
       "      <td>political-affiliation</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'lemma': 'china', 'pos': 'PROPN', 'tag': 'NN...</td>\n",
       "      <td>Counter({'PROPN': 4, 'NOUN': 4, 'ADJ': 1, 'VER...</td>\n",
       "      <td>Counter({'china': 2, 'south': 1, 'sea': 1, 'bu...</td>\n",
       "      <td>Counter({'NNP': 4, 'NN': 3, 'JJ': 1, 'NNS': 1,...</td>\n",
       "      <td>['china', 'foreign-policy', 'military']</td>\n",
       "      <td>['donald trump']</td>\n",
       "      <td>['PERSON']</td>\n",
       "      <td>['president', '-', 'elect']</td>\n",
       "      <td>['new', 'york']</td>\n",
       "      <td>['republican']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30c2723a188</td>\n",
       "      <td>0</td>\n",
       "      <td>with the resources it takes to execute just ov...</td>\n",
       "      <td>health-care</td>\n",
       "      <td>chris-dodd</td>\n",
       "      <td>u.s. senator</td>\n",
       "      <td>connecticut</td>\n",
       "      <td>democrat</td>\n",
       "      <td>democrat</td>\n",
       "      <td>political-affiliation</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'lemma': 'resource', 'pos': 'NOUN', 'tag': '...</td>\n",
       "      <td>Counter({'NOUN': 7, 'VERB': 4, 'PROPN': 2, 'AD...</td>\n",
       "      <td>Counter({'resource': 1, 'take': 1, 'execute': ...</td>\n",
       "      <td>Counter({'NN': 4, 'NNS': 3, 'VB': 2, 'NNP': 2,...</td>\n",
       "      <td>['health-care']</td>\n",
       "      <td>['chris dodd']</td>\n",
       "      <td>['PERSON']</td>\n",
       "      <td>['u.s', '.', 'senator']</td>\n",
       "      <td>['connecticut']</td>\n",
       "      <td>['democrat']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6936b216e5d</td>\n",
       "      <td>0</td>\n",
       "      <td>the (wisconsin) governor has proposed tax give...</td>\n",
       "      <td>corporations,pundits,taxes,abc-news-week</td>\n",
       "      <td>donna-brazile</td>\n",
       "      <td>political commentator</td>\n",
       "      <td>washington, d.c.</td>\n",
       "      <td>democrat</td>\n",
       "      <td>democrat</td>\n",
       "      <td>political-affiliation</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'lemma': 'wisconsin', 'pos': 'PROPN', 'tag':...</td>\n",
       "      <td>Counter({'NOUN': 4, 'PROPN': 1, 'VERB': 1})</td>\n",
       "      <td>Counter({'wisconsin': 1, 'governor': 1, 'propo...</td>\n",
       "      <td>Counter({'NN': 2, 'NNS': 2, 'NNP': 1, 'VBN': 1})</td>\n",
       "      <td>['corporations', 'pundits', 'taxes', 'abc-news...</td>\n",
       "      <td>['donna brazile']</td>\n",
       "      <td>['PERSON']</td>\n",
       "      <td>['political', 'commentator']</td>\n",
       "      <td>['washington', ',', 'd.c', '.']</td>\n",
       "      <td>['democrat']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b5cd9195738</td>\n",
       "      <td>1</td>\n",
       "      <td>says her representation of an ex-boyfriend who...</td>\n",
       "      <td>candidates-biography,children,ethics,families,...</td>\n",
       "      <td>rebecca-bradley</td>\n",
       "      <td>non-define</td>\n",
       "      <td>non-define</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>other-political-groups</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'lemma': 'say', 'pos': 'VERB', 'tag': 'VBZ',...</td>\n",
       "      <td>Counter({'NOUN': 9, 'VERB': 1, 'ADJ': 1})</td>\n",
       "      <td>Counter({'say': 1, 'representation': 1, 'ex': ...</td>\n",
       "      <td>Counter({'NN': 8, 'VBZ': 1, 'NNS': 1, 'JJ': 1})</td>\n",
       "      <td>['candidates-biography', 'children', 'ethics',...</td>\n",
       "      <td>['rebecca bradley']</td>\n",
       "      <td>['PERSON']</td>\n",
       "      <td>['non', '-', 'define']</td>\n",
       "      <td>['non', '-', 'define']</td>\n",
       "      <td>['none']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84f8dac7737</td>\n",
       "      <td>0</td>\n",
       "      <td>at protests in wisconsin against proposed coll...</td>\n",
       "      <td>health-care,labor,state-budget</td>\n",
       "      <td>republican-party-wisconsin</td>\n",
       "      <td>non-define</td>\n",
       "      <td>wisconsin</td>\n",
       "      <td>republican</td>\n",
       "      <td>republican</td>\n",
       "      <td>political-affiliation</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'lemma': 'protest', 'pos': 'NOUN', 'tag': 'N...</td>\n",
       "      <td>Counter({'NOUN': 7, 'VERB': 4, 'ADJ': 3})</td>\n",
       "      <td>Counter({'protest': 1, 'wisconsin': 1, 'propos...</td>\n",
       "      <td>Counter({'NNS': 4, 'NN': 3, 'JJ': 3, 'VBN': 2,...</td>\n",
       "      <td>['health-care', 'labor', 'state-budget']</td>\n",
       "      <td>['republican party', 'wisconsin']</td>\n",
       "      <td>['ORG', 'GPE']</td>\n",
       "      <td>['non', '-', 'define']</td>\n",
       "      <td>['wisconsin']</td>\n",
       "      <td>['republican']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  label                                          statement  \\\n",
       "0  81f884c64a7      1  china is in the south china sea and (building)...   \n",
       "1  30c2723a188      0  with the resources it takes to execute just ov...   \n",
       "2  6936b216e5d      0  the (wisconsin) governor has proposed tax give...   \n",
       "3  b5cd9195738      1  says her representation of an ex-boyfriend who...   \n",
       "4  84f8dac7737      0  at protests in wisconsin against proposed coll...   \n",
       "\n",
       "                                             subject  \\\n",
       "0                      china,foreign-policy,military   \n",
       "1                                        health-care   \n",
       "2           corporations,pundits,taxes,abc-news-week   \n",
       "3  candidates-biography,children,ethics,families,...   \n",
       "4                     health-care,labor,state-budget   \n",
       "\n",
       "                      speaker            speaker_job        state_info  \\\n",
       "0                donald-trump        president-elect          new york   \n",
       "1                  chris-dodd           u.s. senator       connecticut   \n",
       "2               donna-brazile  political commentator  washington, d.c.   \n",
       "3             rebecca-bradley             non-define        non-define   \n",
       "4  republican-party-wisconsin             non-define         wisconsin   \n",
       "\n",
       "  party_affiliation party_affiliation_uni party_affiliation_category_map  ...  \\\n",
       "0        republican            republican          political-affiliation  ...   \n",
       "1          democrat              democrat          political-affiliation  ...   \n",
       "2          democrat              democrat          political-affiliation  ...   \n",
       "3              none                  none         other-political-groups  ...   \n",
       "4        republican            republican          political-affiliation  ...   \n",
       "\n",
       "                          pos_info_without_stopwords  \\\n",
       "0  [{'lemma': 'china', 'pos': 'PROPN', 'tag': 'NN...   \n",
       "1  [{'lemma': 'resource', 'pos': 'NOUN', 'tag': '...   \n",
       "2  [{'lemma': 'wisconsin', 'pos': 'PROPN', 'tag':...   \n",
       "3  [{'lemma': 'say', 'pos': 'VERB', 'tag': 'VBZ',...   \n",
       "4  [{'lemma': 'protest', 'pos': 'NOUN', 'tag': 'N...   \n",
       "\n",
       "                          pos_freq_without_stopwords  \\\n",
       "0  Counter({'PROPN': 4, 'NOUN': 4, 'ADJ': 1, 'VER...   \n",
       "1  Counter({'NOUN': 7, 'VERB': 4, 'PROPN': 2, 'AD...   \n",
       "2        Counter({'NOUN': 4, 'PROPN': 1, 'VERB': 1})   \n",
       "3          Counter({'NOUN': 9, 'VERB': 1, 'ADJ': 1})   \n",
       "4          Counter({'NOUN': 7, 'VERB': 4, 'ADJ': 3})   \n",
       "\n",
       "                        lemma_freq_without_stopwords  \\\n",
       "0  Counter({'china': 2, 'south': 1, 'sea': 1, 'bu...   \n",
       "1  Counter({'resource': 1, 'take': 1, 'execute': ...   \n",
       "2  Counter({'wisconsin': 1, 'governor': 1, 'propo...   \n",
       "3  Counter({'say': 1, 'representation': 1, 'ex': ...   \n",
       "4  Counter({'protest': 1, 'wisconsin': 1, 'propos...   \n",
       "\n",
       "                          tag_freq_without_stopwords  \\\n",
       "0  Counter({'NNP': 4, 'NN': 3, 'JJ': 1, 'NNS': 1,...   \n",
       "1  Counter({'NN': 4, 'NNS': 3, 'VB': 2, 'NNP': 2,...   \n",
       "2   Counter({'NN': 2, 'NNS': 2, 'NNP': 1, 'VBN': 1})   \n",
       "3    Counter({'NN': 8, 'VBZ': 1, 'NNS': 1, 'JJ': 1})   \n",
       "4  Counter({'NNS': 4, 'NN': 3, 'JJ': 3, 'VBN': 2,...   \n",
       "\n",
       "                                   processed_subject  \\\n",
       "0            ['china', 'foreign-policy', 'military']   \n",
       "1                                    ['health-care']   \n",
       "2  ['corporations', 'pundits', 'taxes', 'abc-news...   \n",
       "3  ['candidates-biography', 'children', 'ethics',...   \n",
       "4           ['health-care', 'labor', 'state-budget']   \n",
       "\n",
       "                    speaker_entities    speaker_type  \\\n",
       "0                   ['donald trump']      ['PERSON']   \n",
       "1                     ['chris dodd']      ['PERSON']   \n",
       "2                  ['donna brazile']      ['PERSON']   \n",
       "3                ['rebecca bradley']      ['PERSON']   \n",
       "4  ['republican party', 'wisconsin']  ['ORG', 'GPE']   \n",
       "\n",
       "             speaker_job_tokens                state_info_tokens  \\\n",
       "0   ['president', '-', 'elect']                  ['new', 'york']   \n",
       "1       ['u.s', '.', 'senator']                  ['connecticut']   \n",
       "2  ['political', 'commentator']  ['washington', ',', 'd.c', '.']   \n",
       "3        ['non', '-', 'define']           ['non', '-', 'define']   \n",
       "4        ['non', '-', 'define']                    ['wisconsin']   \n",
       "\n",
       "  party_affiliation_tokens  \n",
       "0           ['republican']  \n",
       "1             ['democrat']  \n",
       "2             ['democrat']  \n",
       "3                 ['none']  \n",
       "4           ['republican']  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../../../../data/processed/train_preprocess_v1.csv')\n",
    "#data = pd.read_csv('../../../../data/processed/train_limpieza_v1.csv')\n",
    "data.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4872d8",
   "metadata": {},
   "source": [
    "Con esos datos preprocesados, seleccionamos que columnas no van a participar en el proceso de entrenamieto. Se ha decidido no usar esas columnas, ya que de todas las características analizadas, eran las que menos información podían aportar a la hora de decidir si estamos ante una *fake news* o no.\n",
    "\n",
    "En una primera idea se realizó el entrenamiento con todas las columnas, pese al mal funcionamiento de los modelos de árboles de decisión, se cambió la estrategiay se emplea esta estrategia de seleccionar solo las columnas que aportan información importante y no es redundadnte.\n",
    "\n",
    "Además, se aplicó la vectorización TF-IDF, permitiendo una representación númerica desde texto, limitando a 5000 las palabras más relevantes del inglés y eliminando palabras que no aportan información relevante. De esta forma se captura la importancia de cada término. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1471c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar la noticia y la etiqueta de veracidad de si es fake new\n",
    "X = data['statement']  \n",
    "y = data['label']  \n",
    "\n",
    "# Dividir los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Vectorización de texto utilizando TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f234285",
   "metadata": {},
   "source": [
    "**Configuración de hiperparámetros de Random Forest y entrenamiento**\n",
    "\n",
    "Para llevar a cabo la configuración de los hiperparámetros hemos tenido en cuenta lo la documentación del modelo. Para el criterio de división empleado, se utiliza Gini ya que proporciona una solución rápida y eficiente en la construcción de los árboles. Además, en cuanto al número de estimadores y la profundidad máxima, es decir, como de grande va a ser nuestro bosque y como de profundo va a ser cada árbol se ha optado por valores más altos. El número de estimadores es elevado, ya que como cada árbol de forma individual tiene que dar su valoración, si se poseen gran cantidad de árboles, el error individual promedio disminuye, y por lo tanto, es más robusto pero tiene mayor consumo computacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6e6261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Entrenamiento del modelo Random Forest\n",
    "class_weights = {0: 5, 1: 2}  \n",
    "rf_model = RandomForestClassifier(\n",
    "    criterion='gini',\n",
    "    n_estimators=500,\n",
    "    max_depth=80,\n",
    "    max_features='sqrt',        \n",
    "    class_weight=class_weights, \n",
    "    random_state=42\n",
    ")\n",
    "rf_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Realizar predicciones\n",
    "y_pred = rf_model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c15c81",
   "metadata": {},
   "source": [
    "**Evaluación**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8129bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.67      0.51       923\n",
      "           1       0.74      0.50      0.60      1762\n",
      "\n",
      "    accuracy                           0.56      2685\n",
      "   macro avg       0.58      0.59      0.56      2685\n",
      "weighted avg       0.63      0.56      0.57      2685\n",
      "\n",
      "Macro Average F1-Score: 0.5557\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo con Macro Average F1-Score\n",
    "# Reporte completo de métricas\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(f\"Macro Average F1-Score: {macro_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ff7a5e",
   "metadata": {},
   "source": [
    "Finalmente, guardamos el vectorizador tfidf, empleando la biblioteca joblib. De esta manera, al guardarlo, nos permite poder reutilizarlos posteriamente, ya sea para un reentreno o para poder realizar la submissión de kaggle con los datos para probar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cad7cced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "# Guardar el vectorizador TF-IDF\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890388d4",
   "metadata": {},
   "source": [
    "A cotninuación se proporcionan 3 planteamientos diferentes\n",
    "1. Utilizando columnas específicas\n",
    "2.  Aplicando smote\n",
    "3. Submuestreo\n",
    "con los cuales no se han obtenido resultados favorables, por no se entra en profundidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d433b2c7",
   "metadata": {},
   "source": [
    "### 1.1.2 Random forest solo con columnas determinadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aa5aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.67      0.51       923\n",
      "           1       0.74      0.50      0.60      1762\n",
      "\n",
      "    accuracy                           0.56      2685\n",
      "   macro avg       0.58      0.59      0.56      2685\n",
      "weighted avg       0.63      0.56      0.57      2685\n",
      "\n",
      "Macro Average F1-Score: 0.5557\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar el dataset\n",
    "data = pd.read_csv('../../../../data/processed/train_preprocess_v1.csv')\n",
    "\n",
    "# Seleccionar características relevantes\n",
    "X = data[['statement', 'subject', 'party_affiliation', 'speaker', 'speaker_job', 'state_info', 'party_affiliation_uni']]\n",
    "y = data['label']  \n",
    "\n",
    "# División de los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Vectorización del texto mediante TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['statement'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['statement'])\n",
    "\n",
    "# Entrenamiento del modelo \n",
    "class_weights = {0: 5, 1: 2}  # No modificar los pesos, ya que empeora el modelo\n",
    "rf_model = RandomForestClassifier(\n",
    "    criterion='gini',\n",
    "    n_estimators=500,\n",
    "    max_depth=80,\n",
    "    max_features='sqrt',        \n",
    "    class_weight=class_weights, \n",
    "    random_state=42\n",
    ")\n",
    "rf_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predicciones para evaluar el rendimiento del modelo\n",
    "y_pred = rf_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluar el modelo con Macro Average F1-Score\n",
    "# Reporte completo de métricas\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(f\"Macro Average F1-Score: {macro_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8de8b8",
   "metadata": {},
   "source": [
    "sin usar statement con one hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8311cb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.65      0.49       923\n",
      "           1       0.72      0.48      0.57      1762\n",
      "\n",
      "    accuracy                           0.54      2685\n",
      "   macro avg       0.56      0.56      0.53      2685\n",
      "weighted avg       0.61      0.54      0.54      2685\n",
      "\n",
      "Macro Average F1-Score: 0.5318\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el dataset\n",
    "data = pd.read_csv('../../../../data/processed/train_preprocess_v1.csv')\n",
    "\n",
    "# Seleccionar las características relevantes (sin la columna 'statement') y la etiqueta\n",
    "X = data[['subject', 'speaker', 'speaker_job', 'state_info', 'party_affiliation', 'party_affiliation_uni']]  # Columnas relevantes\n",
    "y = data['label']  # Etiqueta de veracidad\n",
    "\n",
    "# Realizar el OneHotEncoder para las columnas categóricas\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "\n",
    "# Aplicar One-Hot Encoding a las columnas categóricas\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Entrenamiento del modelo Random Forest\n",
    "class_weights = {0: 5, 1: 2}  # No modificar los pesos, ya que empeora el modelo\n",
    "rf_model = RandomForestClassifier(\n",
    "    criterion='gini',\n",
    "    n_estimators=500,\n",
    "    max_depth=80,\n",
    "    max_features='sqrt',  # 'max_features': ['sqrt', 'log2', None],\n",
    "    class_weight=class_weights,  # 'balanced'\n",
    "    random_state=42\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo con Macro Average F1-Score\n",
    "# Reporte completo de métricas\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(f\"Macro Average F1-Score: {macro_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61cccde",
   "metadata": {},
   "source": [
    "### 1.1.3 Aplico Smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7474a284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.80      0.52       923\n",
      "           1       0.76      0.33      0.46      1762\n",
      "\n",
      "    accuracy                           0.49      2685\n",
      "   macro avg       0.57      0.57      0.49      2685\n",
      "weighted avg       0.63      0.49      0.48      2685\n",
      "\n",
      "Macro Average F1-Score: 0.4915\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE  \n",
    "\n",
    "# Cargar del dataset\n",
    "data = pd.read_csv('../../../../data/processed/train_preprocess_v1.csv')\n",
    "\n",
    "# Seleccionar las características \n",
    "X = data[['statement', 'subject', 'party_affiliation', 'speaker', 'speaker_job', 'state_info', 'party_affiliation_uni']] \n",
    "y = data['label']  \n",
    "\n",
    "# Dividimos los datos \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Vectorización del texto con TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['statement'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['statement'])\n",
    "\n",
    "# Aplicamos SMOTE ÚNICAMENTE AL CONJUNTO DE ENTRENAMIENTO!!! (al de prueba no)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_tfidf, y_train)\n",
    "\n",
    "# Entrenamiento del modelo \n",
    "class_weights = {0: 5, 1: 2}  # No modificar los pesos, ya que empeora el modelo\n",
    "rf_model = RandomForestClassifier(\n",
    "    criterion='gini',\n",
    "    n_estimators=500,\n",
    "    max_depth=80,\n",
    "    max_features='sqrt',        # 'max_features': ['sqrt', 'log2', None],\n",
    "    class_weight=class_weights, #'balanced'\n",
    "    random_state=42\n",
    ")\n",
    "rf_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = rf_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluar el modelo con Macro Average F1-Score\n",
    "# Reporte completo de métricas\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(f\"Macro Average F1-Score: {macro_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69e9723",
   "metadata": {},
   "source": [
    "### 1.1.4 Aplico la técnica del submuestreo para el balanceo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fcb042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.93      0.50       923\n",
      "           1       0.67      0.07      0.13      1762\n",
      "\n",
      "    accuracy                           0.37      2685\n",
      "   macro avg       0.51      0.50      0.32      2685\n",
      "weighted avg       0.56      0.37      0.26      2685\n",
      "\n",
      "Macro Average F1-Score: 0.3178\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Carga del dataset\n",
    "data = pd.read_csv('../../../../data/processed/train_preprocess_v1.csv')\n",
    "\n",
    "# Seleccionar las características \n",
    "X = data[['statement', 'subject', 'party_affiliation', 'speaker', 'speaker_job', 'state_info', 'party_affiliation_uni']] \n",
    "y = data['label'] \n",
    "\n",
    "# Dividimos los datos \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Vectorización del texto utilizando TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['statement'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['statement'])\n",
    "\n",
    "# Submuestreo del conjunto de entrenamiento siendo la clase 0 la minotitaria y la clase 1 la mayoritaria\n",
    "# Vamos a realizar un submuestreo aleatorio igualando el número de ejemplos de ambas clases\n",
    "df_train = pd.DataFrame(X_train_tfidf.toarray())  \n",
    "df_train['label'] = y_train\n",
    "df_train_class_0 = df_train[df_train['label'] == 0]\n",
    "df_train_class_1 = df_train[df_train['label'] == 1]\n",
    "n_minority = len(df_train_class_0)\n",
    "df_train_class_1_under = df_train_class_1.sample(n=n_minority, random_state=42)\n",
    "\n",
    "# Combinamos ambas clases \n",
    "df_train_balanced = pd.concat([df_train_class_0, df_train_class_1_under]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Separar las características y etiquetas después del submuestreo\n",
    "X_train_balanced = df_train_balanced.drop(columns=['label'])\n",
    "y_train_balanced = df_train_balanced['label']\n",
    "\n",
    "# Entrenamiento\n",
    "class_weights = {0: 5, 1: 2}  # No modificar los pesos, ya que empeora el modelo\n",
    "rf_model = RandomForestClassifier(\n",
    "    criterion='gini',\n",
    "    n_estimators=500,\n",
    "    max_depth=80,\n",
    "    max_features='sqrt',        # 'max_features': ['sqrt', 'log2', None],\n",
    "    class_weight=class_weights, #'balanced'\n",
    "    random_state=42\n",
    ")\n",
    "rf_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = rf_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluar el modelo con Macro Average F1-Score\n",
    "# Reporte completo de métricas\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(f\"Macro Average F1-Score: {macro_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfc10dd",
   "metadata": {},
   "source": [
    "## 1.2 XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461106ef",
   "metadata": {},
   "source": [
    "**Descripción**\n",
    "\n",
    "Este modelo también se trata de un algoritmo de aprendizaje supervisado basado en el enfoque de boosting, empleando tamién para las tareas de clsificación y regresión. A diferencia del Random Forest, XGBoost, se encarga de construir los árboles de manera secuencial, y cada árbol va a intentar corregir los errores producidos por el árbol precedente.\n",
    "\n",
    "**Implementación**\n",
    "\n",
    "La implementación es muy similar a la utilizada en el Random Forest, a diferencia de la configuracion de los hiperparámetros.\n",
    "\n",
    "Para la configuración, en el caso de XGBoost, el número de estimadores y la profundidad máxima de los árboles se configuraron de manera similar a los valores de Random Forest, siguiendo las recomendaciones de la documentación de XGBoost. Además de estos hiperparámetros, se utilizó la métrica logloss para la evaluación. También se ha configurado la tasa de aprendizaje del modelo es baja, de tal manera que va a permitir que vaya aprendiendo de forma gradual y sea más generalizable, evitando tender al sobreajuste, y en cada árbol al usar datos con tantas características, se va a emplear un 60% de estas distribuidas de forma aleatoria.\n",
    "Además, otra cosa a destacar es que vamos a codificar las variables categóricas para que se pueda tratar sin problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1ebace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.48      0.44       923\n",
      "           1       0.70      0.63      0.66      1762\n",
      "\n",
      "    accuracy                           0.58      2685\n",
      "   macro avg       0.55      0.55      0.55      2685\n",
      "weighted avg       0.60      0.58      0.59      2685\n",
      "\n",
      "Macro F1-Score: 0.5501\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Carga del dataset\n",
    "data = pd.read_csv('../../../../data/processed/train_preprocess_v1.csv')\n",
    "\n",
    "# Seleccionar las columnas relevantes \n",
    "X = data[['subject', 'party_affiliation', 'speaker', 'speaker_job', 'state_info', 'party_affiliation_uni']] \n",
    "y = data['label']  \n",
    "\n",
    "# División de los datos \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Codificación de las variables categóricas\n",
    "X_train_encoded = pd.get_dummies(X_train, drop_first=True)\n",
    "X_test_encoded = pd.get_dummies(X_test, drop_first=True)\n",
    "\n",
    "# Nos asrguramos que ambas columnas son del mismo tamaño\n",
    "X_train_encoded, X_test_encoded = X_train_encoded.align(X_test_encoded, join='left', axis=1)\n",
    "\n",
    "# Vectorización del texto mediante TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(data['statement'][X_train.index])  \n",
    "X_test_tfidf = tfidf_vectorizer.transform(data['statement'][X_test.index]) \n",
    "\n",
    "# Concatenamos las características de texto mediante TF-IDF y las características categóricas\n",
    "X_train_combined = np.hstack([X_train_tfidf.toarray(), X_train_encoded.values])\n",
    "X_test_combined = np.hstack([X_test_tfidf.toarray(), X_test_encoded.values])\n",
    "\n",
    "# Entrenamos el modelo\n",
    "sample_weights = y_train.map({0: 5, 1: 2}) \n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=80,\n",
    "    learning_rate=0.05,\n",
    "    eval_metric='logloss',\n",
    "    colsample_bytree=0.6,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train_combined, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# Prediccion\n",
    "y_pred = xgb_model.predict(X_test_combined)\n",
    "\n",
    "# Evaluar el modelo con Macro Average F1-Score\n",
    "print(\"Reporte de Clasificación:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Calcular el Macro F1-Score\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(f\"Macro F1-Score: {macro_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a39aef",
   "metadata": {},
   "source": [
    "Nos guardamos el vectorizador tfidf y el modelo entrenado para facilitar el reentrenamiento y la predicción del conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9856f683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgb_model.joblib']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(X_train_encoded.columns, 'train_cat_columns.joblib')\n",
    "\n",
    "# Guardamos el vectorizador TF-IDF\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.joblib')\n",
    "\n",
    "# Guardamos ek modelo entrenado\n",
    "joblib.dump(xgb_model, 'xgb_model.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdc29cf",
   "metadata": {},
   "source": [
    "## 1.3 Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5572cbf",
   "metadata": {},
   "source": [
    "**Descripción del Ensemble: Voting Classifier**\n",
    "\n",
    "Finalmente, para terminar con los árboles de decisón, se ha llevado a cabo el entrenamiento de un ensemble que combina tanto el modelo Random Forest, como el modelo XGBoost, en búsqueda de que se mejore el entrenamiento, la precisión y la robustez del model; y por tanto las métricas. \n",
    "\n",
    "Para ello, vamos a emplear **Voting Classifier**. Esta técnica permite elegir entre un hard voting (donde cada modelo vota por una clase y se toma la mayoría) o un soft voting (donde se promedian las prbabilidades de cada clase y se selecciona aquella con mayor probabilidad final).\n",
    "\n",
    "**Implementación con Voting Classifier**\n",
    "\n",
    "La implementación es similar a los dos modelos anteriores entrenados con las configuraciones ya establecidas. Cabe resaltar que al final, nos hemos decantado por un *soft voting*, la cuál es menos agresiva y clasifica mejor los datos. Además, también vamos a realifzar tanto la codificación de variables categóricas como la vectorización TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e157867a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.35      0.41       923\n",
      "           1       0.71      0.82      0.76      1762\n",
      "\n",
      "    accuracy                           0.66      2685\n",
      "   macro avg       0.61      0.58      0.59      2685\n",
      "weighted avg       0.64      0.66      0.64      2685\n",
      "\n",
      "Macro F1-Score: 0.5853\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Cargarmos el dataset\n",
    "data = pd.read_csv('../../../../data/processed/train_preprocess_v1.csv')\n",
    "\n",
    "# Seleccionar las columnas relevantes para el modelo\n",
    "X = data[['subject', 'party_affiliation', 'speaker', 'speaker_job', 'state_info', 'party_affiliation_uni']] \n",
    "y = data['label']  \n",
    "\n",
    "# Dividimos los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Codificamos las variables categóricas\n",
    "X_train_encoded = pd.get_dummies(X_train, drop_first=True)\n",
    "X_test_encoded = pd.get_dummies(X_test, drop_first=True)\n",
    "\n",
    "# Nos asegurarnos de que ambos conjuntos tengan las mismas columnas\n",
    "X_train_encoded, X_test_encoded = X_train_encoded.align(X_test_encoded, join='left', axis=1)\n",
    "\n",
    "# Vectorización del texto empleando TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(data['statement'][X_train.index])  # Usar solo las filas de X_train\n",
    "X_test_tfidf = tfidf_vectorizer.transform(data['statement'][X_test.index])  # Usar solo las filas de X_test\n",
    "\n",
    "# Concatenamos las características de texto mediante TF-IDF y las características categóricas\n",
    "X_train_combined = np.hstack([X_train_tfidf.toarray(), X_train_encoded.values])\n",
    "X_test_combined = np.hstack([X_test_tfidf.toarray(), X_test_encoded.values])\n",
    "\n",
    "# Configuraciones de los modelos\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=80,\n",
    "    learning_rate=0.05,\n",
    "    eval_metric='logloss',\n",
    "    colsample_bytree=0.6,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    criterion='gini',\n",
    "    n_estimators=500,\n",
    "    max_depth=80,\n",
    "    max_features='sqrt',\n",
    "    class_weight={0: 5, 1: 2},\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Combinamos de los modelos en un ensemble\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('xgb', xgb_model), ('rf', rf_model)],\n",
    "    voting='soft',  # 'soft' para promediar probabilidades\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Entrenamos el ensemble\n",
    "voting_clf.fit(X_train_combined, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = voting_clf.predict(X_test_combined)\n",
    "\n",
    "# Evaluar el modelo\n",
    "print(\"Reporte de Clasificación:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Calcular el Macro F1-Score\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(f\"Macro F1-Score: {macro_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ea3962",
   "metadata": {},
   "source": [
    "finalmente volvemos a guardarnos tanto el vectorizador TF-IDF como el modelo una vez entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9dc249a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['voting_classifier_model.joblib']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "X_train_encoded = pd.get_dummies(X_train, drop_first=True)\n",
    "\n",
    "joblib.dump(X_train_encoded.columns, 'train_cat_columns.joblib')\n",
    "\n",
    "# Guardamos el vectorizador y el modelo \n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.joblib')\n",
    "joblib.dump(voting_clf, 'voting_classifier_model.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e04d981",
   "metadata": {},
   "source": [
    "## Conclusiones,\n",
    "Finalmente, con el modelo Voting Classifier, que combinar Random Forest y XGBoost, se obtiene un rendimiento más solido, obteniendo una *accuracy* del 0.66 y un Mcro F1-Score de 0.5926, siendo el más robusto entre los tres modelos evaluados. Este modelo muestra un buen equilibrio entre precisión y recall, especialmente para la clase mayoritaria (fake news dectectadas), con un recall del 81% y una precisión del 71%. Para la clase minoritaria (fake news no detectadas), el recall es disminuye hasta el 37%, y la precisión disminuye hasta un 51%, reflejando que, aunque detecta menos, la predicción de esta clase es adecuada. En conjunto, el ensemble logra un desempeño más robusto y equilibrado que cualquiera de los modelos individuales.\n",
    "\n",
    "El modelo XGBoost por sí solo presenta una precisión del 60% y un Macro F1-Score de 0.5670. Este modelo obtiene un recall del 68% para la clase mayoritaria y un 46% para la minoritaria, mostrando un mejor equilibrio que Random Forest, aunque con menor precisión para la clase minoritaria (43%).\n",
    "\n",
    "Por último, Random Forest presenta una precisión general más baja (57%) y un Macro F1-Score de 0.5604. Aunque tiene un buen recall para la clase minoritaria (66%), su recall para la clase mayoritaria es más limitado (52%), indicando una menor capacidad para clasificar correctamente las noticias.\n",
    "\n",
    "En resumen, el Voting Classifier combina las fortalezas de ambos modelos y consigue mejorar el equilibrio global en la clasificación, siendo el modelo más adecuado para este problema de clasificación de las diferentes fake news."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1625b1b",
   "metadata": {},
   "source": [
    "## GridSearchCV para random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5164180d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Average F1-Score: 0.4105\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.01      0.03       923\n",
      "           1       0.66      1.00      0.79      1762\n",
      "\n",
      "    accuracy                           0.66      2685\n",
      "   macro avg       0.69      0.51      0.41      2685\n",
      "weighted avg       0.68      0.66      0.53      2685\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import joblib\n",
    "\n",
    "# Cargar el dataset de entrenamiento\n",
    "data = pd.read_csv('../../../../data/processed/train_preprocess_v1.csv')\n",
    "\n",
    "# Seleccionar las características y etiquetas\n",
    "X = data['statement']  \n",
    "y = data['label'] \n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Vectorización del texto utilizando TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Guardar el vectorizador para su uso posterior (si es necesario)\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.joblib')\n",
    "\n",
    "# Definir el modelo Random Forest\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Definir los parámetros a probar para el modelo\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "}\n",
    "\n",
    "# Realizar GridSearchCV para encontrar los mejores parámetros\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Obtener el mejor modelo después de la búsqueda de hiperparámetros\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Realizar predicciones con el mejor modelo\n",
    "y_test_pred = best_rf_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluar el modelo con el Macro Average F1-Score\n",
    "macro_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "print(f\"Macro Average F1-Score: {macro_f1:.4f}\")\n",
    "\n",
    "# Reporte completo de métricas\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998f7615",
   "metadata": {},
   "source": [
    "# 2. Entrega Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28744489",
   "metadata": {},
   "source": [
    "## Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca08859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "# Cargar el dataset de test\n",
    "df_test = pd.read_csv('../../../../data/processed/test_preprocess_v1.csv')  # Asegúrate de usar la ruta correcta\n",
    "\n",
    "# --- Preprocesamiento ---\n",
    "# 1. Asegurarse de que el texto esté preprocesado de la misma manera que en el entrenamiento:\n",
    "#    Esto incluye la vectorización TF-IDF.\n",
    "\n",
    "# Cargar el vectorizador TF-IDF entrenado previamente\n",
    "tfidf_vectorizer = joblib.load('tfidf_vectorizer.joblib')\n",
    "\n",
    "# Aplicar la transformación de texto en los datos de test\n",
    "X_test_tfidf = tfidf_vectorizer.transform(df_test['statement'])\n",
    "\n",
    "# --- Predicción ---\n",
    "# Cargar el modelo Random Forest entrenado\n",
    "rf_model = joblib.load('random_forest_model.joblib')\n",
    "\n",
    "# Realizar las predicciones en los datos de test\n",
    "y_test_pred = rf_model.predict(X_test_tfidf)\n",
    "\n",
    "# --- Exportar Submission ---\n",
    "# Crear el archivo CSV con las columnas requeridas: 'id' y 'label'\n",
    "submission = pd.DataFrame({\n",
    "    'id': df_test['id'],  # Usar el 'id' del dataset de prueba\n",
    "    'label': y_test_pred  # Las predicciones del modelo\n",
    "})\n",
    "\n",
    "# Obtener la fecha actual en formato 'YYYY-MM-DD'\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Generar el nombre del archivo con la fecha actual\n",
    "filename = f'randomforest_{current_date}.csv'\n",
    "\n",
    "# Guardar el archivo con el nombre que incluye la fecha\n",
    "submission.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Archivo de submission '{filename}' generado correctamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f4700b",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c9174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el dataset de test\n",
    "df_test = pd.read_csv('../../../../data/processed/test_preprocess_v1.csv')\n",
    "\n",
    "# Columnas categóricas usadas en entrenamiento\n",
    "cat_columns = ['subject', 'party_affiliation', 'speaker', 'speaker_job', 'state_info', 'party_affiliation_uni']\n",
    "\n",
    "# Cargar vectorizador y columnas categóricas guardadas en entrenamiento\n",
    "tfidf_vectorizer = joblib.load('tfidf_vectorizer.joblib')\n",
    "train_cat_columns = joblib.load('train_cat_columns.joblib')\n",
    "\n",
    "# Codificar variables categóricas en test y alinear con columnas de entrenamiento\n",
    "X_test_cat = pd.get_dummies(df_test[cat_columns], drop_first=True)\n",
    "X_test_cat_aligned = X_test_cat.reindex(columns=train_cat_columns, fill_value=0)\n",
    "\n",
    "# Vectorizar texto 'statement' en test\n",
    "X_test_tfidf = tfidf_vectorizer.transform(df_test['statement'])\n",
    "\n",
    "# Combinar características TF-IDF y variables categóricas\n",
    "X_test_combined = np.hstack([X_test_tfidf.toarray(), X_test_cat_aligned.values])\n",
    "\n",
    "# Cargar modelo XGBoost entrenado\n",
    "xgb_model = joblib.load('xgb_model.joblib')\n",
    "\n",
    "# Predecir etiquetas\n",
    "y_test_pred = xgb_model.predict(X_test_combined)\n",
    "\n",
    "# Crear archivo de submission para Kaggle\n",
    "submission = pd.DataFrame({\n",
    "    'id': df_test['id'],\n",
    "    'label': y_test_pred\n",
    "})\n",
    "\n",
    "# Fecha actual para nombre del archivo\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "filename = f'xgboost_submission_{current_date}.csv'\n",
    "\n",
    "submission.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Archivo de submission '{filename}' generado correctamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dddb39",
   "metadata": {},
   "source": [
    "## Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1d9c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el dataset de test\n",
    "df_test = pd.read_csv('../../../../data/processed/test_preprocess_v1.csv')  # Ajusta la ruta si es necesario\n",
    "\n",
    "# Cargar el vectorizador TF-IDF entrenado previamente\n",
    "tfidf_vectorizer = joblib.load('tfidf_vectorizer.joblib')\n",
    "\n",
    "# Columnas categóricas usadas en entrenamiento (las que se usaron para get_dummies)\n",
    "cat_columns = ['subject', 'party_affiliation', 'speaker', 'speaker_job', 'state_info', 'party_affiliation_uni']\n",
    "\n",
    "# Preprocesar las variables categóricas de test con get_dummies\n",
    "X_test_cat = pd.get_dummies(df_test[cat_columns], drop_first=True)\n",
    "\n",
    "# Vectorizar la columna de texto 'statement'\n",
    "X_test_tfidf = tfidf_vectorizer.transform(df_test['statement'])\n",
    "\n",
    "# Alinear las columnas categóricas para que coincidan con las usadas en entrenamiento\n",
    "X_test_cat_aligned = X_test_cat.reindex(columns=joblib.load('train_cat_columns.joblib'), fill_value=0)\n",
    "\n",
    "# Combinar las columnas vectorizadas TF-IDF con las categóricas codificadas\n",
    "X_test_combined = np.hstack([X_test_tfidf.toarray(), X_test_cat_aligned.values])\n",
    "\n",
    "# Cargar el modelo VotingClassifier entrenado\n",
    "voting_clf = joblib.load('voting_classifier_model.joblib')\n",
    "\n",
    "# Realizar las predicciones\n",
    "y_test_pred = voting_clf.predict(X_test_combined)\n",
    "\n",
    "# Crear DataFrame para submission con las columnas requeridas\n",
    "submission = pd.DataFrame({\n",
    "    'id': df_test['id'],\n",
    "    'label': y_test_pred\n",
    "})\n",
    "\n",
    "# Guardar el archivo con timestamp para evitar sobreescrituras\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "filename = f'voting_classifier_submission_{current_date}.csv'\n",
    "submission.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Archivo de submission '{filename}' generado correctamente.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
