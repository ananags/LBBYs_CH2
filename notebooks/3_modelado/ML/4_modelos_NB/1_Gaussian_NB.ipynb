{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian NB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Explicación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes es un clasificador rápido y muy sencillo de entrenar que asume que, dadas las clases (“fake” - 1 o “real” - 0), cada característica numérica sigue una distribución normal e independiente de las demás. Esto lo convierte en una excelente línea base: en segundos calcula las medias y varianzas por clase, te da un benchmark inicial y te permite identificar qué variables (longitud de texto, frecuencia de POS, TF-IDF de palabras clave…) discriminan mejor entre noticias falsas y verdaderas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- Implementación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.11' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/Users/afgdl.ASUSS15ANGEL/AppData/Local/Microsoft/WindowsApps/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../../../data/processed/train_preprocess_v1.csv\")\n",
    "df_test = pd.read_csv(\"../../../../data/processed/test_preprocess_v1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.- Intento 1: GaussianNB sobre rasgos básicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión (fila=verdadero, columna=predicho):\n",
      "[[ 844  197]\n",
      " [1506  407]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.3591    0.8108    0.4978      1041\n",
      "         1.0     0.6738    0.2128    0.3234      1913\n",
      "\n",
      "    accuracy                         0.4235      2954\n",
      "   macro avg     0.5165    0.5118    0.4106      2954\n",
      "weighted avg     0.5629    0.4235    0.3849      2954\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(columns=['id', 'statement', 'label']) \n",
    "y = df['label']\n",
    "\n",
    "num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), num_cols),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)\n",
    "], remainder='drop')\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('pre', preprocessor),\n",
    "    ('clf', GaussianNB())\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.33,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "print(\"Matriz de confusión (fila=verdadero, columna=predicho):\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "columns are missing: {'party_affiliation'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m X_test = df_test.drop(columns=[\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mstatement\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ————————————————————————————————\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 4) Predicción y creación del CSV\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# ————————————————————————————————\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m y_pred = \u001b[43mpipe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m submission = pd.DataFrame({\n\u001b[32m      9\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m:    df_test[\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     10\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m: y_pred\n\u001b[32m     11\u001b[39m })\n\u001b[32m     14\u001b[39m filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mgaussiannb_submission_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime.datetime.now().strftime(\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LBBYs_CH2/venv/lib/python3.11/site-packages/sklearn/pipeline.py:787\u001b[39m, in \u001b[36mPipeline.predict\u001b[39m\u001b[34m(self, X, **params)\u001b[39m\n\u001b[32m    785\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n\u001b[32m    786\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iter(with_final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m         Xt = \u001b[43mtransform\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    788\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m1\u001b[39m].predict(Xt, **params)\n\u001b[32m    790\u001b[39m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LBBYs_CH2/venv/lib/python3.11/site-packages/sklearn/utils/_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LBBYs_CH2/venv/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:1090\u001b[39m, in \u001b[36mColumnTransformer.transform\u001b[39m\u001b[34m(self, X, **params)\u001b[39m\n\u001b[32m   1088\u001b[39m     diff = all_names - \u001b[38;5;28mset\u001b[39m(column_names)\n\u001b[32m   1089\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m diff:\n\u001b[32m-> \u001b[39m\u001b[32m1090\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcolumns are missing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiff\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1091\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1092\u001b[39m     \u001b[38;5;66;03m# ndarray was used for fitting or transforming, thus we only\u001b[39;00m\n\u001b[32m   1093\u001b[39m     \u001b[38;5;66;03m# check that n_features_in_ is consistent\u001b[39;00m\n\u001b[32m   1094\u001b[39m     _check_n_features(\u001b[38;5;28mself\u001b[39m, X, reset=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mValueError\u001b[39m: columns are missing: {'party_affiliation'}"
     ]
    }
   ],
   "source": [
    "X_test = df_test.drop(columns=['id', 'statement'])\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id':    df_test['id'],\n",
    "    'label': y_pred\n",
    "})\n",
    "\n",
    "\n",
    "filename = f\"gaussiannb_submission_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}.csv\"\n",
    "submission.to_csv(filename, columns=['id', 'label'], index=False)\n",
    "\n",
    "print(f\" Submission generada correctamente: '{filename}'\")\n",
    "print(submission.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.- Intento 2: Incorporando ratios POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión (fila=verdadero ⇒ columna=predicho):\n",
      "[[ 681  360]\n",
      " [1161  752]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3697    0.6542    0.4724      1041\n",
      "           1     0.6763    0.3931    0.4972      1913\n",
      "\n",
      "    accuracy                         0.4851      2954\n",
      "   macro avg     0.5230    0.5236    0.4848      2954\n",
      "weighted avg     0.5682    0.4851    0.4885      2954\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import ast\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def add_pos_ratios(df):\n",
    "    df = df.copy()\n",
    "    noun, verb, adj, adv = [], [], [], []\n",
    "\n",
    "    for row in df['pos_freq']:\n",
    "        try:\n",
    "            if isinstance(row, str):\n",
    "                d = ast.literal_eval(row)\n",
    "            elif isinstance(row, dict):\n",
    "                d = row\n",
    "            else:\n",
    "                d = dict(row)\n",
    "        except Exception:\n",
    "            d = {}\n",
    "\n",
    "        total = sum(d.values()) or 1\n",
    "        noun.append(d.get('NOUN', 0) / total)\n",
    "        verb.append(d.get('VERB', 0) / total)\n",
    "        adj .append(d.get('ADJ',  0) / total)\n",
    "        adv .append(d.get('ADV',  0) / total)\n",
    "\n",
    "    df['noun_ratio'] = noun\n",
    "    df['verb_ratio'] = verb\n",
    "    df['adj_ratio']  = adj\n",
    "    df['adv_ratio']  = adv\n",
    "    return df\n",
    "\n",
    "X = add_pos_ratios(df.drop(columns=['id', 'statement', 'label']))\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.33,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "\n",
    "num_cols = ['num_tokens', 'num_sentences', 'num_tokens_without_stopwords',\n",
    "            'noun_ratio', 'verb_ratio', 'adj_ratio', 'adv_ratio']\n",
    "cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), num_cols),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)\n",
    "], remainder='drop')\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('pre', preprocessor),\n",
    "    ('clf', GaussianNB())\n",
    "])\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(\"Matriz de confusión (fila=verdadero ⇒ columna=predicho):\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Submission generada correctamente: 'gaussiannb_proporciones_de_POS_submission_20250513.csv'\n",
      "            id  label\n",
      "0  dc32e5ffa8b      0\n",
      "1  aa49bb41cab      1\n",
      "2  dddc8d12ac1      0\n",
      "3  bcfe8f51667      1\n",
      "4  eedbbaff5ab      0\n"
     ]
    }
   ],
   "source": [
    "def add_pos_ratios(df):\n",
    "    df = df.copy()\n",
    "    noun, verb, adj, adv = [], [], [], []\n",
    "    for row in df['pos_freq']:\n",
    "        try:\n",
    "            d = ast.literal_eval(row) if isinstance(row, str) else dict(row)\n",
    "        except Exception:\n",
    "            d = {}\n",
    "        total = sum(d.values()) or 1\n",
    "        noun.append(d.get('NOUN', 0) / total)\n",
    "        verb.append(d.get('VERB', 0) / total)\n",
    "        adj .append(d.get('ADJ',  0) / total)\n",
    "        adv .append(d.get('ADV',  0) / total)\n",
    "    df['noun_ratio'] = noun\n",
    "    df['verb_ratio'] = verb\n",
    "    df['adj_ratio']  = adj\n",
    "    df['adv_ratio']  = adv\n",
    "    return df\n",
    "\n",
    "X_test = add_pos_ratios(df_test.drop(columns=['id', 'statement']))\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id':    df_test['id'],\n",
    "    'label': y_pred\n",
    "})\n",
    "\n",
    "filename = f\"gaussiannb_proporciones_de_POS_submission_{datetime.datetime.now().strftime('%Y%m%d')}.csv\"\n",
    "submission.to_csv(filename, columns=['id', 'label'], index=False)\n",
    "\n",
    "print(f\" Submission generada correctamente: '{filename}'\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.- Intento 3: Variables demográficas y de speaker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión (fila=verdadero, columna=predicho):\n",
      "[[ 966   75]\n",
      " [1718  195]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3599    0.9280    0.5187      1041\n",
      "           1     0.7222    0.1019    0.1787      1913\n",
      "\n",
      "    accuracy                         0.3930      2954\n",
      "   macro avg     0.5411    0.5149    0.3487      2954\n",
      "weighted avg     0.5945    0.3930    0.2985      2954\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import (\n",
    "    FunctionTransformer, StandardScaler,\n",
    "    OneHotEncoder, QuantileTransformer\n",
    ")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report,\n",
    "    make_scorer, f1_score\n",
    ")\n",
    "\n",
    "X = df[['subject','speaker_type', 'speaker_job', 'party_affiliation_uni']]\n",
    "y = df['label']\n",
    "\n",
    "num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), num_cols),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)\n",
    "], remainder='drop')\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('pre', preprocessor),\n",
    "    ('clf', GaussianNB())\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.33,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "print(\"Matriz de confusión (fila=verdadero, columna=predicho):\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Submission generada correctamente: 'gaussiannb_submission_20250513_2207.csv'\n",
      "            id  label\n",
      "0  dc32e5ffa8b      0\n",
      "1  aa49bb41cab      0\n",
      "2  dddc8d12ac1      0\n",
      "3  bcfe8f51667      0\n",
      "4  eedbbaff5ab      0\n"
     ]
    }
   ],
   "source": [
    "X_test = df_test[['subject','speaker_type', 'speaker_job', 'party_affiliation_uni']]\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id':    df_test['id'],\n",
    "    'label': y_pred\n",
    "})\n",
    "\n",
    "\n",
    "filename = f\"gaussiannb_submission_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}.csv\"\n",
    "submission.to_csv(filename, columns=['id', 'label'], index=False)\n",
    "\n",
    "print(f\" Submission generada correctamente: '{filename}'\")\n",
    "print(submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'statement', 'subject', 'speaker', 'speaker_job', 'state_info',\n",
      "       'party_affiliation', 'party_affiliation_uni',\n",
      "       'party_affiliation_category_map', 'statement_tokens', 'num_tokens',\n",
      "       'num_sentences', 'pos_info', 'pos_freq', 'lemma_freq', 'tag_freq',\n",
      "       'entities', 'stopwords', 'statement_tokens_without_stopwords',\n",
      "       'num_tokens_without_stopwords', 'pos_info_without_stopwords',\n",
      "       'pos_freq_without_stopwords', 'lemma_freq_without_stopwords',\n",
      "       'tag_freq_without_stopwords', 'processed_subject', 'speaker_entities',\n",
      "       'speaker_type', 'speaker_job_tokens', 'state_info_tokens',\n",
      "       'party_affiliation_tokens'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.- Intento 4: MultinomialNB con TF-IDF y búsqueda de parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor F1 (CV): 0.7857383249956282\n",
      "Mejores parámetros: {'nb__alpha': 1.0, 'tfidf__max_features': None, 'tfidf__ngram_range': (1, 2)}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.37      0.54      3155\n",
      "           1       0.74      1.00      0.85      5795\n",
      "\n",
      "    accuracy                           0.78      8950\n",
      "   macro avg       0.87      0.68      0.69      8950\n",
      "weighted avg       0.83      0.78      0.74      8950\n",
      "\n",
      "Matriz de confusión:\n",
      " [[1157 1998]\n",
      " [   0 5795]]\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "    ('nb',    MultinomialNB())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__max_features': [2000, 5000, None],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "    'nb__alpha': [0.1, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "grid.fit(df['statement'], df['label'])\n",
    "\n",
    "print(\"Mejor F1 (CV):\", grid.best_score_)\n",
    "print(\"Mejores parámetros:\", grid.best_params_)\n",
    "\n",
    "y_pred = grid.predict(df['statement'])\n",
    "print(classification_report(df['label'], y_pred))\n",
    "print(\"Matriz de confusión:\\n\", confusion_matrix(df['label'], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Submission generada: submission_nb_20250517_1424.csv\n",
      "            id  label\n",
      "0  dc32e5ffa8b      1\n",
      "1  aa49bb41cab      1\n",
      "2  dddc8d12ac1      1\n",
      "3  bcfe8f51667      1\n",
      "4  eedbbaff5ab      1\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid.predict(df_test['statement'])\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id':    df_test['id'],\n",
    "    'label': y_pred\n",
    "})\n",
    "\n",
    "filename = f\"submission_nb_{datetime.datetime.now():%Y%m%d_%H%M}.csv\"\n",
    "submission.to_csv(filename, index=False, columns=['id','label'])\n",
    "\n",
    "print(f\" Submission generada: {filename}\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.- Intento 5: pipelines mixtos y BERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from scipy.sparse import hstack\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train classification report GNB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      1.00      0.72      3155\n",
      "           1       1.00      0.57      0.73      5795\n",
      "\n",
      "    accuracy                           0.72      8950\n",
      "   macro avg       0.78      0.79      0.72      8950\n",
      "weighted avg       0.84      0.72      0.72      8950\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8950/8950 [00:00<00:00, 10430.27 examples/s]\n",
      "Map: 100%|██████████| 3836/3836 [00:00<00:00, 10553.57 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 156\u001b[39m\n\u001b[32m    151\u001b[39m model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=\u001b[32m2\u001b[39m)\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;66;03m# Entrenamiento con Trainer de Huggingface\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./bert_fake_news_model\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msteps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msteps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./logs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mf1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m)\u001b[49m \n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "def list_to_text(x):\n",
    "    if isinstance(x, str):\n",
    "        x = x.strip(\"[]\").replace(\"'\", \"\").replace('\"', \"\")\n",
    "        tokens = [tok.strip() for tok in x.split(\",\") if tok.strip()]\n",
    "        return \" \".join(tokens)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "for col in ['statement_tokens_without_stopwords', 'processed_subject', 'party_affiliation_tokens', 'speaker_job_tokens']:\n",
    "    df[col] = df[col].apply(list_to_text)\n",
    "    df_test[col] = df_test[col].apply(list_to_text)\n",
    "\n",
    "\n",
    "text_col = 'statement_tokens_without_stopwords'\n",
    "cat_cols = ['processed_subject', 'party_affiliation_tokens', 'speaker_job_tokens']\n",
    "\n",
    "X_train_text = df[text_col]\n",
    "X_train_cat = df[cat_cols]\n",
    "y_train = df['label']\n",
    "\n",
    "X_test_text = df_test[text_col]\n",
    "X_test_cat = df_test[cat_cols]\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.key].values\n",
    "\n",
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, keys):\n",
    "        self.keys = keys\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.keys]\n",
    "\n",
    "\n",
    "text_pipeline = Pipeline([\n",
    "    ('selector', TextSelector(text_col)),\n",
    "    ('tfidf', TfidfVectorizer(min_df=5, max_df=0.9, ngram_range=(1,2), sublinear_tf=True))\n",
    "])\n",
    "\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('selector', ColumnSelector(cat_cols)),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "X_text_train_tfidf = text_pipeline.fit_transform(df)\n",
    "X_cat_train_oh = cat_pipeline.fit_transform(df)\n",
    "\n",
    "X_text_test_tfidf = text_pipeline.transform(df_test)\n",
    "X_cat_test_oh = cat_pipeline.transform(df_test)\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "X_train_combined = hstack([X_text_train_tfidf, X_cat_train_oh])\n",
    "X_test_combined = hstack([X_text_test_tfidf, X_cat_test_oh])\n",
    "\n",
    "X_train_dense = X_train_combined.toarray()\n",
    "X_test_dense = X_test_combined.toarray()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_dense)\n",
    "X_test_scaled = scaler.transform(X_test_dense)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_train_pred = gnb.predict(X_train_scaled)\n",
    "print(\"Train classification report GNB:\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "y_test_pred = gnb.predict(X_test_scaled)\n",
    "\n",
    "joblib.dump(gnb, 'gnb_fake_news_model.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(text_pipeline, 'text_pipeline.pkl')\n",
    "joblib.dump(cat_pipeline, 'cat_pipeline.pkl')\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "def concat_text(row):\n",
    "    cats = []\n",
    "    for c in cat_cols:\n",
    "        cats.append(row[c])\n",
    "    cats_concat = \" \".join(cats)\n",
    "    return row['statement'] + \" \" + cats_concat\n",
    "\n",
    "df['bert_input'] = df.apply(concat_text, axis=1)\n",
    "df_test['bert_input'] = df_test.apply(concat_text, axis=1)\n",
    "\n",
    "hf_train_dataset = Dataset.from_pandas(df[['bert_input','label']])\n",
    "hf_test_dataset = Dataset.from_pandas(df_test[['bert_input']])\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['bert_input'], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "tokenized_train = hf_train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = hf_test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./bert_fake_news_model',\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    ") \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1_w, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_train,  \n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained('./bert_fake_news_model')\n",
    "tokenizer.save_pretrained('./bert_fake_news_model')\n",
    "\n",
    "\n",
    "predictions = trainer.predict(tokenized_test)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "\n",
    "df_test['label_pred'] = pred_labels\n",
    "df_test[['id', 'label_pred']].to_csv('test_predictions_bert.csv', index=False)\n",
    "filename = f\"submission_nb_{datetime.datetime.now():%Y%m%d_%H%M}.csv\"\n",
    "df_test.to_csv(filename, index=False, columns=['id','label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusión**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El recorrido evidencia que, sin enriquecer el modelo con características lingüísticas relevantes o sin equilibrar el sesgo de clases, un Naive Bayes básico se estanca alrededor del 40–50 % de exactitud y favorece desproporcionadamente una etiqueta. Añadir razones POS impulsó el rendimiento, pero el verdadero salto llegará al explotar métodos de lenguaje profundo. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
