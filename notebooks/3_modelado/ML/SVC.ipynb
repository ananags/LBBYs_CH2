{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5ecad52",
   "metadata": {},
   "source": [
    "# Modelo SVC para detección de noticias falsas\n",
    "\n",
    "En este notebook, entrenaremos un modelo de clasificación basado en **Support Vector Classification (SVC)** para predecir si una afirmación política es **verdadera** o **falsa**.\n",
    "\n",
    "Se utilizarán técnicas de preprocesamiento de texto (TF-IDF) y metadatos categóricos (codificados y escalados). Además, se aplicará balanceo de clases con SMOTE, selección de características, y búsqueda de hiperparámetros con GridSearchCV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2c983569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c295dc",
   "metadata": {},
   "source": [
    "### Carga dataset\n",
    "\n",
    "Se cargan los datasets de entrenamiento y prueba preprocesados, que contienen:\n",
    "\n",
    "- `statement`: texto de la afirmación (columna principal para NLP).\n",
    "- `subject`: tema de la afirmación (categórica).\n",
    "- `speaker`: persona que hizo la afirmación (categórica).\n",
    "- `party_affiliation`: partido político (categórica).\n",
    "- `state_info_*`: columnas relacionadas con la ubicación geográfica.\n",
    "- `label`: etiqueta binaria (0 = falso, 1 = verdadero) para entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "116400cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datasets limpios\n",
    "train_data = pd.read_csv('C:/Users/inesg/dev/LBBYs_CH2/data/processed/train_simp_preprocess_v2.csv')\n",
    "test_data = pd.read_csv('C:/Users/inesg/dev/LBBYs_CH2/data/processed/test_simp_preprocess_v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bee942",
   "metadata": {},
   "source": [
    "### Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a1057d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_folder = \"C:/Users/inesg/dev/LBBYs_CH2/notebooks/3_summision\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a417ea1b",
   "metadata": {},
   "source": [
    "### Preprocesamiento de los datos\n",
    "#### 1. Preprocesamiento del texto con TF-IDF\n",
    "Se transforma la columna `statement` con TF-IDF, limitando a las 1000 palabras más importantes y eliminando stopwords en inglés.\n",
    "\n",
    "Esto convierte el texto en vectores numéricos que el modelo puede procesar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aae622a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear y entrenar el tokenizer con TF-IDF\n",
    "tokenizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X_text = tokenizer.fit_transform(train_data['statement']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce83818",
   "metadata": {},
   "source": [
    "#### 2. Preprocesamiento de metadatos\n",
    "Se codifican las variables categóricas (`subject`, `speaker`, `party_affiliation`) con LabelEncoder y se reduce la información de `state_info_*` a un indicador binario de presencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2762026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder_subject = LabelEncoder()\n",
    "label_encoder_speaker = LabelEncoder()\n",
    "label_encoder_party = LabelEncoder()\n",
    "\n",
    "train_data['subject_encoded'] = label_encoder_subject.fit_transform(train_data['subject'])\n",
    "train_data['speaker_encoded'] = label_encoder_speaker.fit_transform(train_data['speaker'])\n",
    "\n",
    "state_info_columns = [col for col in train_data.columns if col.startswith('state_info')]\n",
    "train_data['state_info_encoded'] = train_data[state_info_columns].apply(\n",
    "    lambda x: 1 if any(isinstance(val, str) and len(val) > 0 for val in x) else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "train_data['party_affiliation_encoded'] = label_encoder_party.fit_transform(train_data['party_affiliation'])\n",
    "\n",
    "X_metadata = train_data[['subject_encoded', 'speaker_encoded', 'state_info_encoded', 'party_affiliation_encoded']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026119c6",
   "metadata": {},
   "source": [
    "#### 3. Escalar los metadatos\n",
    "Se normalizan los valores numéricos de los metadatos para que estén en la misma escala y el modelo no se sesgue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2674bf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_metadata_scaled = scaler.fit_transform(X_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed68930",
   "metadata": {},
   "source": [
    "#### 4. Unión texto y metadatos\n",
    "Se concatenan las características textuales y los metadatos para formar la matriz de características completa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6f64435",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = np.concatenate([X_text, X_metadata_scaled], axis=1)\n",
    "y = train_data['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7981f29",
   "metadata": {},
   "source": [
    "### División en train/test\n",
    "Dividimos los datos en conjunto de entrenamiento y prueba para evaluar la generalización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8fb3d91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8c2075",
   "metadata": {},
   "source": [
    "### Manejo clases desbalanceadas\n",
    "Si las clases están desbalanceadas, se aplica `SMOTE` para generar muestras sintéticas de la clase minoritaria solo en el conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9887c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases balanceadas en train tras SMOTE: [4616 4616]\n"
     ]
    }
   ],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "print(f\"Clases balanceadas en train tras SMOTE: {np.bincount(y_resampled)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481a36ef",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be25861",
   "metadata": {},
   "source": [
    "#### 1. GridSearchCV\n",
    "Se busca la mejor combinación de `C`, `gamma` y `kernel` para el modelo SVC, usando validación cruzada y priorizando el recall de la clase 0 (falsas).\n",
    "\n",
    "(En primer lugar he aplicado el gridSearch priorizando el accuaracy, luego lo he modificado para priorizar el f1-score de la clase 0 que era el  que peor resultados tenia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06a96e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1bc5ea07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros encontrados: {'C': 10, 'gamma': 1, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "svc = SVC(class_weight='balanced')\n",
    "# Usar recall de clase 0\n",
    "scorer_recall_0 = make_scorer(recall_score, pos_label=0)\n",
    "grid_search = GridSearchCV(svc, param_grid, scoring=scorer_recall_0, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_resampled, y_resampled)\n",
    "print(f\"Mejores parámetros encontrados: {grid_search.best_params_}\")\n",
    "\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1da975e",
   "metadata": {},
   "source": [
    "##### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2e5e3b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas en test:\n",
      "Accuracy: 0.6291\n",
      "Precision: 0.6799\n",
      "Recall: 0.8253\n",
      "F1-Score: 0.7456\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.25      0.32       611\n",
      "           1       0.68      0.83      0.75      1179\n",
      "\n",
      "    accuracy                           0.63      1790\n",
      "   macro avg       0.55      0.54      0.53      1790\n",
      "weighted avg       0.59      0.63      0.60      1790\n",
      "\n",
      "[[153 458]\n",
      " [206 973]]\n"
     ]
    }
   ],
   "source": [
    "# Mejor modelo encontrado\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Realizar predicciones\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "print(\"Métricas en test:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_test):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_test):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_test):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_test):.4f}\")\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "print(confusion_matrix(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e894debb",
   "metadata": {},
   "source": [
    "##### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "887cae74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission guardada en: C:/Users/inesg/dev/LBBYs_CH2/notebooks/3_summision/submission_grid_search_updated_1.csv\n"
     ]
    }
   ],
   "source": [
    "def encode_with_unknown_handling(le, series):\n",
    "    # Valores conocidos en train\n",
    "    known_labels = set(le.classes_)\n",
    "    # Reemplazar valores no conocidos por el más frecuente o un valor fijo\n",
    "    replacement = le.classes_[0]  # O el más frecuente en train\n",
    "    series_fixed = series.apply(lambda x: x if x in known_labels else replacement)\n",
    "    return le.transform(series_fixed)\n",
    "\n",
    "# Aplicar a cada columna categórica\n",
    "test_data['subject_encoded'] = encode_with_unknown_handling(label_encoder_subject, test_data['subject'])\n",
    "test_data['speaker_encoded'] = encode_with_unknown_handling(label_encoder_speaker, test_data['speaker'])\n",
    "test_data['party_affiliation_encoded'] = encode_with_unknown_handling(label_encoder_party, test_data['party_affiliation'])\n",
    "\n",
    "# Preprocesar texto test\n",
    "X_test_text = tokenizer.transform(test_data['statement']).toarray()\n",
    "\n",
    "# Codificar metadatos test\n",
    "test_data['state_info_encoded'] = test_data[state_info_columns].apply(\n",
    "    lambda x: 1 if any(isinstance(val, str) and len(val) > 0 for val in x) else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "X_test_metadata = test_data[['subject_encoded', 'speaker_encoded', 'state_info_encoded', 'party_affiliation_encoded']]\n",
    "\n",
    "# Escalar metadatos test\n",
    "X_test_metadata_scaled = scaler.transform(X_test_metadata)\n",
    "\n",
    "# Concatenar todo para obtener la matriz final con 1004 features\n",
    "X_test_final = np.concatenate([X_test_text, X_test_metadata_scaled], axis=1)\n",
    "\n",
    "# Predecir usando el modelo entrenado y matriz final\n",
    "y_pred_submission = best_model.predict(X_test_final)\n",
    "\n",
    "# Crear DataFrame submission\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_data['id'],\n",
    "    'label': y_pred_submission\n",
    "})\n",
    "\n",
    "submission_csv_path = f\"{submissions_folder}/submission_grid_search_updated_1.csv\"\n",
    "submission_df.to_csv(submission_csv_path, index=False)\n",
    "\n",
    "print(f\"Submission guardada en: {submission_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ba87bf",
   "metadata": {},
   "source": [
    "#### 2. SelectKBest\n",
    "Esta técnica selecciona las características más relevantes para la clasificación basándose en el análisis estadístico **ANOVA F-value**.\n",
    "\n",
    "Seleccionar solo las características más importantes puede ayudar a reducir ruido, mejorar el rendimiento y acelerar el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0acd2883",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\inesg\\dev\\LBBYs_CH2\\venv\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:111: UserWarning: Features [1002] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\inesg\\dev\\LBBYs_CH2\\venv\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "selector = SelectKBest(f_classif, k=100)\n",
    "X_train_selected = selector.fit_transform(X_resampled, y_resampled)\n",
    "X_test_selected = selector.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4568f795",
   "metadata": {},
   "source": [
    "##### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c7d1aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas con SelectKBest:\n",
      "Accuracy: 0.5642\n",
      "Precision: 0.7021\n",
      "Recall: 0.5878\n",
      "F1-Score: 0.6399\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.52      0.45       611\n",
      "           1       0.70      0.59      0.64      1179\n",
      "\n",
      "    accuracy                           0.56      1790\n",
      "   macro avg       0.55      0.55      0.54      1790\n",
      "weighted avg       0.60      0.56      0.57      1790\n",
      "\n",
      "[[317 294]\n",
      " [486 693]]\n"
     ]
    }
   ],
   "source": [
    "svc_selected_model = SVC(C=10, gamma=1, kernel='rbf')\n",
    "svc_selected_model.fit(X_train_selected, y_resampled)\n",
    "\n",
    "y_pred_selectKBest = svc_selected_model.predict(X_test_selected)\n",
    "\n",
    "print(\"Métricas con SelectKBest:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_selectKBest):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_selectKBest):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_selectKBest):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_selectKBest):.4f}\")\n",
    "print(classification_report(y_test, y_pred_selectKBest))\n",
    "print(confusion_matrix(y_test, y_pred_selectKBest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee811121",
   "metadata": {},
   "source": [
    "##### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b0b25ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission guardada en: C:/Users/inesg/dev/LBBYs_CH2/notebooks/3_summision/submission_selectKBest.csv\n"
     ]
    }
   ],
   "source": [
    "# Preprocesar texto test\n",
    "X_test_text = tokenizer.transform(test_data['statement']).toarray()\n",
    "\n",
    "# Codificar metadatos test con manejo de valores desconocidos\n",
    "def encode_with_unknown_handling(le, series):\n",
    "    known_labels = set(le.classes_)\n",
    "    replacement = le.classes_[0]\n",
    "    series_fixed = series.apply(lambda x: x if x in known_labels else replacement)\n",
    "    return le.transform(series_fixed)\n",
    "\n",
    "test_data['subject_encoded'] = encode_with_unknown_handling(label_encoder_subject, test_data['subject'])\n",
    "test_data['speaker_encoded'] = encode_with_unknown_handling(label_encoder_speaker, test_data['speaker'])\n",
    "\n",
    "test_data['state_info_encoded'] = test_data[state_info_columns].apply(\n",
    "    lambda x: 1 if any(isinstance(val, str) and len(val) > 0 for val in x) else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "test_data['party_affiliation_encoded'] = encode_with_unknown_handling(label_encoder_party, test_data['party_affiliation'])\n",
    "\n",
    "X_test_metadata = test_data[['subject_encoded', 'speaker_encoded', 'state_info_encoded', 'party_affiliation_encoded']]\n",
    "\n",
    "# Escalar metadatos test\n",
    "X_test_metadata_scaled = scaler.transform(X_test_metadata)\n",
    "\n",
    "# Concatenar texto y metadatos\n",
    "X_test_final = np.concatenate([X_test_text, X_test_metadata_scaled], axis=1)\n",
    "\n",
    "# Aplicar SelectKBest al test\n",
    "X_test_selected = selector.transform(X_test_final)\n",
    "\n",
    "# Predecir con modelo entrenado\n",
    "y_pred_selectKBest_submission = svc_selected_model.predict(X_test_selected)\n",
    "\n",
    "# Crear DataFrame de submission\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_data['id'],\n",
    "    'label': y_pred_selectKBest_submission\n",
    "})\n",
    "\n",
    "# Guardar CSV\n",
    "submission_csv_path = f\"{submissions_folder}/submission_selectKBest.csv\"\n",
    "submission_df.to_csv(submission_csv_path, index=False)\n",
    "\n",
    "print(f\"Submission guardada en: {submission_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17670615",
   "metadata": {},
   "source": [
    "#### 3. RFE (Recursive Feature Elimination)\n",
    "\n",
    "RFE es una técnica que elimina recursivamente las características menos importantes basándose en la importancia que asigna un estimador (en este caso un SVC lineal).\n",
    "\n",
    "Se entrena un modelo con todas las características, se elimina la menos importante, y se repite hasta quedarse con el número deseado de características.\n",
    "\n",
    "Esto ayuda a obtener un subconjunto de características muy relevantes para mejorar la generalización y reducir ruido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a41b032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46783ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_base = SVC(kernel='linear', C=1)\n",
    "rfe_selector = RFE(estimator=svc_base, n_features_to_select=50)\n",
    "\n",
    "X_train_rfe = rfe_selector.fit_transform(X_resampled, y_resampled)\n",
    "\n",
    "X_test_rfe = rfe_selector.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eca16f",
   "metadata": {},
   "source": [
    "##### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ede1796b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy con RFE: 0.5056\n",
      "Precision con RFE: 0.7466\n",
      "Recall con RFE: 0.3774\n",
      "F1-Score con RFE: 0.5014\n",
      "Reporte de clasificación con RFE:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.75      0.51       611\n",
      "           1       0.75      0.38      0.50      1179\n",
      "\n",
      "    accuracy                           0.51      1790\n",
      "   macro avg       0.57      0.57      0.51      1790\n",
      "weighted avg       0.62      0.51      0.50      1790\n",
      "\n",
      "Matriz de confusión con RFE:\n",
      "[[460 151]\n",
      " [734 445]]\n"
     ]
    }
   ],
   "source": [
    "svc_rfe_model = SVC(C=10, gamma=1, kernel='rbf')\n",
    "svc_rfe_model.fit(X_train_rfe, y_resampled)\n",
    "\n",
    "y_pred_rfe = svc_rfe_model.predict(X_test_rfe)\n",
    "\n",
    "print(f\"Accuracy con RFE: {accuracy_score(y_test, y_pred_rfe):.4f}\")\n",
    "print(f\"Precision con RFE: {precision_score(y_test, y_pred_rfe):.4f}\")\n",
    "print(f\"Recall con RFE: {recall_score(y_test, y_pred_rfe):.4f}\")\n",
    "print(f\"F1-Score con RFE: {f1_score(y_test, y_pred_rfe):.4f}\")\n",
    "\n",
    "print(\"Reporte de clasificación con RFE:\")\n",
    "print(classification_report(y_test, y_pred_rfe))\n",
    "print(\"Matriz de confusión con RFE:\")\n",
    "print(confusion_matrix(y_test, y_pred_rfe))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e33e630",
   "metadata": {},
   "source": [
    "##### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b606541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission guardada en: C:/Users/inesg/dev/LBBYs_CH2/notebooks/3_summision/submission_rfe.csv\n"
     ]
    }
   ],
   "source": [
    "# Preprocesar texto test\n",
    "X_test_text = tokenizer.transform(test_data['statement']).toarray()\n",
    "\n",
    "# Codificar metadatos test con manejo de valores desconocidos\n",
    "def encode_with_unknown_handling(le, series):\n",
    "    known_labels = set(le.classes_)\n",
    "    replacement = le.classes_[0]\n",
    "    series_fixed = series.apply(lambda x: x if x in known_labels else replacement)\n",
    "    return le.transform(series_fixed)\n",
    "\n",
    "test_data['subject_encoded'] = encode_with_unknown_handling(label_encoder_subject, test_data['subject'])\n",
    "test_data['speaker_encoded'] = encode_with_unknown_handling(label_encoder_speaker, test_data['speaker'])\n",
    "\n",
    "test_data['state_info_encoded'] = test_data[state_info_columns].apply(\n",
    "    lambda x: 1 if any(isinstance(val, str) and len(val) > 0 for val in x) else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "test_data['party_affiliation_encoded'] = encode_with_unknown_handling(label_encoder_party, test_data['party_affiliation'])\n",
    "\n",
    "X_test_metadata = test_data[['subject_encoded', 'speaker_encoded', 'state_info_encoded', 'party_affiliation_encoded']]\n",
    "\n",
    "# Escalar metadatos test\n",
    "X_test_metadata_scaled = scaler.transform(X_test_metadata)\n",
    "\n",
    "# Concatenar texto y metadatos\n",
    "X_test_final = np.concatenate([X_test_text, X_test_metadata_scaled], axis=1)\n",
    "\n",
    "# Aplicar RFE selector al test\n",
    "X_test_rfe = rfe_selector.transform(X_test_final)\n",
    "\n",
    "# Predecir con modelo entrenado\n",
    "y_pred_rfe_submission = svc_rfe_model.predict(X_test_rfe)\n",
    "\n",
    "# Crear DataFrame de submission\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_data['id'],\n",
    "    'label': y_pred_rfe_submission\n",
    "})\n",
    "\n",
    "# Guardar CSV\n",
    "submission_csv_path = f\"{submissions_folder}/submission_rfe.csv\"\n",
    "submission_df.to_csv(submission_csv_path, index=False)\n",
    "\n",
    "print(f\"Submission guardada en: {submission_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9b78d2",
   "metadata": {},
   "source": [
    "#### 4. SVC sobre SBERT Embeddings con GridSearchCV\n",
    "Para abordarlo, primero convertimos cada afirmación de texto en un vector denso usando un modelo SBERT ligero (`all-MiniLM-L6-v2`), lo que nos permite capturar relaciones semánticas y contexto más allá de las simples frecuencias de palabra. A continuación, normalizamos estos vectores con un escalador estándar y entrenamos un clasificador SVC con kernel RBF y pesos de clase equilibrados para enfrentar el desbalance de etiquetas. Para ajustar los hiperparámetros críticos (`C` y `gamma`), aplicamos una búsqueda de cuadrícula (`GridSearchCV`) focalizada en maximizar la métrica macro-F1 mediante validación cruzada estratificada de tres pliegues. Una vez seleccionados los valores óptimos, evaluamos el rendimiento en un conjunto de validación independiente, reentrenamos el modelo final con todo el conjunto de entrenamiento y generamos las predicciones para el test, guardándolas en el CSV de envío.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1e0769c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 224/224 [00:18<00:00, 12.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'svc__C': 1, 'svc__gamma': 'scale'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 56/56 [00:04<00:00, 13.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4714    0.5626    0.5130       631\n",
      "           1     0.7338    0.6566    0.6931      1159\n",
      "\n",
      "    accuracy                         0.6235      1790\n",
      "   macro avg     0.6026    0.6096    0.6030      1790\n",
      "weighted avg     0.6413    0.6235    0.6296      1790\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 280/280 [00:22<00:00, 12.66it/s]\n",
      "Batches: 100%|██████████| 120/120 [00:08<00:00, 13.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission guardada en submission.csv\n"
     ]
    }
   ],
   "source": [
    "# 1. Carga datos de entrenamiento\n",
    "train_df = pd.read_csv('train_simp_preprocess_v2.csv')\n",
    "X = train_df['statement'].tolist()\n",
    "y = train_df['label'].values\n",
    "\n",
    "# 2. Split train/test para búsqueda de hiperparámetros\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Define embedder SBERT ligero\n",
    "enbedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def embed_corpus(texts):\n",
    "    return enbedder.encode(texts, show_progress_bar=True)\n",
    "\n",
    "# 4. Pipeline: embeddings → escalado → SVC\n",
    "pipe = Pipeline([\n",
    "    ('embed',  FunctionTransformer(lambda txts: embed_corpus(txts), validate=False)),\n",
    "    ('scale',  StandardScaler()),\n",
    "    ('svc',    SVC(kernel='rbf', class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# 5. GridSearchCV sobre f1_macro para C y gamma\n",
    "param_grid = {\n",
    "    'svc__C':     [0.1, 1, 10],\n",
    "    'svc__gamma': ['scale', 0.01, 0.1]\n",
    "}\n",
    "grid = GridSearchCV(pipe, param_grid, cv=3, scoring='f1_macro', n_jobs=-1, verbose=2)\n",
    "\n",
    "# 6. Entrenar grid\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "\n",
    "# 7. Evaluación en validación\n",
    "y_val_pred = grid.predict(X_val)\n",
    "print(classification_report(y_val, y_val_pred, digits=4))\n",
    "\n",
    "# 8. Retrain mejor modelo con todos los datos de entrenamiento\n",
    "best_pipe = grid.best_estimator_\n",
    "best_pipe.fit(X, y)\n",
    "\n",
    "# 9. Generar submission\n",
    "test_df = pd.read_csv('test_simp_preprocess_v2.csv')\n",
    "X_test = test_df['statement'].tolist()\n",
    "preds = best_pipe.predict(X_test)\n",
    "submission = pd.DataFrame({'id': test_df['id'], 'label': preds})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission guardada en submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d67fe20",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "1. **Macro-F1 Score**  \n",
    "   - El mejor macro-F1 se obtiene con el **SVC sobre SBERT Embeddings** (0.6030), muy por encima de **SelectKBest** (0.54) y **RFE** (0.51).\n",
    "\n",
    "2. **Balance entre clases**  \n",
    "   - Aunque RFE logró la mayor precisión macro (0.57), su macro-F1 cae a 0.51 porque sacrifica recall en la clase mayoritaria (“true”).  \n",
    "   - El SVC sobre SBERT mejora notablemente el F1 de ambas clases (0.5130 para “fake” y 0.6931 para “true”), ofreciendo un trade-off más equilibrado.\n",
    "\n",
    "3. **Por qué SVC puro con TF-IDF se queda corto**  \n",
    "   - **Alta dimensionalidad y sparsity** de TF-IDF penaliza la generalización.  \n",
    "   - **Falta de contexto**: no captura relaciones sintácticas ni semánticas profundas.  \n",
    "   - **Desbalance de clases**: con macro-F1 alrededor de 0.5, el modelo no aprende bien ninguno de los dos segmentos.\n",
    "\n",
    "4. **Vías de mejora**  \n",
    "   - Integrar **embeddings pre-entrenados** (SBERT, FastText, BERT) como base, tal como hicimos en el SVC sobre SBERT.  \n",
    "   - Aplicar **reduction of dimensionality** (SVD, UMAP) o **kernel approximation** antes de SVC.  \n",
    "   - Probar **ensembles suaves** (votación, stacking) y **calibración de probabilidades** para afinar scores.  \n",
    "   - Explorar **modelos basados en transformadores** o ensamblajes avanzados (Random Forest, XGBoost, LightGBM) sobre embeddings enriquecidos.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
